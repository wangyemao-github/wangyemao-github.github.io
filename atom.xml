<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://wangyemao-github.github.io</id>
    <title>Gridea</title>
    <updated>2021-01-28T02:18:54.251Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://wangyemao-github.github.io"/>
    <link rel="self" href="https://wangyemao-github.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://wangyemao-github.github.io/images/avatar.png</logo>
    <icon>https://wangyemao-github.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[部署]]></title>
        <id>https://wangyemao-github.github.io/post/bu-shu/</id>
        <link href="https://wangyemao-github.github.io/post/bu-shu/">
        </link>
        <updated>2021-01-21T07:18:29.000Z</updated>
        <content type="html"><![CDATA[<p>Standalone</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Savepoint]]></title>
        <id>https://wangyemao-github.github.io/post/savepoint/</id>
        <link href="https://wangyemao-github.github.io/post/savepoint/">
        </link>
        <updated>2021-01-21T01:04:58.000Z</updated>
        <content type="html"><![CDATA[<h1 id="什么是savepoint-savepoint-与checkpoint的差异">什么是savepoint ？ savepoint 与checkpoint的差异？</h1>
<p>savepoint(保存点)是流作业执行状态的一致性镜像，通过flink的<strong>checkpoint机制</strong>创建的。你可以使用savepoint来停止和恢复，fork，或更新Flink作业。<strong>savepoint由两部分组成：一个基于稳定存储（比如，HDFS, S3）的包含二进制文件的目录（通常比较大）；一个元数据文件(相对较小)</strong>。稳定存储上的文件为作业指定状态镜像的网络数据。savepoint的元数据文件以绝对路径的形式(主要地)包含了指向保存点稳定存储的所有文件的指针。</p>
<blockquote>
<p><strong>注意</strong>：为了允许程序和Flink版本的升级，检查是否为操作分配了ID是非常重要的</p>
</blockquote>
<p>从概念上讲，Flink的savepoint与checkpoint的区别同传统数据库系统中的备份与恢复日志的区别很相似。checkpoint的主要目的是在作业意外失败时提供恢复机制。checkpoint的生命周期是由Flink管理的。也就是说，checkpoint是由Flink创建、持有和释放的，不需要与用户交互。作为一种恢复和周期性的触发方法，checkpoint实现的两个主要设计目标是：（1）轻量级的创建；（2）尽可能快递恢复。针对这些目标的优化可以利用某些属性，例如，作业代码在两次执行尝试之间不会改变。<strong>checkpoint通常在用户终止作业之后被删除(除非显式地配置为保checkpoint)</strong>。</p>
<p>相反地，savepoint是由用户创建、持有和删除的。它们的用途就是计划性地手动备份和恢复。比如，Flink的版本更新，变更job图，变更并行度，像红/蓝部署那样派生另一个job，等等。当然，<strong>savepoint必须在作业停止之后继续存在</strong>。<strong>从概念上将，savepoint的生成和恢复成本可能会更高，并且它更对地关注与可以执行以及对前面提到的作业变更的支持</strong>。</p>
<p>撇开这些概念上的差异，<strong>当前checkpoint和savepoint的实现基本上使用相同的代码并生成相同的格式。但是，目前存在一个例外</strong>，在未来可能会引入更多的差异。<strong>例外的情况是基于RocksDB 后备状态的增量checkpoint。这种情况使用的是RocksDB的内部格式，而不是flink的原生savepoint格式</strong>。这就是checkpoint比savepoint更轻量级的第一个具体事例。</p>
<h1 id="分配操作idsoperator-ids">分配操作IDs(Operator IDs)</h1>
<p>强烈建议您按照本节的描述调整您的程序，以便将来能够升级您的程序.<strong>主要需要的更改是通过uid(String)方法手动指定操作符id</strong>。这些id用于确定每个操作符的状态</p>
<pre><code class="language-java">DataStream&lt;String&gt; stream = env.
  // Stateful source (e.g. Kafka) with ID
  .addSource(new StatefulSource())
  .uid(&quot;source-id&quot;) // ID for the source operator
  .shuffle()
  // Stateful mapper with ID
  .map(new StatefulMapper())
  .uid(&quot;mapper-id&quot;) // ID for the mapper
  // Stateless printing sink
  .print(); // Auto-generated ID
</code></pre>
<p>如果您不手动指定id，它们将自动生成。只要这些id不更改，就可以从保存点自动恢复。<strong>生成的id取决于程序的结构，并且对程序更改很敏感。因此，强烈建议手动分配这些id</strong></p>
<h2 id="savepoint-state">Savepoint State</h2>
<p>你可以将savepoint看成是Operator ID 与每个有状态操作的State之间的映射<br>
<img src="https://wangyemao-github.github.io/post-images/1611209765479.png" alt="" loading="lazy"></p>
<p>在上面的例子中，print sink是无状态的，因此它不是保存点状态的一部分。默认情况下，我们尝试将保存点的每个条目映射回新程序</p>
<h1 id="operations操作">Operations（操作）</h1>
<p>您可以使用命令行客户端来触发savepoints、完成savepoints并取消作业、从savepoint恢复以及删除savepoint<br>
<strong>在Flink1.2.0版本之后，也可以使用webui，从savepoint恢复作业</strong></p>
<h2 id="触发savepoints">触发savepoints</h2>
<p>当触发一个savepoints时，将创建一个新的savepoints目录，其中存储数据和元数据。可以通过<strong>配置默认目标目录</strong>或使用触发器命令指定自定义目标目录来控制此目录的位置。<br>
<strong>注意</strong>：目标目录必须是JobManager和TaskManager都可以访问的位置。比如，分布式文件系统上的某个目录<br>
例如使用FsStateBackend或RocksDBStateBackend的savepoint目录结构：<br>
<img src="https://wangyemao-github.github.io/post-images/1611210701803.png" alt="" loading="lazy"></p>
<p><strong>注意</strong>：尽管看起来savepoint可以被移动，但由于元数据文件中保存的是状态文件的绝对路径，目前这是不可能的。请跟踪 FLINK-5778 查看为解除这一限制的处理进展</p>
<p><strong>注意</strong>：如果你使用 MemoryStateBackend，元数据和savepoint状态数据都将存储在**_metadata** 文件中。由于它是自包含的，你可以移动文件并从任何位置恢复。</p>
<p><strong>注意</strong>：不建议移动或删除正在运行的作业的最后一个保存点，因为这可能会妨碍故障恢复。保存点对“exactly-once”的sink是有副作用的，因此为了确保“exactly-once&quot; 语义，如果在最后一个savepoint之后没有新的检查点，那么savepoint将用于恢复。</p>
<h2 id="trigger-a-savepoint">Trigger a Savepoint</h2>
<blockquote>
<p>bin/flink savepoint :jobId [:targetDirectory]<br>
这将为ID:jobId的作业触发一个保存点，并返回创建的保存点的路径。您需要此路径来恢复和删除保存点</p>
</blockquote>
<h2 id="cancel-job-with-savepoint">Cancel Job with Savepoint</h2>
<blockquote>
<p>$ bin/flink cancel -s [:targetDirectory] :jobId<br>
这将自动触发ID:jobid作业的保存点并取消作业。此外，还可以指定存放保存点的目标文件系统目录。该目录需要被JobManager(s)和TaskManager(s)访问。</p>
</blockquote>
<h2 id="resuming-from-savepoints">Resuming from Savepoints</h2>
<blockquote>
<p>$ bin/flink run -s :savepointPath [:runArgs]<br>
这将提交作业并指定要从中恢复的保存点。您可以指定保存点目录或元数据文件的路径</p>
</blockquote>
<h2 id="allowing-non-restored-state">Allowing Non-Restored State</h2>
<p>默认情况下，恢复操作将尝试将保存点的所有状态映射回您正在使用的程序。如果存在删除某些操作，可以通过**--allowNonRestoredState（short: -n）选项**，跳过不能映射到新程序的状态</p>
<blockquote>
<p>$ bin/flink run -s :savepointPath -n [:runArgs]</p>
</blockquote>
<h2 id="disposing-savepoints">Disposing Savepoints</h2>
<blockquote>
<p>$ bin/flink savepoint -d :savepointPath<br>
该命令会处理保存在:savepointPath路径的保存点<br>
<strong>注意</strong> ：也可以通过常规文件系统操作手动删除保存点，而不会影响其他保存点或检查点(记住，每个保存点都是自包含的)</p>
</blockquote>
<p><strong>configuration：</strong></p>
<blockquote>
<p>可以通过键<strong>state.savepoints.dir</strong>配置一个默认的savepoint目标目录。当触发savepoint时，该目录将用于存储savepoint。你也可以通过触发命令指定自定义的目标目录以覆盖默认值。</p>
<h1 id="default-savepoint-target-directory">Default savepoint target directory</h1>
<p>state.savepoints.dir: hdfs:///flink/savepoints</p>
</blockquote>
<h1 id="q-a">Q &amp; A</h1>
<ol>
<li>是否应该为job中的所有操作(operator)分配IDs<br>
<strong>根据经验来说，是的</strong><br>
严格来说，只需要通过uid()方法将IDs分配给job中有状态的操作就足够了。savepoint只保存有状态操作的状态，无状态的操作不是savepoint的一部分<br>
实践中，建议为所有的操作都指定IDs。因为一些Flink内置的操作比如window操作也是有状态的，并且内置的操作实际上是否是有状态的也并不是很清楚。如果你确定某个操作时状态的，那么你可以直接跳过指定IDs。</li>
<li>如果向job中添加了一个需要状态的新操作，会发生什么<br>
当您向作业添加一个新的操作时，它将在没有任何状态的情况下被初始化。Savepoints包含了所有有状态操作的状态。无状态的操作并不是savepoint的一部分。新增有状态操作就类似于无状态操作的处理</li>
<li>如果我从我的job中删除一个有状态的操作符会发生什么<br>
默认情况下，savepoint恢复将尝试将所有保存状态与还原的作业匹配。因此，如果从包含已删除操作状态的保存点进行恢复，则将失败<br>
可以通过**--allowNonRestoredState（short: -n）选项**，跳过不能映射到新程序的状态恢复：
<blockquote>
<p>$ bin/flink run -s :savepointPath -n [:runArgs]</p>
</blockquote>
</li>
<li>如果job中重新排序有状态操作会发生什么<br>
如果您给这些操作分配了id，它们将像正常一样被恢复<br>
如果您没有分配id，那么自动生成的有状态操作符的id很可能在重新排序后发生变化。这将导致您无法从以前的保存点恢复。</li>
<li>如果在job中添加、删除或重新排序没有状态的操作会发生什么情况<br>
如果将id分配给有状态操作，则无状态操作符将不会影响保存点恢复。<br>
如果没有分配id，则有状态操作符的自动生成的id很可能在重新排序后发生变化。这将导致您无法从以前的保存点恢复。</li>
<li>当我在恢复时改变程序的并行性时会发生什么<br>
如果保存点是通过Flink &gt;= 1.2.0触发的，并且没有使用像checkpoint等过时的状态API，那么您可以简单地从保存点恢复程序并指定一个新的并行度。<br>
如果你要从一个由Flink &lt; 1.2.0触发的保存点恢复，或者使用现在已经不支持的api，你必须首先将你的作业和保存点迁移到Flink &gt;= 1.2.0，然后才能改变并行度。<strong>请参阅升级作业和Flink版本指南</strong>。</li>
<li>可以移动保存点文件在稳定存储<br>
这个问题的快速答案是目前没有，因为元数据文件引用稳定存储文件作为绝对路径的技术原因。较长的答案是:如果由于某种原因必须移动文件，有两种可能的解决方法。首先，更简单但潜在更危险的是，您可以使用编辑器在元数据文件中找到旧路径，并用新路径替换它们。其次，可以使用SavepointV2Serializer类作为起点，以编程方式读取、操作和使用新路径重写元数据文件。</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[upgrading applications and flink version —翻译from官网]]></title>
        <id>https://wangyemao-github.github.io/post/upgrading-applications-and-flink-version-fan-yi-from-guan-wang/</id>
        <link href="https://wangyemao-github.github.io/post/upgrading-applications-and-flink-version-fan-yi-from-guan-wang/">
        </link>
        <updated>2021-01-20T14:36:40.000Z</updated>
        <content type="html"><![CDATA[<p>Flink 流程序(DataStream Programs)通常设计为长时间运行，比如几周、几个月甚至几年。同所有长时间运行的服务一样，Flink流程序需要不断进行维护，包括：<strong>bug修复</strong>，<strong>实现改进</strong>，或<strong>将应用程序迁移到更高版本的Flink集群</strong><br>
<strong>本文主要描述：如何更新Flink流程序，以及如何将正在运行的流程序迁移到不同的Flink集群</strong></p>
<h1 id="重启流程序">重启流程序</h1>
<p>升级Flink流应用程序以及将应用程序迁移到不同的集群操作路线都是基于Flink的<strong>Savepoint特性</strong>。一个savepoint就是应用在某个特定时间点的一致性快照。<br>
有两种方式可以从正在运行的流应用程序中获取savepoint：</p>
<ol>
<li>应用程序继续运行的同时执行savepoint：
<blockquote>
<p>./bin/flink savepoint <jobID> [pathToSavepoint]<br>
推荐定期获取savepoint，以便能从之前的时间点重新重启应用程序。</p>
</blockquote>
</li>
<li>停止应用程序的同时，执行savepoint：
<blockquote>
<p>./bin/flink cancel -s [pathToSavepoint] <jobID><br>
这意味着应用程序在完成savepoint之后会立即被取消。也就是说，在此次savepoint发生之后不会再有其他的checkpoint发生</p>
</blockquote>
</li>
</ol>
<p>给定从某个应用程序获取的savepoint，可以从savepoint（保存点）启动相同或者兼容的应用（具体参见应用状态兼容章节）。从savepoint的位置启动意味着：应用程序的操作状态是使用持久化到savepoint中的操作状态来做初始化的。这是通过使用保存点启动应用程序来完成的。</p>
<blockquote>
<p>./bin/flink run -d -s [pathToSavepoint] ~/application.jar</p>
</blockquote>
<p>启动的应用程序的操作使用执行savepoint(保存点)时，原始应用程序(即，执行savepoint(保存点)的应用程序)的操作状态进行初始化。启动的应用程序将准确地从此状态点开始继续处理。</p>
<p><strong>注意</strong>：即便Flink始终会恢复应用的状态，它仍然不能恢复(回退)对外部系统的写操作。如果你的应用程序是基于应用程序未停止时获取的savepoint(保存点) 恢复的，这可能是一个问题。在这种情况下，应用程序在执行savepoint之后，可能又发出了很多数据，重启的应用程序（取决于你是否更改了应用程序的逻辑）可能会再次发出相同的数据。这种行为的确切影响可能会因为SinkFunction和存储系统的不同而有很大的不同。对于像幂等写入到Cassandra这样的键值存储系统来说，发送两次数据可能不会有什么问题；但是在追加数据到kafka这样的持久日志的情况下，就会存在问题。在任何情况下，你都应该仔细检查和测试重新启动的应用程序的行为。</p>
<h1 id="应用程序状态兼容性">应用程序状态兼容性</h1>
<p>为了修复bug或者改进应用程序而升级应用程序时，通常的目标都是在保留其状态的同时替换应用程序逻辑。我们通过从原始应用程序的保存点（savepoint）启动升级后的应用程序来实现。但是，这只有在两个应用程序的状态是兼容时，才是有效的。也就是说，升级后的应用程序的操作能够从原始应用程序的操作状态初始化它们的状态。<br>
本节，主要讨论如何修改应用程序以保持状态的兼容。</p>
<h2 id="匹配操作状态">匹配操作状态</h2>
<p>当应用程序基于某个savepoint重启时，Flink将存储在savepoint中的操作状态与启动应用的有状态操作进行匹配。匹配是基于操作ID，操作ID也会存储在savepoint中。根据操作在应用程序操作拓扑的位置，每个操作都有一个默认的操作ID。因此，一个未修改的应用总是可以从它自己的savepoints重启。但是，如果应用程序修改了，操作的默认ID很可能会被修改。因此，修改的应用程序只有在它的操作ID都已经显示指定的情况下，才能从savepoint中重启。给操作分配ID非常简单，使用uid(String)方法完成，如下：</p>
<pre><code class="language-scala">val mappedEvents: DataStream[(Int, Long)] = events
  .map(new MyStatefulMapFunc()).uid(&quot;mapper-1&quot;)
</code></pre>
<p><strong>注意</strong>：因为存储在savepoint中的IDs和重启的应用程序的操作IDs必须相同，因此，强烈推荐，强烈建议为将来可能升级的应用程序的所有操作符分配唯一id。此建议适用于所有的操作，即，所有显示申明或没有申明操作状态的操作，因为有的操作它的内部状态对用户而言是不可见的（比如Window操作）。升级没有分配操作ID的应用程序要困难的多，可能只能通过使用setUidHash()方法的低级解决方法实现。<br>
<strong>重要的</strong>：自1.3.X版本开始，上述也使用与操作链中的操作<br>
默认地，所有存储在savepoint中的状态都必须与重启应用的操作匹配。然而，在从某个savepoint启动应用程序时，用户可能会显示地同意跳过（丢弃）那些不能匹配上操作的状态。<strong>对于在savepoint中未查找到状态的有状态操作，它会使用默认的状态进行初始化</strong>。用户可以通过调用ExecutionConfig#disableAutoGeneratedUIDs来强制执行最佳实践，如果任何操作符不包含自定义唯一ID，那么该操作将导致作业提交失败</p>
<h2 id="有状态的操作与用户函数">有状态的操作与用户函数</h2>
<p>在升级应用时，用户函数和操作都可以自由修改，除了一条限制。<strong>也就是操作状态的数据类型不能修改</strong>。这是很重要的，因为从某个savepoint中获取的状态在加载到某个操作之前，(当前)不能转换为不同的数据类型。因此，因此，在升级应用程序时更改操作符状态的数据类型会破坏应用程序状态的一致性，并阻止从保存点重新启动升级的应用程序。<br>
操作状态可以是用户自定义状态也可以是内部状态</p>
<ul>
<li>用户定义操作状态：在包含了用户定义操作状态的函数中，状态的类型是由用户显示定义的。虽然不能更改操作符状态的数据类型，但克服这一限制的解决方案是使用不同的数据类型定义第二个状态，并实现将状态从原始状态迁移到新状态的逻辑。这种方法需要良好的迁移策略和对<strong>键分区状态行为</strong>的充分理解。</li>
<li>内部操作状态：诸如window或join操作，都维持了用户不可见的内部的操作状态。对于这些操作，内部状态的数据类型依赖于操作的输入和输出类型。因此，变更各自的输入或输出类型会破坏应用程序的一致性，并阻止升级。下表列出了具有内部状态的操作，并显示了状态数据类型如何与它的输入和输出类型相关联。对于应用于keyed Stream的操作，key的类型也时状态数据类型的一部分<br>
<img src="https://wangyemao-github.github.io/post-images/1611160060748.png" alt="" loading="lazy"></li>
</ul>
<h2 id="应用程序拓扑">应用程序拓扑</h2>
<p>除了变更一个或多个现有操作外，还可以通过变更程序拓扑来升级应用程序。即，通过增加或移除操作，变更操作的并行度，或者修改操作的链行为。<br>
通过改变程序拓扑升级应用程序时，为了保持应用状态的一致性，需要考虑如下几点：</p>
<ul>
<li><strong>增加或移除无状态的操作</strong>：这个没有问题除非下述情况之一</li>
<li><strong>增加一个有状态操作</strong>：操作的状态将被初始化为默认状态，除非它接管另一个操作的状态</li>
<li><strong>移除一个有状态的操作</strong>：被移除的操作的状态将丢失，除非被另一个操作接管。当启动升级应用程序时，您必须明确同意放弃该状态</li>
<li><strong>变更操作的输入或输出类型</strong>：当在具有内部状态的操作之前或之后添加新操作时，您必须确保有状态操作的输入或输出类型不会被修改以保持内部操作状态的数据类型不变</li>
<li><strong>变更操作链</strong>：多操作操作可以连接在一起以提高性能。自Flink1.3.X版本之后，当从savepoint恢复时，保持状态的一致性的同时修改操作链是有可能。可以中断链，将有状态操作移出链。还可以将新的或现有的有状态操作追加或注入到链中，或者修改链中的操作顺序。但是，当将基于保存点升级到1.3.x版本时，最重要的是要保证在链接方面拓扑结构没有改变。如上面的匹配操作符状态部分所述，应该为链中的所有操作符分配一个ID。</li>
</ul>
<h1 id="升级flink框架版本">升级Flink框架版本</h1>
<p>本节描述在不同版本之间升级Flink和在不同版本之间迁移作业的一般方法。<br>
简而言之，这个过程包括两个基本步骤：</p>
<ol>
<li>在前面的旧Flink版本中获取要迁移的作业的保存点</li>
<li>在新的Flink版本下从以前版本获取的保存点恢复您的job<br>
除了这两个基本步骤之外，还需要一些额外的步骤，这取决于您想要更改Flink版本的方式。在本指南中，我们区分了跨Flink版本升级的两种方法:就地升级和阴影复制升级。<br>
对于就地升级，在获取保存点之后，您需要：</li>
<li>停止/取消所有运行的作业</li>
<li>停掉旧版本的flink集群</li>
<li>升级flink到新版本集群</li>
<li>重启新版本集群<br>
对于影子拷贝，你需要：</li>
<li>在从保存点恢复之前，除了旧版本的Flink安装外，还要安装新版本的Flink</li>
<li>使用新版本的Flink从保存点恢复</li>
<li>如果一切正常，请停止并关闭旧版本的Flink群集<br>
下面，我们将首先介绍成功的工作迁移的先决条件，然后详细介绍我们前面概述的步骤。</li>
</ol>
<h2 id="前提条件">前提条件</h2>
<p>在开始迁移之前，请检查要迁移的作业是否遵循保存点的最佳实践。另外，请查看API迁移指南，看看是否有任何与将保存点迁移到新版本相关的API更改。<br>
特别地，我们建议您检查作业是否为操作设置了显式的uid。<br>
这是一个软先决条件，如果您忘记分配uid，那么恢复应该仍然有效。如果遇到行不通的情况，你可以使用setUidHash(String hash)调用，手动将以前Flink版本中生成的legacy vertex ids添加到作业中。对于每个操作(在操作链中：只有head 操作)你必须分配32位的十六进制字符串，你可以在web ui或者日志中查看，它代表了对应操作的hash值。<br>
除了操作符uid之外，目前还有两个会导致迁移失败的作业迁移的先决条件：</p>
<ol>
<li>不支持RocksDB半异步模式下的状态迁移。如果旧的作业使用这种模式，您仍然可以在savepoint之前将作业更改为使用完全异步模式。</li>
<li>另一个重要的前提条件是，所有的保存点数据必须对于从新安装Flink版本在相同的(绝对的)路径下都是可访问的。这还包括访问从保存点文件(状态后端快照的输出)中引用的任何其他文件，包括但不限于通过状态处理器API修改而引用的其他保存点。任何保存点数据目前都是由元数据文件中的绝对路径引用的，因此保存点不能通过典型的文件系统操作进行重定位</li>
</ol>
<h2 id="step1-对现有job做savepoint并停job">STEP1: 对现有job做savepoint并停job</h2>
<p>版本迁移的第一个主要步骤是获取一个保存点，并停止在旧的Flink版本上运行的作业<br>
您可以使用这个命令来做到这一点：</p>
<blockquote>
<p>$ bin/flink stop [--savepointPath :savepointPath] :jobId<br>
更多信息，参看 savepoint documentation.</p>
</blockquote>
<h2 id="step2-升级集群到新flinl版本">STEP2: 升级集群到新Flinl版本</h2>
<p>在这一步中，我们更新集群的框架版本。这基本上意味着用新版本替换Flink安装的内容。这个步骤取决于你在你的集群中运行Flink的方式(例如单机，Mesos)<br>
如果您不熟悉在集群中安装Flink，请阅读部署和集群设置文档</p>
<h2 id="step3基于savepoint在新的flink版本下恢复作业">STEP3：基于savepoint在新的Flink版本下恢复作业</h2>
<p>作为作业迁移的最后一步，从上面更新的集群上的保存点恢复。您可以使用这个命令来做到这一点：</p>
<blockquote>
<p>$ bin/flink run -s :savepointPath [:runArgs]<br>
同样，要了解更多细节，请查看savepoint文档。</p>
</blockquote>
<h2 id="compatibility-table">Compatibility Table</h2>
<p>保存点在Flink版本之间是兼容的，如下表所示:</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Upgrade Flink to 1.12 —Release Notes(翻译官网)]]></title>
        <id>https://wangyemao-github.github.io/post/upgrade-flink-to-112-release-notesfan-yi-guan-wang/</id>
        <link href="https://wangyemao-github.github.io/post/upgrade-flink-to-112-release-notesfan-yi-guan-wang/">
        </link>
        <updated>2021-01-18T01:50:01.000Z</updated>
        <summary type="html"><![CDATA[<p>如果你计划将Flink版本升级到1.12，请仔细阅读如下注意点：</p>
]]></summary>
        <content type="html"><![CDATA[<p>如果你计划将Flink版本升级到1.12，请仔细阅读如下注意点：</p>
<!-- more -->
<h1 id="已知存在的问题">已知存在的问题</h1>
<h2 id="未对齐的checkpoint-恢复可能会导致数据流损毁-flink-20654">未对齐的checkpoint 恢复可能会导致数据流损毁 FLINK-20654</h2>
<p>在Flink 1.12 版本中，同时使用<strong>未对齐的检查点(UnalignedCheckpoints) <strong>加</strong>两个/多个输入任务或单输入任务的联合输入(Union inputs)</strong> 组合情况，可能会导致状态数据损毁。<br>
<strong>发生的场景</strong>：在完全恢复完成之前触发了新的检查点就会发生这种情况。导致一个带有两个或多个输入的任务发生状态损毁的前提是：该任务必须在完成恢复溢出的飞行数据的同时接受检查点屏障。在这种情况下，新的检查点是可以执行成功的，但是因为传输中的数据已损毁/丢失，当尝试从已损毁的检查点恢复时，这将导致各种反序列化/数据流已损毁的错误。</p>
<h1 id="apis-相关变更和注意点">APIs 相关变更和注意点</h1>
<h2 id="移除了executionconfig-中的deprecated-方法-flink-19084">移除了ExecutionConfig 中的deprecated 方法 FLINK-19084</h2>
<ul>
<li>移除了过期的ExecutionConfig#isLatencyTrackingEnabled方法，替代方法：ExecutionConfig#getLatencyTrackingInterval代替</li>
<li>过期的或无效的方法被移除：ExecutionConfig#enable/disableSysoutLogging ；ExecutionConfig#set/isFailTaskOnCheckpointError</li>
<li>从cli中移除了无效的-q选项</li>
</ul>
<h2 id="移除过期的runtimecontextgetallaccumulators-方法-flink-19032">移除过期的RuntimeContext#getAllAccumulators 方法 FLINK-19032</h2>
<p>移除了RuntimeContext#getAllAccumulators方法，替代方法：RuntimeContext#getAccumulator</p>
<h2 id="因为存在数据丢失的风险设置方法过期checkpointconfigsetprefercheckpointforrecovery-flink-20441">因为存在数据丢失的风险，设置方法过期：CheckpointConfig#setPreferCheckpointForRecovery FLINK-20441</h2>
<p>因为偏好选择旧的检查点，而不是新的保存点来进行恢复可能会导致数据丢失，设置CheckpointConfig#setPreferCheckpointForRecovery为过期方法</p>
<h2 id="flip-134datastream-api中批执行相关">FLIP-134：DataStream API中批执行相关</h2>
<ol>
<li>
<p>允许显示地在KeyedStream.intervalJoin()上配置时间行为 FLINK-19479<br>
在FLINK 1.12之前，KeyedStream.intervalJoin()操作基于全局设置的流的时间特性（TimeCharacteristic)来改变行为。在Flink 1.12中，在IntervalJoin类中显示地引入了inProcessingTime() 和 inEventTime()方法，join不再基于全局的时间特性改变行为。</p>
</li>
<li>
<p><em>将DataSteam API中的timeWindow()方法设置为过期方法 FLINK-19318</em><br>
在Flink 1.12中，DataStream API中的timeWindow()方法设置为过期。请使用window(WindowAssigner) + TumblingEventTimeWindows/SlidingEventTimeWindows/TumblingProcessingTimeWindows/SlidingProcessingTimeWindows 替代。更多的信息，请查看TimeCharacteristic/setStreamTimeCharacteristic的过期描述</p>
</li>
<li>
<p><em>将StreamExecutionEnvironment.setStreamTimeCharacteristic()和TimeCharacteristic 设置为过期 FLINK-19319</em><br>
在Flink 1.12中，默认的流时间特性已经变更为Event Time，因此，你不需要再显示地调用这个方法来开启Event-time 支持。显示使用处理时间（Processing Time）窗口和计时器可以在Event-time模式下使用。如果想要禁用水印（watermark），请使用ExecutionConfig.setAutoWatermarkInterval(long)方法。如果想使用IngestionTime，请手动设置合适的WatermarkStrategy。如果使用基于时间特性（time characteristic）更改行为的通用的“time window”操作（比如，KeyedStream.timeWindow()），请使用等效的显示指定Processing time 或event Time的操作。</p>
</li>
<li>
<p>允许在CEP PatternStream中显示配置时间行为  FLINK-19326<br>
在Flink 1.12之前，CEP 操作基于全局设置的流时间特性来变更行为。在Flink 1.12中，PatternStream中引入了显示设置的inProcessingTime() 和inEventTime()方法，CEP操作不再基于全局的时间特性变更行为</p>
</li>
</ol>
<h2 id="api-清理">API 清理</h2>
<ol>
<li>移除了剩余的UdfAnalyzer 配置 FLINK-13857<br>
移除了ExecutionConfig#get/setCodeAnalysisMode方法和 SkipCodeAnalysis 类。在这个变更之前，这些类或方法也没有启作用，所以也没有必要使用</li>
<li><em>移除过期的DataStream#split FLINK-19083</em><br>
DataStream#split方法在几个版本中被标记为已弃用后被删除，替代方法：请使用Side Outputs 替换</li>
<li><em>移除了过期的DataStream#fold() 方法以及所有相关的类 FLINK-19035</em><br>
长时间弃用的（Window）DataStream#fold方法在Flink 1.12版本中已经移除。请使用其他操作比如：（Window）DataStream#reduce方法替代，该方法在分布式系统中具有更好的性能</li>
</ol>
<h2 id="extend-compositetypeserializersnapshot-to-allow-composite-serializers-to-signal-migration-based-on-outer-configuration-flink-17520">Extend CompositeTypeSerializerSnapshot to allow composite serializers to signal migration based on outer configuration FLINK-17520</h2>
<p>CompositeTypeSerializerSnapshot # isOuterSnapshotCompatible(TypeSerializer)方法已经设置为过期，为了支持新的OuterSchemaCompatibility#resolveOuterSchemaCompatibility(TypeSerializer)方法。请自行实现它。与旧方法相比，新方法允许复合序列化器基于外部模式和配置来指示状态模式迁移。</p>
<h2 id="bump-scala-macros-version-to-211-flink-19278">Bump Scala Macros Version to 2.1.1 FLINK-19278</h2>
<p>Flink现在依赖Scala Macros 2.1.1 。这也意味着不再支持(Scala&lt;&quot;2.11.11&quot;)的版本</p>
<h1 id="connectors-and-formats">Connectors and Formats</h1>
<h2 id="移除了kafka-010x-和011x-connectors-flink-19152">移除了Kafka 0.10.x 和0.11.x connectors  FLINK-19152</h2>
<p>在Flink 1.12中，移除了kafka 0.10.x 和 0.11.x connectors。请使用通用的kafka链接器（universal kafka connector），它支持所有0.10.2.x 版本之后的kafka集群版本。<br>
请参阅文档，链接如何升级Flink kafka connector 版本 https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/connectors/kafka.html</p>
<h2 id="csv序列化模式包含了行分隔符-flink-19868">Csv序列化模式包含了行分隔符 FLINK-19868</h2>
<p>csv.line-delimiter 选项已经从CSV格式中移除。因为行分隔符应该由连接器定义，而不是由格式(format)去定义。如果用户已经在以前的Flink版本中使用了这个选项，在升级到Flink 1.12时，应该alter such table to remove this option。应该没有太多用户使用了这个选项。</p>
<h2 id="升级到kafka-schema-registry-client-550-flink-18546">升级到Kafka Schema Registry Client 5.5.0 FLINK-18546</h2>
<p>flink-avro-confluent-schema-registry模块不再作为fat-jar提供。你应该将它所依赖的添加到你的fat-jar中。Sql-客户端用户可以使用 flink-sql-avro-confluent-schema-registry  fat -jar。</p>
<h2 id="将avro-version-版本从182升级到1100-flink-18192">将Avro version 版本从1.8.2升级到1.10.0 FLINK-18192</h2>
<p>flink-avro模块的Avro默认版本已经升级到1.10.如果由于某些原因，你需要使用一个旧版本（有来自Hadoop的Avro，或者你使用了基于旧版本Avro的类），请在项目中显示地降低Avro版本。<br>
注意：我们发现Avro 1.10版本的性能比1.8.2版本有所下降。如果你关心性能，并且你可以很好地使用旧版本的Avro，考虑降级Avro的版本。</p>
<h2 id="为sql-客户端打包flink-avro时创建一个uber-jar">为SQL 客户端打包flink-avro时，创建一个uber jar</h2>
<p>SQL客户端jar被重命名为flink-sql-avro-1.12.0.jar，之前为： flink-avro-1.12.0-sql-jar.jar。并且不再需要手动增加Avro依赖。</p>
<h1 id="部署">部署</h1>
<h2 id="默认log4j配置在达到100mb后滚动日志-flink-8357">默认log4j配置：在达到100MB后滚动日志  FLINK-8357</h2>
<p>默认的log4j配置已经变更：除了已有的在Flink启动时滚动日志文件外，当日志文件大小达到100MB时，也会滚动日志文件。Flink总共保存了10个日志文件，有效地将日志目录总大小限制为1GB。</p>
<h2 id="在flink的docker镜像中默认使用jemalloc-flink-19125">在Flink的docker镜像中默认使用jemalloc FLINK-19125</h2>
<p>在Flink的docker镜像中，使用jemalloc作为默认的内存分配器，以减少内存碎片问题。在docker-entrypoint.sh 脚本中设置“disable-jemalloc”，可以回滚到使用glibc内存分配器。更多细节，请参考https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/standalone/docker.html</p>
<h2 id="升级mesos版本到17-flink-19783">升级Mesos版本到1.7 FLINK-19783</h2>
<p>Mesos依赖从1.0.1升级到了1.7</p>
<h2 id="如果flink进程超时之后还没有终止会发送sigkill信号-flink-17470">如果FLINK进程超时之后还没有终止会发送SIGKILL信号  FLINK-17470</h2>
<p>在Flink 1.12版本中，变更了standalong 脚本的行为：如果SIGTERM信号没有成功地终止Flink进程，就发出SIGKILL信号</p>
<h2 id="引入了非阻塞的job提交-flink-16866">引入了非阻塞的job提交 FLINK-16866</h2>
<p>提交job的语义稍有改变。提交的调用会立即返回，此时作业处于新的初始化状态。当作业处于该状态时，诸如触发保存点或检索完整作业详细信息等操作不可用<br>
一旦job的JobManager已经创建，该job处于created状态，所有的调用都是可用的</p>
<h1 id="运行时runtime">运行时Runtime</h1>
<h2 id="flip-141intra-slot-managed-memory-sharing-槽内托管的内存共享">FLIP-141：Intra-Slot Managed Memory Sharing （槽内托管的内存共享）</h2>
<p>配置项：python.fn-execution.buffer.memory.size和python.fn-execution.framework.memory.size 已经移除，因此不再产生具体作用。python.fn-execution.memory.managed的默认值变更为True，因此托管的内存将默认被Python workers 使用。当同时使用Python udf 和 RocksDB状态后端时，或者在批处理中使用内存批处理算法时，用户可以通过覆盖“managed memory consumer weights”，以控制在数据处理和Python之间如何共享托管内存（RocksDB状态后端或者批处理算法）。</p>
<h2 id="flip-119-pipeline-region-schedling-flink-16430">FLIP-119 Pipeline Region Schedling FLINK-16430</h2>
<p>从Flink 1.12 开始，jobs将以pipelined regions 为单元进行调度。<strong>a pipelined region</strong> 就是一个流水线任务集。这就意味着，对于由多个pipelined regions组成的流作业，不再等待所有的任务获得slot之后才开始部署任务。相反，一旦某个region包含的任务获得了足够的slot，就可以进行部署。对于批作业，将不会为任务分配slots，也不会单独部署任务；相反，某个region获得了足够的slots，在该region中的所有task将一起部署。<br>
使用jobmanager.scheduler.scheduling-strategy: legacy 可以启用原有调度。</p>
<h1 id="flink-112-apache-kafka-connector">Flink 1.12 Apache Kafka Connector</h1>
<p>universal kafka connector 向后兼容kafka 0.10.0及之后的客户端版本。<br>
依赖：</p>
<blockquote>
<dependency>
>	<groupId>org.apache.flink</groupId>
>	<artifactId>flink-connector-kafka_2.11</artifactId>
>	<version>1.12.0</version>
</dependency>
</blockquote>
<h2 id="kafka-consumer">kafka consumer</h2>
<p>使用同之前版本：</p>
<pre><code class="language-java">Properties properties = new Properties();
properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);
properties.setProperty(&quot;group.id&quot;, &quot;test&quot;);
DataStream&lt;String&gt; stream = env
	.addSource(new FlinkKafkaConsumer&lt;&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties));
</code></pre>
<p>###配置如何确定分区的起始位置----同之前版本</p>
<pre><code class="language-java">final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

FlinkKafkaConsumer&lt;String&gt; myConsumer = new FlinkKafkaConsumer&lt;&gt;(...);
myConsumer.setStartFromEarliest();     // start from the earliest record possible
myConsumer.setStartFromLatest();       // start from the latest record
myConsumer.setStartFromTimestamp(...); // start from specified epoch timestamp (milliseconds)
myConsumer.setStartFromGroupOffsets(); // the default behaviour

DataStream&lt;String&gt; stream = env.addSource(myConsumer);
</code></pre>
<ul>
<li>默认setStartFromGroupOffsets: 基于consumer group（由consumer属性中group.id指定）提交到kafka brokers的offsets开始读取partition数据。如果从某个partition没有找到对应的offsets，将基于配置的属性：auto.offset.reset 来读取数据。</li>
<li>setStartFromEarliest() / setStartFromLatest() ： 从最早或最新位置开始读取。在这种模式下提交到kafka的offset将被忽略，不会作为起始位置。如果offsets超出了partition的范围，将根据配置属性：auto.offset.reset 来读取数据</li>
<li>setStartFromTimestamp(long)：从指定的timestamp开始读取数据。对于每一个partition，时间戳等于或大于指定时间的记录将用作开始位置。如果某个partition的最新记录早于指定的时间戳，分区将简单地从最新数据开始读取。在这种模式下，提交给kafka的offset将会被忽略，不会作为起始位置。</li>
<li>指定每个partition的准确offsets.</li>
</ul>
<pre><code class="language-java">Map&lt;KafkaTopicPartition, Long&gt; specificStartOffsets = new HashMap&lt;&gt;();
specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 0), 23L);
specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 1), 31L);
specificStartOffsets.put(new KafkaTopicPartition(&quot;myTopic&quot;, 2), 43L);

myConsumer.setStartFromSpecificOffsets(specificStartOffsets);
</code></pre>
<p>注意：如果consumer读取的分区在提供的偏移量映射中没有指定偏移量，该分区的起始位置策略将回退到默认的group offsets行为（setStartFromGroupOffsets）</p>
<p><strong>注意：</strong><br>
<em>当作业从故障中自动恢复或使用保存点手动恢复时，这些起始位置配置将不会起作用。在恢复时，每个kafka 分区的起始位置由存储在保存点或检查点的偏移量决定</em></p>
<h3 id="kafka-consumer及容错">kafka consumer及容错</h3>
<p>启用Flink checkpoint，Flink kafka consumer消费topic的记录的同时会定期checkpoint kafka偏移量，以及其他的操作状态。在作业失败时，Flink将流程序恢复到最新的检查点状态，并从存储在检查点的偏移量位置开始消费kafka中的记录。<br>
检查点的间隔定义了程序在失败的情况下，最多回退消费的数据量。为了使用容错的kafka consumer，需要在作业中开启拓扑checkpoint。<br>
如果不开启checkpoint，kafka consumer经周期提交offsets给zookeeper。<br>
<strong>Checkpointing 配置</strong>：</p>
<table>
<thead>
<tr>
<th>key</th>
<th>default</th>
<th>type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>execution.checkpointing.externalized-checkpoint-retention</td>
<td>none</td>
<td>枚举值<br>可能取值：DELETE_ON_CANCELLATION，RETAIN_ON_CANCELLATION</td>
<td>外部化的检查点将其元数据写到外部持久化存储中，在拥有的作业失败或被挂起（以作业状态JobStatus＃FAILED或JobStatus＃SUSPENDED终止）时，不会自动清除。在这种情况下，您必须手动清除检查点状态 ，包括元数据以及具体程序状态数据.<br>该模式定义了在作业取消时应该如何清除外部检查点。 如果您选择在“取消时保留外部检查点”，则在取消作业（以作业状态JobStatus＃CANCELED终止）时也必须手动处理检查点清除.<br> 外部检查点目录通过：state.checkpoints.dir配置</td>
</tr>
<tr>
<td>execution.checkpointing.interval</td>
<td>none</td>
<td>Duration</td>
<td>checkpoint 周期调度的间隔<br>该设置是checkpoint的基本间隔，checkpoint的触发可能会受：execution.checkpointing.max-concurrent-checkpoints和execution.checkpointing.min-pause影响而延迟</td>
</tr>
<tr>
<td>execution.checkpointing.min-pause</td>
<td>1</td>
<td>Integer</td>
<td>可能同时进行的最大检查点尝试次数。如果该值为n，则当当前有n个检查点尝试时，不会触发任何检查点。要触发下一个检查点，一次检查点尝试需要完成或过期</td>
</tr>
<tr>
<td>execution.checkpointing.min-pause</td>
<td>0ms</td>
<td>Duration</td>
<td>checkpoint的最小间隔。相对于并发点指定的最大并发数量，次设置定义了在可能触发另一个检查点之后，检查点协调器多久可以触发两一个检查点</td>
</tr>
<tr>
<td>execution.checkpointing.mode</td>
<td>EXACTLY_ONCE</td>
<td>枚举值<br>可能的取值:<br> EXACTLY_ONCE, AT_LEAST_ONCE</td>
<td>checkpoint 模式</td>
</tr>
<tr>
<td>execution.checkpointing.prefer-checkpoint-for-recovery</td>
<td>false</td>
<td>Boolean</td>
<td>如果开启，那job恢复时，将从最近的检查点恢复，而不是更新的savepoint</td>
</tr>
<tr>
<td>execution.checkpointing.timeout</td>
<td>10min</td>
<td>Duration</td>
<td>checkpoint被丢弃之前可以执行的最大时间</td>
</tr>
<tr>
<td>execution.checkpointing.tolerable-failed-checkpoints</td>
<td>none</td>
<td>Integer</td>
<td>容忍失败的checkpoint数。如果设置为0，表示不能容忍任何的checkpoint失败</td>
</tr>
<tr>
<td>execution.checkpointing.unaligned</td>
<td>false</td>
<td>Boolean</td>
<td>开启未对齐的checkpoints，这在背压的情况下能极大减少checkpoint时间.<br>未对齐的检查点将存储在buffers的数据作为状态的一部分，这将允许检查点屏障越过这些buffer数据。因此，检查点的持续时间将与当前的吞吐量无关，因为嵌入到数据流中的检查点屏障将不再起到实质作用<br>未对齐的检查点仅在如下情况下可以开启：execution.checkpointing.mode是EXACTLY_ONCE 并且 execution.checkpointing.max-concurrent-checkpoints=1</td>
</tr>
</tbody>
</table>
<h3 id="kafka-consumer-topic-and-partition-discovery">kafka Consumer topic and partition discovery</h3>
<ul>
<li>Partition discovery<br>
Flink Kafka Consumer 支持发现动态创建的kafka分区，并提供exactly-once 保证。默认，<strong>partition discovery</strong> 未开启，通过在提供的属性配置中，设置flink.partition-discovery.interval-millis为非负值开启此功能。单位：毫秒</li>
<li>topic discovery<br>
Kafka消费者还能够通过使用正则表达式匹配主题名称来发现</li>
</ul>
<pre><code class="language-java">    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

Properties properties = new Properties();
properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);
properties.setProperty(&quot;group.id&quot;, &quot;test&quot;);

FlinkKafkaConsumer&lt;String&gt; myConsumer = new FlinkKafkaConsumer&lt;&gt;(
    java.util.regex.Pattern.compile(&quot;test-topic-[0-9]&quot;),
    new SimpleStringSchema(),
    properties);

DataStream&lt;String&gt; stream = env.addSource(myConsumer);
</code></pre>
<p>为了允许consumer在作业开始运行后发现动态创建的主题，请为flink.partition-discovery.interval-millis设置一个非负值。这使consumer可以发现名称匹配指定模式的新主题的分区。</p>
<h3 id="kafka-consumer-offset-提交行为配置">kafka consumer offset 提交行为配置</h3>
<p>Flink kafka consumer支持配置offset如何提交给kafka broker。注意：flink kafka consumer并不依赖于提交给kafka broker的offset来进行容错。提交的offset仅为了监控的目的暴露消费进度的一种途径。<br>
依赖于是否启动了检查点，配置offset提交行为的方法不同：</p>
<ul>
<li>未开启checkpoint：在没有开启检查点时，flink kafka consumer依赖于内部使用的kafka客户端自动的定期offset提交功能。因此，在提供的属性配置中简单地设置enable.auto.commit / auto.commit.interval.ms 键的合适值，就能开启/关闭offset的提交。</li>
<li>开启checkpoint：如果开启检查点，在checkpoint完成之后，flink kafka consumer将保存在检查点状态中的offsets提交给kafka。这将确保提交给kafka broker的offset与保存在检查点状态中的offset是一致的。用户可以通过调用setCommitOffsetsOnCheckpoints(boolean)方法来选择开启或关闭offset的提交。默认为true。注意：在这种情况下，属性中配置的自动周期offset提交设置将完全被忽略。</li>
</ul>
<h3 id="kafka-consumers-and-timestamp-extractionwatermark-emission">kafka consumers and TimeStamp Extraction/WaterMark Emission</h3>
<p>在大多数场景下，记录的时间戳是嵌入在数据本身，或者ConsumerRecord元数据中的。另外，用户可能想要周期性地或者不定期地发出水印。比如：基于kafka流中的特殊记录，这些记录包含了时间时间水印。对于这些情况，Flink kafka consumer 可以指定<strong>WaterMark Strategy</strong>.<br>
可以按如下自定义WaterMark Strategy，也可以使用预定义的：</p>
<pre><code class="language-java">Properties properties = new Properties();
properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);
properties.setProperty(&quot;group.id&quot;, &quot;test&quot;);

FlinkKafkaConsumer&lt;String&gt; myConsumer =
    new FlinkKafkaConsumer&lt;&gt;(&quot;topic&quot;, new SimpleStringSchema(), properties);
myConsumer.assignTimestampsAndWatermarks(
    WatermarkStrategy.
        .forBoundedOutOfOrderness(Duration.ofSeconds(20)));

DataStream&lt;String&gt; stream = env.addSource(myConsumer);
</code></pre>
<p>如果一个水印分配器依赖于从Kafka读取的记录来提升其水印(这是通常的情况)，那么所有的主题和分区都需要有连续的记录流。否则，整个应用程序的水印无法前进，所有基于时间的操作(如时间窗口或带有计时器的函数)都无法前进。单个空闲的Kafka分区会导致这种行为。考虑设置适当的idness超时来缓解这个问题</p>
<p>Kafka Producer<br>
Flink的kafka producer—FlinkKafkaProducer允许将流记录写到一个或多个kafka topic中。<br>
构造方法接受如下参数：<br>
(1)事件需要写入的默认topic<br>
(2)SerializationSchema/KafkaSerializationSchema用于将数据序列化到kafka<br>
(3) kafka client 配置，如下配置项是必须的：bootstrap.servers<br>
(4)默认的容错语义<br>
<img src="https://wangyemao-github.github.io/post-images/1611501777416.png" alt="" loading="lazy"></p>
<p>The SerializationSchema<br>
Flink kafka Producer需要知道如何将Java/Scala 对象转化为二进制数据。<strong>KafkaSerializationSchema</strong>允许用户去指定这样一个模式。每一条记录都会调用方法：ProducerRecord&lt;byte[], byte[]&gt; serialize(T element, @Nullable Long timestamp) ；生成一个ProducerRecord，写到kafka。<br>
这让用户可以细粒度地控制数据如何写入kafka，通过producer record，你可以：<br>
（1）设置header values<br>
（2）为每一条记录定义keys<br>
（3）指定自定义数据分区</p>
<p>Kafka Producers 以及容错<br>
在开启Fink checkpoint的情况下，FlinkKafkaProducer可以提供exactly-once 传送保证。<br>
除了启用FlinkKafkaProducer的检查点之外，您还可以通过向FlinkKafkaProducer传递适当的语义参数来选择三种不同的操作模式：<br>
Semantic.NONE：Flink不会做任何保证。Producer records可能会丢失，也可能会重复<br>
Semantic.AT_LEAST_ONCE（默认设置）：保证没有记录会丢失，但是可能会重复<br>
Semantic.Exactly_ONCE: kafka事务将用于提供exactly-once 语义。当你使用事务写入Kafka时，对于从kafka消费数据的任何应用程序都不要忘记设置期望的隔离级别（read_committed or read_uncommitted，默认为后者）</p>
<p>注意事项：<br>
Semantic.Exactly_Once 依赖于commit transaction 功能，commit transanction从检查点恢复之后，执行下一次checkpoint之前启动。如果Flink应用程序崩溃到完成重启之间的时间大于Kafka的事务超时时间，则将丢失数据（Kafka将自动中止超过超时时间的事务）。 考虑到这一点，请根据您的预期停机时间适当配置事务超时。<br>
默认，kafka broker将transaction.max.timeout.ms设置为15分钟。该属性不允许producer设置大于该值的事务超时时间。默认情况下，FlinkKafkaProducer将生产者配置中的transaction.timeout.ms属性设置为1小时，因此使用Semantic.EXACTLY_ONCE模式之前，应先增大transaction.max.timeout.ms的设置。<br>
在kafkaConsumer的read_committed模式下，任何未完成的事务（既未中止也未完成）将阻止来自给定kafka topic的所有未完成事务的读取。话句话说，在以下事件序列之后：<br>
（1）用户启动transaction 1，写入一些记录<br>
（2）用户启动transaction 2，写入一些其他记录<br>
（3）用户提交了transaction 2<br>
即便transaction 2中的记录已经提交，但是直到transaction 1 被提交或中止之前，它们对用户仍旧是不可见的。这包含了两层含义：</p>
<ol>
<li>首先，在Flink应用程序的正常工作中，用户可以预计生产到kafka topic的记录的可见性延迟，该延迟等于完成checkpoint之间的平均时间。</li>
<li>其次，在Flink应用程序失败的情况下，应用正在写入的记录对用户不具有可见性。直到应用程序重新启动或经过到达配置的事务超时时间为止。此注释仅适用于有多个代理/应用程序写入同一个Kafka主题的情况。</li>
</ol>
<p><strong>注意</strong> ：Semantic.EXACTLY_ONCE模式为每个FlinkKafkaProducer实例使用固定大小的KafkaProducer池。 每个检查点使用这些生产者中的每一个。 如果并发检查点的数量超过池大小，则FlinkKafkaProducer将引发异常，并使整个应用程序失败。 请相应地配置最大池大小和最大并发检查点数。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flink 时间操作—Window]]></title>
        <id>https://wangyemao-github.github.io/post/flink-shi-jian-cao-zuo-window/</id>
        <link href="https://wangyemao-github.github.io/post/flink-shi-jian-cao-zuo-window/">
        </link>
        <updated>2021-01-17T06:51:32.000Z</updated>
        <summary type="html"><![CDATA[<p>本文主要讲解Flink中window相关设计、原理和应用。</p>
]]></summary>
        <content type="html"><![CDATA[<p>本文主要讲解Flink中window相关设计、原理和应用。</p>
<!-- more -->
<h1 id="flink窗口的概述及分类">Flink窗口的概述及分类</h1>
<p>Flink<br>
在Google的Dataflow编程模型论文《The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing》中介绍了该模型流批一体计算中的核心设计，其中重点是Window操作。流处理的本质是处理无限持续产生的数据集，批处理的本质是处理有限不变的数据集，因此，批处理实际上是流处理的一种特列，而窗口就是流和批一体的桥梁。<br>
可见，Window操作就是用来将一个无限的流切分为一个个有限的数据集，并在有界数据集上进行操作的一种机制。Flink中提供了3类默认的窗口：计数窗口(Count Window)、时间窗口（Time Window）和会话窗口（Session Window）。</p>
<ol>
<li><strong>Count Window</strong>
<ul>
<li><strong>Tumble Count Window</strong> : 累计固定个数的元素就视为一个窗口，该类型窗口无法像时间窗口一样实现切分好</li>
<li><strong>Sliding Count Window</strong> ：累计固定的个数视为一个窗口，每超过一定个数的滑动个数，则产生一个新的窗口</li>
</ul>
</li>
<li><strong>Time Window</strong>
<ul>
<li><strong>Tumble Time Window</strong> : 在时间上按照事先约定的窗口大小切分的窗口，窗口之间不会相互重叠</li>
<li><strong>Sliding Time Window</strong> ：在时间上按照事先约定的窗口大小、滑动步长切分的窗口，滑动窗口之间可能会存在相互重叠的情况</li>
</ul>
</li>
<li><strong>Session Window</strong><br>
一种特殊的窗口，当超过一段时间，该窗口没有收到新的数据元素，则视为该窗口结束。因此，无法事先确定窗口的长度、元素个数，窗口之间也不会相互重叠</li>
</ol>
<!--  画图说明 几种window的区别   -->
<h1 id="flink-window的实现原理与机制">Flink Window的实现原理与机制</h1>
<p>数据进入窗口之后，如何分配窗口，如何驱逐数据，以及如何触发计算的逻辑如下图所示：<br>
<img src="https://wangyemao-github.github.io/post-images/1610883497379.png" alt="" loading="lazy"><br>
数据流中每一个数据进入Window算子时，</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flink Time]]></title>
        <id>https://wangyemao-github.github.io/post/flink-time/</id>
        <link href="https://wangyemao-github.github.io/post/flink-time/">
        </link>
        <updated>2021-01-13T01:47:03.000Z</updated>
        <summary type="html"><![CDATA[<p>在Flink中有状态流式处理中，Time承担着一定的作用。比如：进行时间序列的分析，基于特定时间段的聚合（window操作），以及基于发生时间的事件处理等。本文主要讲解Flink中时间概念，及针对不同的时间特性，Flink中提取时间戳及生成水印的策略</p>
]]></summary>
        <content type="html"><![CDATA[<p>在Flink中有状态流式处理中，Time承担着一定的作用。比如：进行时间序列的分析，基于特定时间段的聚合（window操作），以及基于发生时间的事件处理等。本文主要讲解Flink中时间概念，及针对不同的时间特性，Flink中提取时间戳及生成水印的策略</p>
<!-- more -->
<h1 id="flink-time">Flink Time</h1>
<h2 id="time-notition">Time notition</h2>
<p>Flink流程序中支持多种不同的时间概念，包括：Processing Time，Event Time 和 Ingestion Time。下面为官网的说明图及解释：</p>
<figure data-type="image" tabindex="1"><img src="https://wangyemao-github.github.io/post-images/1610709415381.png" alt="" loading="lazy"></figure>
<ul>
<li>Processing Time<br>
Processing Time 是事件在执行相关操作时，所在机器的系统时间。当流程序在Processing Time上运行时，所有基于时间(比如windows操作)都将使用当时机器的系统时间。每小时处理时间窗口将包括在系统时钟指示整小时之间到达特定操作的所有记录。例如：如果应用程序在上午9:15开始运行，那第一个小时Processing Time 窗口将包括从上午9:15到上午10:00的事件；下一个窗口包含从上午10:00到11:00的事件。<br>
Processing Time是最简单的时间概念。它不需要流和机器之间的协调，并且提供了最好的性能及最低的延迟。但是，由于会受到事件到达系统的速度，以及事件在系统内操作流动速度的影响，在分布式和异步环境下，基于Processing Time的流程序不能提供确定性的结果</li>
<li>Event Time<br>
Event Time 是指事件的发生时间，通常事件到达Flink之前本身就携带了这个时间。在基于事件时间的流程序中，时间取决于数据，跟机器系统时间无关。使用Event Time，需要指定如何提取事件时间，以及如何生成Event Time watermark。<br>
在理想情况下，无论事件什么时候到达，无论事件如何乱序到达，基于Event Time的流程序总会得到一致且确定的结果。但事实上，除非事件按照既定的顺序到达，否则流程序会因为需要等待一些无序的事件而产生延迟。由于只能等待一段有限的时间，因此很难保障基于Event Time的流程序总是得到一致且确定性的结果。</li>
<li>Ingestion Time<br>
Ingestion Time 是事件进入Flink 的时间。在源操作处，每个事件将当前机器的系统时间作为时间戳，后续的所有基于时间的操作(window操作)都将使用这个时间戳。<br>
<strong>与另外两个时间的对比</strong></li>
</ul>
<ol>
<li>与Event Time 相比，Ingestion Time 无法处理任何无序或延迟事件，因为时间都是在进入Flink源时，统一分配的机器系统时间，事件的顺序已被重新定义；但是使用Ingestion Time的流程序由Flink自动分配时间戳，自动生成水印，不需要用户代码额外指定</li>
<li>与Processing Time 相比，Ingestion Time稍微更贵一些，但结果更可预测。</li>
</ol>
<h2 id="设定时间特性">设定时间特性</h2>
<p>下面的代码片段为在Flink中设定不同时间特性的：</p>
<pre><code class="language-java">final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
//设置时间特性为Processing Time
env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); 
//设置时间特性为Ingestion Time
env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);
//设置时间特性为Event Time
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
</code></pre>
<h1 id="watermarks">WaterMarks</h1>
<p>支持Event Time的流程序中，由于各种原因，不可能完全保证事件都按照既定的顺序到达，通常事件达到都是无序的并且存在延迟。那基于时间的操作，比如window操作就需要一种方法来衡量时间的进度。在Flink中这种衡量Event Time进度的机制就称作<strong>WaterMarks</strong>。<br>
在Flink中，watermarks作为数据流的一部分在流程序中流动，并携带了一个时间戳<strong>t</strong>。<strong>watermark(t)<strong>表明所有时间戳&lt;=t的事件都已经到达。<br>
下图为有序事件流和无序事件流中watermark：<br>
<img src="https://wangyemao-github.github.io/post-images/1610506420248.png" alt="" loading="lazy"><br>
<img src="https://wangyemao-github.github.io/post-images/1610506426262.png" alt="" loading="lazy"><br>
在有序事件流中，watermark只是事件流中简单地周期性标记。<br>
在无序事件流中，watermark充当着很重要的作用。一旦</strong>watermark(t)<strong>到达某个操作，操作将使用watermark携带的时间戳更新内部</strong>event time clock</strong>。当满足特定条件时触发计算<br>
设置不同的时间特性，watermark是否需要用户程序设定如下表：</p>
<table>
<thead>
<tr>
<th></th>
<th>Processing Time</th>
<th>Event Time</th>
<th>Ingestion Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>watermark</td>
<td>不需要</td>
<td>用户程序指定</td>
<td>系统自动生成</td>
</tr>
</tbody>
</table>
<h1 id="time-characteristic-watermark-生成-源码分析">Time Characteristic &amp; watermark 生成 源码分析</h1>
<p>在Flink流程序中，当指定时间特性为Ingestion Time之后，在源操作处，对于到来的每个事件，将使用当时的机器系统时间作为事件时间戳并自动生成水印。<br>
在Flink中，源操作处如何发送数据（提取时间戳）、如何生成水印都是由<strong>SourceFunction.SourceContext</strong>接口定义的</p>
<pre><code class="language-java">public interface SourceFunction&lt;T&gt; extends Function, Serializable {
    /**
     *  Interface that source functions use to emit elements, and possibly watermarks   
     */
    interface SourceContext&lt;T&gt; {
        //Emits one element from the source, without attaching a timestamp
        void collect(T element);
        // Emits one element from the source, and attaches the given timestamp
        void collectWithTimestamp(T element, long timestamp);
        //Emits the given {@link Watermark}
        void emitWatermark(Watermark mark);
    }
}
</code></pre>
<p>针对不同的时间特性(Time Characteristic)，StreamSourceContext定义了三种不同的SourceContext实现。NonTimeStampContext、AutomaticWatermarkContext、ManualWatermarkContext。其中后两者都继承自WatermarkContext。</p>
<pre><code class="language-java">//Source contexts for various stream time characteristics
public class StreamSourceContexts {
    /**
     * Depending on the {@link TimeCharacteristic}, this method will return
     * the adequate {@link    org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext}
    **/ 
    public static &lt;OUT&gt; SourceFunction.SourceContext&lt;OUT&gt; getSourceContext(
			TimeCharacteristic timeCharacteristic,
			ProcessingTimeService processingTimeService,
			Object checkpointLock,
			StreamStatusMaintainer streamStatusMaintainer,
			Output&lt;StreamRecord&lt;OUT&gt;&gt; output,
			long watermarkInterval,
			long idleTimeout){
                final SourceFunction.SourceContext&lt;OUT&gt; ctx;
                switch (timeCharacteristic) {
			case EventTime:
				ctx = new ManualWatermarkContext&lt;&gt;(
					output,
					processingTimeService,
					checkpointLock,
					streamStatusMaintainer,
					idleTimeout);
				break;
			case IngestionTime:
				ctx = new AutomaticWatermarkContext&lt;&gt;(
					output,
					watermarkInterval,
					processingTimeService,
					checkpointLock,
					streamStatusMaintainer,
					idleTimeout);
				break;
			case ProcessingTime:
				ctx = new NonTimestampContext&lt;&gt;(checkpointLock, output);
				break;
			default:
				throw new IllegalArgumentException(String.valueOf(timeCharacteristic));
            }
}
</code></pre>
<p>从上面的代码片段中可见，不同的时间特性（TimeCharacteristic）会实现不同SourceContext，其对应关系为：ProcessTime对应NonTimestampContext；EventTime对应ManualWatermarkContext；IngestionTime对应AutomaticWatermarkContext。</p>
<h2 id="processingtime-时间戳-和水印生成">ProcessingTime &amp; 时间戳 和水印生成</h2>
<p>对于指定Processing Time 情况，时间戳与水印如何生成，具体看<strong>NonTimestampContext</strong>实现：</p>
<pre><code class="language-java">/**
 *  A source context that attached {@code -1} as a timestamp to all records,   *   and that does not forward watermarks
 **/
private static class NonTimestampContext&lt;T&gt; implements SourceFunction.SourceContext&lt;T&gt;{
    public void collect(T element) {
			synchronized (lock) {
				output.collect(reuse.replace(element));
			}
	}
    public void collectWithTimestamp(T element, long timestamp) {
			// ignore the timestamp
			collect(element);
	}
    public void emitWatermark(Watermark mark) {
			// do nothing
	}
}
</code></pre>
<p>从上面代码可见，NonTimestampContext没有为事件生成时间戳，对于传入时间戳的情况，也会将时间戳忽略。另外，发送水印方法为空实现，即不会向下游发送水印(Watermark)。<br>
事实上，对于使用ProcessingTime情况，生成时间戳和向下游发送水印并没有实际意义。因为，对于时间相关的操作(window类操作)，各个计算节点会根据机器的系统时间定义触发器，触发计算，而不是根据watermark来触发。</p>
<h2 id="ingestiontime-时间戳-和水印生成">IngestionTime &amp; 时间戳 和水印生成</h2>
<p>对于IngestionTime情况，时间戳生成及水印生成具体看<strong>AutomaticWatermarkContext</strong>实现。另外前面说到AutomaticWatermarkContext和ManualWatermarkContext都继承自WatermarkContext。此处首先会大概说明下<strong>WatermarkContext</strong>具体实现了什么功能：<br>
WatermarkContext定义了与watermark相关的行为(因为不是本文的重点，此处简要说明)：</p>
<ul>
<li>负责管理当前的StreamStatus，确保StreamStatus向下游传递；</li>
<li>负责空闲监测逻辑，当超过设定的时间间隔还没有收到数据或者watermark时，会认为Task处于空闲状态。空闲监测逻辑是很有意义上的：如果某个源操作Task标志为空闲状态，后续的Checkpoint中不需要再等待该Task barrier对齐，不需要耗费资源或时间等待</li>
</ul>
<h3 id="如何生成事件时间戳">如何生成事件时间戳</h3>
<p>回到AutomaticWatermarkContext，我们看下在IngestionTime情况，系统是如何自动赋值时间戳，如何自动生成watermark，首先看下时间戳：</p>
<pre><code class="language-java">protected void processAndCollect(T element) {
			lastRecordTime = this.timeService.getCurrentProcessingTime();
			output.collect(reuse.replace(element, lastRecordTime));

			// this is to avoid lock contention in the lockingObject by
			// sending the watermark before the firing of the watermark
			// emission task.
			if (lastRecordTime &gt; nextWatermarkTime) {
				// in case we jumped some watermarks, recompute the next watermark time
				final long watermarkTime = lastRecordTime - (lastRecordTime % watermarkInterval);
				nextWatermarkTime = watermarkTime + watermarkInterval;
				output.emitWatermark(new Watermark(watermarkTime));

				// we do not need to register another timer here
				// because the emitting task will do so.
			}
	}

    protected void processAndCollectWithTimestamp(T element, long timestamp) {
			processAndCollect(element);
	}
</code></pre>
<p>两个向下游发送数据的方法最终都是执行都是上面的processAndCollect(element)方法。对于每一条数据记录，会取当前的ProcessingTime(机器系统时间)作为事件时间戳，然后发送到下游。</p>
<h3 id="如何生成水印">如何生成水印</h3>
<p>在AutomaticWatermarkContext的构造方法中会初始化两个重要的属性：</p>
<pre><code class="language-java">private AutomaticWatermarkContext(
				final Output&lt;StreamRecord&lt;T&gt;&gt; output,
				final long watermarkInterval,
				final ProcessingTimeService timeService,
				final Object checkpointLock,
				final StreamStatusMaintainer streamStatusMaintainer,
				final long idleTimeout) {
			super(timeService, checkpointLock, streamStatusMaintainer, idleTimeout);
            
			this.watermarkInterval = watermarkInterval;
			long now = this.timeService.getCurrentProcessingTime();
			this.nextWatermarkTimer = this.timeService.registerTimer(now + watermarkInterval,
				new WatermarkEmittingTask(this.timeService, checkpointLock, output));
		}
</code></pre>
<p>第一个属性是<strong>watermarkInterval</strong>，它指定了水印(watermark)的生成周期，具体的取值为：</p>
<pre><code class="language-java">watermarkInterval = getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(); 
</code></pre>
<p>默认情况下，生成周期为200ms，当然可以调用ExecutionConfig#setAutoWatermarkInterval（long interval）重新指定<br>
第二个重要的属性是<strong>nextWatermarkTimer</strong>，它是一个ProcessingTimeService定时器，当<strong>ProceingTime = now + watermarkInterval</strong>时，也就是间隔<strong>watermarkInterval</strong>时间之后，会触发回调<strong>WatermarkEmittingTask#onProcessingTime</strong>方法，下面为从WatermarkEmittingTask中摘取的重要代码片段：</p>
<pre><code class="language-java"> private class WatermarkEmittingTask implements ProcessingTimeCallback{

     public void onProcessingTime(long timestamp) {
				final long currentTime = timeService.getCurrentProcessingTime();
                synchronized (lock) {
					// we should continue to automatically emit watermarks if we are active
					if (streamStatusMaintainer.getStreamStatus().isActive()) {
						if (idleTimeout != -1 &amp;&amp; currentTime - lastRecordTime &gt; idleTimeout) {
						} else if (currentTime &gt; nextWatermarkTime) {
							// align the watermarks across all machines. this will ensure that we don't have watermarks that creep along at different intervals because the machine clocks are out of sync 
							final long watermarkTime = currentTime - (currentTime % watermarkInterval);
							output.emitWatermark(new Watermark(watermarkTime));
							nextWatermarkTime = watermarkTime + watermarkInterval;
						}
					}
				}
				long nextWatermark = currentTime + watermarkInterval;
				nextWatermarkTimer = this.timeService.registerTimer(
						nextWatermark, new WatermarkEmittingTask(this.timeService, lock, output));
			}
 }
</code></pre>
<p>上面代码片段中可以看到：（1）在对水印时间进行矫正之后，会将水印时间发送到下游；（2）注册下一次触发时间。也就是说，在<strong>AutomaticWatermarkContext</strong>中使用一个定时器，每当注册触发时间到来时，会自动向下游发送水印，并且会持续地自动注册下一次触发时间。<strong>触发时间为：（作业启动时刻+watermark周期*n）</strong></p>
<h2 id="event-time-时间戳和水印生成">Event Time &amp; 时间戳和水印生成</h2>
<p>最后，我们看一下基于Event Time的流程序，在源操作处，是如何处理事件时间戳和水印生成的。具体的实现在<strong>ManualWatermarkContext</strong>：</p>
<pre><code class="language-java">private static class ManualWatermarkContext&lt;T&gt; extends WatermarkContext&lt;T&gt;{
    protected void processAndCollectWithTimestamp(T element, long timestamp) {
			output.collect(reuse.replace(element, timestamp));
	}
    protected void processAndEmitWatermark(Watermark mark) {
			output.emitWatermark(mark);
	}
    protected boolean allowWatermark(Watermark mark) {
			return true;
	}
}
</code></pre>
<p>从上面代码片段中可以看出：在使用Event Time的情况下，ManualWatermarkContext不会额外生成时间戳，也不会生成watermark。对于数据和水印都只做透传。<br>
那么，这种情况下透传的时间戳和水印是从哪里来的呢？答案就是UDF(User Defined Function)。具体可以去看下<strong>StreamSource#run</strong>方法：在run方法中根据设置的时间特性创建StreamSourceContexts 之后，会将创建的方法：在run方法中根据设置的时间特性创建StreamSourceContexts传给userFunction：</p>
<pre><code class="language-java">this.ctx = StreamSourceContexts.getSourceContext(
			timeCharacteristic,
			getProcessingTimeService(),
			lockingObject,
			streamStatusMaintainer,
			collector,
			watermarkInterval,
			-1);
userFunction.run(ctx);
</code></pre>
<p>时间戳提取及水印的生成在用户定义的SourceFunction中完成之后，到StreamSourceContexts时，只做数据的透传。</p>
<h2 id="flink-kafka-source-时间戳和水印生成">Flink Kafka Source 时间戳和水印生成</h2>
<p>本节以Flink-source-connector中KafkaConsumer 为例，说明在设置不同时间特性情况下，时间戳和水印如何生成。基于Flink1.8版本，kafka-010。<br>
对于消费kafka的每条记录会调用emitRecord()方法发出数据，该方法中会进行时间戳的提取和水印生成：</p>
<pre><code class="language-java">//emit the actual record. this also updates offset state atomically
//and deals with timestamps and watermark generation
protected void emitRecord(
			T record,
			KafkaTopicPartitionState&lt;TopicPartition&gt; partition,
			long offset,
			ConsumerRecord&lt;?, ?&gt; consumerRecord) throws Exception {
		// we attach the Kafka 0.10 timestamp here
		emitRecordWithTimestamp(record, partition, offset, consumerRecord.timestamp());
}
</code></pre>
<p>在emitRecord()方法中调用了emitRecordWithTimestamp()方法，并且传入的时间戳为：consumerRecord.timestamp()。也就是从kafka中消费出来的数据所带的时间戳。具体看下**emitRecordWithTimestamp()**方法：</p>
<pre><code class="language-java">protected void emitRecordWithTimestamp(
			T record, KafkaTopicPartitionState&lt;KPH&gt; partitionState, long offset, long timestamp) throws Exception {
		if (record != null) {
			if (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) {
				// fast path logic, in case there are no watermarks generated in the fetcher  emit the record, using the checkpoint lock to guarantee atomicity of record emission and offset state update
				synchronized (checkpointLock) {
					sourceContext.collectWithTimestamp(record, timestamp);
					partitionState.setOffset(offset);
				}
			} else if (timestampWatermarkMode == PERIODIC_WATERMARKS) {
				emitRecordWithTimestampAndPeriodicWatermark(record, partitionState, offset, timestamp);
			} else {
				emitRecordWithTimestampAndPunctuatedWatermark(record, partitionState, offset, timestamp);
			}
		} 
</code></pre>
<p>上面代码中，根据timestampWatermarkMode的不同取值会调用不同的方法发出数据。首先看下<strong>timestampWatermarkMode</strong>取值：</p>
<pre><code class="language-java">if (watermarksPeriodic == null) {
			if (watermarksPunctuated == null) {
				// simple case, no watermarks involved
				timestampWatermarkMode = NO_TIMESTAMPS_WATERMARKS;
			} else {
				timestampWatermarkMode = PUNCTUATED_WATERMARKS;
			}
		} else {
			if (watermarksPunctuated == null) {
				timestampWatermarkMode = PERIODIC_WATERMARKS;
			} else {
				throw new IllegalArgumentException(&quot;Cannot have both periodic and punctuated watermarks&quot;);
			}
}
</code></pre>
<p>在没有设置属性watermarksPeriodic和watermarksPunctuated时，timestampWatermarkMode取值为NO_TIMESTAMPS_WATERMARKS；否则，如果设置了watermarksPunctuated，timestampWatermarkMode的取值为PUNCTUATED_WATERMARKS；否则，如果设置了watermarksPeriodic，则timestampWatermarkMode的取值为PERIODIC_WATERMARKS。</p>
<ol>
<li>对于时间特性(TimeCharacteristic)设置为processing Time 或 Ingestion Time的情况：</li>
</ol>
<blockquote>
<p>这种情况是没有必要设置watermarksPeriodic和watermarksPunctuated属性的。因为，对于Processing Time情况，设置时间戳和水印没有实际意义；而对于Ingestion Time情况，具体会在SourceStreamContext中自动生成。<br>
这种情况下，timestampWatermarkMode取值为NO_TIMESTAMPS_WATERMARKS，直接调用了方法: sourceContext.collectWithTimestamp(record, timestamp)，后续如何提取时间戳，如何生成水印依赖于设置的时间特性，由具体SourceStreamContext实现（前面已经说明，此处不再赘述）。</p>
</blockquote>
<ol start="2">
<li>对于Event Time情况：</li>
</ol>
<blockquote>
<p>对于这种情况，如果想要在源操作处完成时间戳提取以及水印的生成，需要自定义具体AssignerWithPunctuatedWatermarks或AssignerWithPeriodicWatermarks的实现，包括如何提取时间戳，以及如何生成下一个水印。<br>
在FlinkKafkaConsumerBase类中提供了两个方法分别接收用户自定义的AssignerWithPunctuatedWatermarks或AssignerWithPeriodicWatermarks。具体为：</p>
</blockquote>
<pre><code class="language-java">public FlinkKafkaConsumerBase&lt;T&gt; assignTimestampsAndWatermarks(AssignerWithPunctuatedWatermarks&lt;T&gt; assigner) {}

public FlinkKafkaConsumerBase&lt;T&gt; assignTimestampsAndWatermarks(AssignerWithPeriodicWatermarks&lt;T&gt; assigner){}
</code></pre>
<h2 id="基于flink-kafka-consumer的消费实例">基于Flink kafka consumer的消费实例</h2>
<p>下面以一个具体的实例，阐述Flink消费kafka数据时如何在数据源出自定义时间戳提取以及生成水印：</p>
<ol>
<li>kafka producer端<pre><code class="language-java">public class WriteDataToKafka {
 public static void main(String[] args) throws IOException {
     Map&lt;String, String&gt; config = new HashMap&lt;String, String&gt;();
     config.putAll(ReadPropertiesUtils.readConfig(&quot;config.properties&quot;));
     Properties properties = new Properties();
     properties.put(&quot;zookeeper.connect&quot;, config.get(&quot;zookeeper.connect&quot;));
     properties.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
     properties.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);
     properties.put(&quot;bootstrap.servers&quot;, config.get(&quot;metadata.broker.list&quot;));

     try {
         KafkaProducer&lt;String, String&gt; kafkaProducer = new KafkaProducer&lt;String, String&gt;(properties);
         final String[] arr = new String[]{&quot;a&quot;, &quot;b&quot;};
         Timer timer = new Timer();
         timer.schedule(new TimerTask() {
             @Override
             public void run() {
                 for (int i = 0; i &lt; arr.length; i++) {
                     SampleObj sampleObj = new SampleObj();
                     sampleObj.setValue(arr[i]);
                     sampleObj.setOccurTime(System.currentTimeMillis());
                     System.out.println(JSONObject.toJSON(sampleObj).toString());
                     ProducerRecord&lt;String, String&gt; record =
                             new ProducerRecord&lt;String, String&gt;(&quot;test_topic&quot;,
                                     JSONObject.toJSON(sampleObj).toString());
                     kafkaProducer.send(record);
                     kafkaProducer.flush();
                 }
             }
         }, 0L, 1000);
     } catch (Exception e) {
         e.printStackTrace();
     }
 }
</code></pre>
</li>
</ol>
<p>}</p>
<pre><code>每隔1s向kafka 的test_topic中分别发送两条value取值为&quot;a&quot;或&quot;b&quot;的两条数据，数据的时间戳为构造数据时机器的系统时间。
发送数据对应的实体类定义为：
``` java 
public class SampleObj {
 private String value;
 private Long occurTime;}
</code></pre>
<ol start="2">
<li>Flink 端消费数据代码<br>
Flink 应用程序消费test_topic 的数据，每隔10s统计下各个value对应的count数，并打印到控制台。<pre><code class="language-java">public class FlinkKafkaConsumerSample {
        public static void main(String[] args) throws Exception {
     final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
     env.enableCheckpointing(5000);
     //设定时间特性为Event Time
     env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);
     Properties props = new Properties();
     props.setProperty(&quot;bootstrap.servers&quot;, &quot;10.154.8.27:9092&quot;);
     props.setProperty(&quot;group.id&quot;, &quot;alarm_consumer_test&quot;);

     FlinkKafkaConsumer010&lt;JSONObject&gt; consumer =
             new FlinkKafkaConsumer010&lt;&gt;(&quot;test_topic&quot;, new JsonFormatDeserializer(), props);
     consumer.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks&lt;JSONObject&gt;() {

         @Override
         public long extractTimestamp(JSONObject element, long previousElementTimestamp) {
             if (StringUtils.isNotBlank(element.getString(&quot;occurTime&quot;))) {
                 System.out.println(&quot;数据：&quot; + element.getString(&quot;value&quot;) + &quot;:&quot; + timeStamp2Date(element.getLong(&quot;occurTime&quot;)));
                 return Long.valueOf(element.getString(&quot;occurTime&quot;));
             } else return 0L;
         }

         @Nullable
         @Override
         public Watermark checkAndGetNextWatermark(JSONObject lastElement, long extractedTimestamp) {
             if (StringUtils.isNotBlank(lastElement.getString(&quot;occurTime&quot;))) {
                 return new Watermark(Long.valueOf(lastElement.getString(&quot;occurTime&quot;)));
             } else return null;
         }
     });

     env.addSource(consumer)
             .keyBy((KeySelector&lt;JSONObject, String&gt;) value -&gt; value.getString(&quot;value&quot;))
             .timeWindow(Time.seconds(10))
             .process(new ProcessWindowFunction&lt;JSONObject, Tuple3&lt;String, Long, String&gt;, String, TimeWindow&gt;() {
                 @Override
                 public void process(String s, Context context, Iterable&lt;JSONObject&gt; elements, Collector&lt;Tuple3&lt;String, Long, String&gt;&gt; out) throws Exception {
                     System.out.println(&quot;currentProcessTime:&quot; + timeStamp2Date(context.currentProcessingTime()) +
                             &quot;watermark:&quot; + timeStamp2Date(context.currentWatermark()) +
                             &quot;window info,start:&quot; + timeStamp2Date(context.window().getStart()) +
                             &quot;;end:&quot; + timeStamp2Date(context.window().getEnd()));
                     long sum = 0L;
                     long maxTime = Long.MIN_VALUE;
                     for (JSONObject jsonObject : elements) {
                         if (jsonObject.getLong(&quot;occurTime&quot;) &gt; maxTime) {
                             maxTime = jsonObject.getLong(&quot;occurTime&quot;);
                         }
                         sum += 1;
                     }
                     out.collect(new Tuple3&lt;&gt;(s, sum, timeStamp2Date(maxTime)));
                 }
             }).print();
     env.execute();
 }
     public static String timeStamp2Date(Long time) {
     String seconds = String.valueOf(time);
     if (seconds == null || seconds.isEmpty() || seconds.equals(&quot;null&quot;)) {
         return &quot;&quot;;
     }
     SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;);
     return sdf.format(new Date(Long.valueOf(seconds)));
 }
     private static class JsonFormatDeserializer implements DeserializationSchema&lt;JSONObject&gt; {
     @Override
     public JSONObject deserialize(byte[] message) throws IOException {
         try {
             return JSON.parseObject(message, JSONObject.class);
         } catch (Exception e) {
             return null;
         }
     }

     @Override
     public boolean isEndOfStream(JSONObject nextElement) {
         return false;
     }

     @Override
     public TypeInformation&lt;JSONObject&gt; getProducedType() {
         return TypeInformation.of(JSONObject.class);
     }
 }
}
</code></pre>
</li>
<li>运行结果片段<br>
<img src="https://wangyemao-github.github.io/post-images/1610698549901.png" alt="" loading="lazy"><br>
以上就是一个基于Event Time，自定义时间戳提取以及水印生成的Flink消费kafka数据的简单实例。除了在数据源处提取时间戳，生成水印之外，还能在一些简单的操作之后再指定相关生成策略。具体可以参见Flink#WarkMark。</li>
</ol>
<h1 id="总结">总结</h1>
<ol>
<li>本文首先对Flink支持的多种时间概念，包括：Processing Time、Event Time、Ingestion Time 的定义、内涵、差异进行了详细说明，并给出了Flink应用程序中如何设置时间特性</li>
<li>WaterMark是Flink基于时间操作中的一个很重要的概念，本节简要地对WaterMark的作用进行说明</li>
<li>之后，基于Flink1.8版本，本文对三种时间特性下，Flink在数据源处如何提取时间戳、如何生成水印并发送到下游进行了解读</li>
<li>接着，本文结合源码分析了Flink-kafka-source-connector时间戳提取和水印生成进行说明</li>
<li>最后，基于Flink消费kafka数据场景，给出了一个简单的在源处提取时间戳和生成水印的实例。</li>
</ol>
<h1 id="相关参考">相关参考</h1>
<ol>
<li>https://ci.apache.org/projects/flink/flink-docs-release-1.12/concepts/timely-stream-processing.html</li>
<li>http://www.54tianzhisheng.cn/2018/12/11/Flink-time/</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flink State 之 KeyedState]]></title>
        <id>https://wangyemao-github.github.io/post/flink-state-zhi-keyedstate/</id>
        <link href="https://wangyemao-github.github.io/post/flink-state-zhi-keyedstate/">
        </link>
        <updated>2021-01-03T02:44:22.000Z</updated>
        <summary type="html"><![CDATA[<p>本文主要总结Flink State 中KeyedState，以及KeyedState的应用</p>
]]></summary>
        <content type="html"><![CDATA[<p>本文主要总结Flink State 中KeyedState，以及KeyedState的应用</p>
<!-- more -->
<h1 id="keyedstate">KeyedState</h1>
<p>KeyedState在KeyedStream中使用。状态跟特定Key绑定的，即KeyedStream流上的每一个Key对应一个State对象。。</p>
<h2 id="keyedstate-支持的状态类型">KeyedState 支持的状态类型</h2>
<p>KeyedState可以使用所有Flink支持的State类型。FoldingState跟ReducingState类似，但是已标记为废弃，不建议再使用<br>
<img src="https://wangyemao-github.github.io/post-images/1609670108841.png" alt="" loading="lazy"></p>
<h2 id="statedescriptor">StateDescriptor</h2>
<p>对应于每一类State，Flink内部都设计了对应的<strong>StateDescriptor</strong>状态描述。它指定了状态的一些属性，包括：State名称，State中类型信息，序列化/反序列化器，State生存时间等。</p>
<h3 id="valuestatedescriptor定义">ValueStateDescriptor定义</h3>
<pre><code class="language-java">ValueStateDescriptor&lt;Tuple2&lt;Long,Long&gt;&gt; descriptor = 
              new ValueStateDescriptor&lt;&gt;(&quot;average&quot;,
                    TypeInformation.of(new TypeHint&lt;Tuple2&lt;Long,Long&gt;&gt;(){}),
                    Tuple2.of(0L,0L));
</code></pre>
<p>代码片段定义了一个ValueStateDescriptor，指定状态名称为“average”，状态类型为Tuple2&lt;Long,Long&gt;，默认值为：Tuple2&lt;0L,0L&gt;</p>
<h2 id="不同状态类型详细对比">不同状态类型详细对比</h2>
<table>
<thead>
<tr>
<th>State类型</th>
<th>存储数据类型</th>
<th>StateDescriptor</th>
<th>使用</th>
<th>操作</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueState<T></td>
<td>T类型的单值状态</td>
<td>ValueStateDescriptor<T></td>
<td>ValueState<T> getState(ValueStateDescriptor<T>)</td>
<td>更新：update(T);<br>获取：T value();</td>
<td>每一个Key与T类型状态值绑定</td>
</tr>
<tr>
<td>ListState<T></td>
<td>Key上的状态值为一个列表</td>
<td>ListStateDescriptor<T></td>
<td>ListState<T> getListState(ListStateDescriptor<T>)</td>
<td>追加数据：add(T)/addAll(List<T>);<br>获取状态数据列表：Iterable<T> get()</td>
<td></td>
</tr>
<tr>
<td>ReducingState<T></td>
<td>存储一个T类型的值</td>
<td>ReducingStateDescriptor<T></td>
<td>ReducingState<T> getReducingState(ReducingStateDescriptor<T>)</td>
<td>添加数据：add(T);</td>
<td>这种状态通过用户传入的reduceFunction，每次调用add方法添加值时，会调用reduceFunction，最后合并到一个单一的状态值</td>
</tr>
<tr>
<td>AggregatingState&lt;IN, OUT&gt;</td>
<td>存储一个值</td>
<td>AggregatingStateDescriptor&lt;IN, ACC, OUT&gt;</td>
<td>AggregatingState&lt;IN, OUT&gt; getAggregatingState(AggregatingStateDescriptor&lt;IN, ACC, OUT&gt;)</td>
<td>添加数据：add(IN)</td>
<td>聚合State，与ReducingState不同的是，这里的聚合类型可以是不同的元素类型。每次添加值时，会调用AggregateFunction函数计算聚合结果</td>
</tr>
<tr>
<td>MapState&lt;UK, UV&gt;</td>
<td>Map</td>
<td>MapStateDescriptor&lt;UK,UV&gt;</td>
<td>MapState&lt;UK, UV&gt; getMapState(MapStateDescriptor&lt;UK, UV&gt;)</td>
<td>添加：put(UK,UV)/putAll(UK,UV);<br>获取：UV get(UK);<br>遍历：Iterable&lt;Map.Entry&gt; entries()/Iterator&lt;Map.Entry&gt; iterator();</td>
<td>key-value结构数据</td>
</tr>
</tbody>
</table>
<h1 id="keyedstate使用">KeyedState使用</h1>
<p>##KeyedStream<br>
KeyedState只能应用于KeyedStream，如果你想在DataStream上使用Keyed State，你需要首先指定<strong>Key</strong>，Key用于State（也用于流记录本身）的分区。<br>
###指定Key的方式<br>
DataStream的keyBy()方法用于指定Key，并将DataStream转换为KeyedStream。它有如下几种重载方法，官方</p>
<blockquote>
<p>KeySelector</p>
<blockquote>
<p>KeyedStream&lt;T, K&gt; keyBy(KeySelector&lt;T, K&gt; key)<br>
通过实现KeySelector Function中的getKey()方法，自由指定Key的提取<br>
Tuple Keys<br>
KeyedStream&lt;T, Tuple&gt; keyBy(int... fields)<br>
应用于数据类型为简单Tuple类型，通过Tuple下表索引指定Key，当指定多个时为组合键<br>
Expression Keys<br>
KeyedStream&lt;T, Tuple&gt; keyBy(String... fields)<br>
应用于复杂的Tuple类型或POJO类型，对于POJO类型，String参数用于指定字段名</p>
</blockquote>
</blockquote>
<p>Flink的数据模型并不是基于键值对的。因此，KeyedStream并不会将物理上将数据处理为键何值。Key是“虚拟的”：它被定义为在实际数据上用于指导分组操作的函数。</p>
<p>KeyedState保存在StateBackend中</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flink State 之 Operator State 应用]]></title>
        <id>https://wangyemao-github.github.io/post/flink-state-zhi-operator-state-ying-yong/</id>
        <link href="https://wangyemao-github.github.io/post/flink-state-zhi-operator-state-ying-yong/">
        </link>
        <updated>2020-12-29T12:34:38.000Z</updated>
        <summary type="html"><![CDATA[<p>上文已经说到：Flink中，根据数据集是否根据Key进行分区，将状态分为Keyde State和Operator State两种类型。本文主要关注Operator State以及Operator State相关应用。</p>
]]></summary>
        <content type="html"><![CDATA[<p>上文已经说到：Flink中，根据数据集是否根据Key进行分区，将状态分为Keyde State和Operator State两种类型。本文主要关注Operator State以及Operator State相关应用。</p>
<!-- more -->  
<h1 id="operator-state">Operator State</h1>
<p>Operator State，也称作non-keyed State。每一个Operator State与一个并行的Operator实例绑定，每个Operator实例中都持有所有数据元素的一部分状态。<br>
在典型的Flink有状态应用中，并不需要用到Operator State。它主要是作为一种特殊类型的状态，应用于Source/Sink的实现，以及在没有键进行状态分区的场景。</p>
<h2 id="支持的数据类型">支持的数据类型</h2>
<p>目前Operator State只支持使用<strong>ListState</strong></p>
<h2 id="重分布机制">重分布机制</h2>
<p>当并行度改变时，Operator State接口支持在并行的操作实例之间重分布State，并且支持多种重分布方案：</p>
<ul>
<li>均匀重分布：每一个并行Operator都返回一个状态列表（List）。整个状态空间逻辑上就是所有状态列表的并集。在恢复/重分布阶段，整个状态列表根据并行操作数目均分为子状态列表。每一个并行操作分配一个子状态列表，子状态列表可能为空，也可能包含一个或多个元素。</li>
<li>合并重分布：这种方式将状态的划分交给用户，与均匀重分布方式相比具有更好的灵活性。在恢复/重分布阶段，每一个并行操作都获得了完整的一份状态列表。如果状态基数很大，不建议使用这种方式。因为Checkpoint 元数据会存储每个列表项的偏移量，这可能导致RPC帧大小或内存不足错误。</li>
</ul>
<h1 id="operator-state-应用">Operator State 应用</h1>
<h2 id="如何初始化一个状态state">如何初始化一个状态(State)</h2>
<p>在Flink中，使用状态描述<strong>StateDescriptor</strong>来初始化State，它包含了State的名称以及保存的State值的类型信息</p>
<pre><code class="language-java">ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =
    new ListStateDescriptor&lt;&gt;(
        &quot;buffered-elements&quot;,
        TypeInformation.of(new TypeHint&lt;Tuple2&lt;String,Integer&gt;&gt;() {}));
</code></pre>
<p>上面的代码片段中定义了一个State名称为：buffered-elements，State值类型为：Tuple2&lt;String,Integer&gt; 的状态描述<br>
##CheckpointedFunction<br>
要使用Operator State，有状态的函数需要实现<strong>CheckpointedFunction</strong>接口，该接口需要实现两个方法：</p>
<pre><code class="language-java">void snapshotState(FunctionSnapshotContext context) throws Exception;
void initializeState(FunctionInitializationContext context) throws Exception;
</code></pre>
<p>无论什么时候执行checkpoint，都会调用snapshotState()方法。因此该方法主要定义状态的快照保存逻辑；相应地，每当用户定义的函数被初始化时（无论是函数首次初始化，还是函数实际上从更早一个检查点恢复），都会调用initializeState()方法。因此，initializeState()不仅是不同类型状态初始化的地方，而且也包含了状态恢复的逻辑。</p>
<h2 id="sinkfunction-sample">SinkFunction Sample</h2>
<pre><code class="language-java">public class BufferingSink implements SinkFunction&lt;Tuple2&lt;String, Integer&gt;&gt;,            CheckpointedFunction {

    private final int threshold;

    private transient ListState&lt;Tuple2&lt;String, Integer&gt;&gt; checkpointedState;

    private List&lt;Tuple2&lt;String, Integer&gt;&gt; bufferedElements;

    public BufferingSink(int threshold) {
        this.threshold = threshold;
        this.bufferedElements = new ArrayList&lt;&gt;();
    }

    @Override
    public void invoke(Tuple2&lt;String, Integer&gt; value, Context contex) throws Exception {
        bufferedElements.add(value);
        if (bufferedElements.size() == threshold) {
            for (Tuple2&lt;String, Integer&gt; element: bufferedElements) {
                // send it to the sink
            }
            bufferedElements.clear();
        }
    }

    @Override
    public void snapshotState(FunctionSnapshotContext context) throws Exception {
        checkpointedState.clear();
        for (Tuple2&lt;String, Integer&gt; element : bufferedElements) {
            checkpointedState.add(element);
        }
    }

    @Override
    public void initializeState(FunctionInitializationContext context) throws Exception {
        ListStateDescriptor&lt;Tuple2&lt;String, Integer&gt;&gt; descriptor =
            new ListStateDescriptor&lt;&gt;(
                &quot;buffered-elements&quot;,
                TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Integer&gt;&gt;() {}));

        checkpointedState = context.getOperatorStateStore().getListState(descriptor);

        if (context.isRestored()) {
            for (Tuple2&lt;String, Integer&gt; element : checkpointedState.get()) {
                bufferedElements.add(element);
            }
        }
    }
}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flink State 之总体介绍]]></title>
        <id>https://wangyemao-github.github.io/post/flink-state-zong-ti-jie-shao/</id>
        <link href="https://wangyemao-github.github.io/post/flink-state-zong-ti-jie-shao/">
        </link>
        <updated>2020-12-29T08:09:21.000Z</updated>
        <summary type="html"><![CDATA[<p>本文主要对Flink State进行整体介绍，包括有状态的流式计算框架Flink与传统流式计算框架的区别，Flink State类型划分，State存储与管理以及动态扩容。</p>
]]></summary>
        <content type="html"><![CDATA[<p>本文主要对Flink State进行整体介绍，包括有状态的流式计算框架Flink与传统流式计算框架的区别，Flink State类型划分，State存储与管理以及动态扩容。</p>
<!-- more -->
<h1 id="一-有状态流计算介绍">一、有状态流计算介绍</h1>
<h2 id="什么是有状态的计算">什么是有状态的计算</h2>
<p>计算任务的结果不仅仅依赖于输入，还依赖于它的当前状态。比如，很常见的业务场景wordCount，在计算的过程中要不断地把输入累加到count上去，那么<strong>这里的count就是一个state</strong><br>
<img src="https://wangyemao-github.github.io/post-images/1609229474736.jpg" alt="" loading="lazy"><br>
Flink基于用户定义代码对到来的数据进行转换操作，处理过程中不仅依赖用户定义算子，还会涉及State数据的读写操作。这些State数据会存储在本地的State backend中，在Checkpoint的时候持久化到配置的Checkpoint 目录</p>
<h2 id="传统流式框架">传统流式框架</h2>
<p>在之前的实时计算框架—Spark进行流式数据处理过程中，流式计算通过将源源不断的数据切分为一个个很小的时间间隔批块，然后针对每一个微批块进行数据计算。它并不是真正意义上的流式处理。每一个微批，只需要保存最终的计算结果，因此，它对于State的需求还是比较小的。</p>
<p>实际上，流式计算对State的要求是非常高的。因为流系统中输入是一个无限制的流，需要保证很长一段时间持续不间断运行，这个过程中就需要很好地将状态数据管理起来。传统的流式计算框架缺乏对State的有效支持：</p>
<ul>
<li>状态数据的存储和访问</li>
<li>状态数据的备份和恢复</li>
<li>状态数据的划分和动态扩容</li>
</ul>
<h2 id="flink提供了丰富的状态访问和高效的容错机制">Flink提供了丰富的状态访问和高效的容错机制</h2>
<ul>
<li>多种数据类型：Value、List、Map、Reducing、Folding、Aggregating</li>
<li>多种划分方式： Keyed State、Operator State</li>
<li>多种存储格式： MemoryStateBackend、FsStateBackend、RocksDBStateBackend</li>
<li>高效的备份和恢复：提供ExactlyOnce保证、增量及异步备份本地恢复</li>
</ul>
<h1 id="二-flink-state-类别">二、Flink State 类别</h1>
<p>Flink中，State按照是否有Key划分为Keyed State和Operator State</p>
<h2 id="keyed-state">Keyed State</h2>
<p><strong>Keyed State</strong>在Keyed Stream中使用。状态是跟特定的Key绑定的，即Keyed Stream流上的每一个Key对应一个State对象。</p>
<h2 id="operator-state">Operator State</h2>
<p><strong>Operator State</strong>(non-keyed state)跟一个特定操作的并行实例绑定，整个操作只对应一个State。典型的Operator State应用场景就是Kafka Source Connector，kafka consumer的每一个并行实例都维护了topic partition与offsets的映射关系作为它的Operator State。<br>
<strong>Broadcast State</strong>是一个特殊的Operator State。它的引入是为了支持一个流的记录需要被广播到下游的所有并行任务的场景，在这种场景中，它们被用来在所有的并行任务中维护相同的状态。然后，可以在处理第二个流的记录时访问这个状态。broadcast state与operator state的其他区别：</p>
<ol>
<li>状态的数据类型为Map格式</li>
<li>它仅在具有两个输入流的特定操作上可用，一个输入流为broadcasted stream ，一个输入流为non-broadcasted</li>
<li>这些操作可以定义多个具有不同名称的broadcast state</li>
</ol>
<p><strong>详细对比：</strong></p>
<table>
<thead>
<tr>
<th>对比维度</th>
<th>Keyed State</th>
<th>Operator State</th>
</tr>
</thead>
<tbody>
<tr>
<td>使用场景</td>
<td>只能用于KeyedStream上的算子</td>
<td>可以用于所有的算子，常用于Source，例如FlinkKafkaConsumer</td>
</tr>
<tr>
<td>应用</td>
<td>通过RuntimeContext访问，操作函数需要实现RichFunction接口</td>
<td>实现CheckpointedFunction或ListCheckpointed接口</td>
</tr>
<tr>
<td>是否需要手动声明快照(snapshot)和恢复（restore)方法</td>
<td>由backend自行实现，对用户透明</td>
<td>需要手动实现snapshot和restore方法</td>
</tr>
<tr>
<td>支持数据接口</td>
<td>包括：ValueState、ListState、ReducingState、AggregatingState、MapState</td>
<td>ListState。特别说明：Broadcast State 是MapState类型</td>
</tr>
<tr>
<td>是否存在当前处理的 key（current key）</td>
<td>keyed state的value总是与一个current key对应。一个Operator实例处理多个key，访问相应的多个State</td>
<td>无当前key概念。一个Operator实例对应一个State</td>
</tr>
<tr>
<td>并发改变时State重分布</td>
<td>基于Key-Group，State随着Key在实例间迁移</td>
<td>并发改变时，有多种重新分配方式可选：均匀分配；合并后每个实例都得到全量状态</td>
</tr>
<tr>
<td>存储对象是否 on heap</td>
<td>keyed state backend 有on-heap和off-heap(RocksDB)的多种实现</td>
<td>目前operator state backend仅有一种on-heap的实现</td>
</tr>
<tr>
<td>状态数据大小【这只是个经验判断，不是绝对的判断区分标准】</td>
<td>一般而言，Keyed state规模的相对比较大的</td>
<td>一般而言，我们认为operator state的数据规模是比较小的</td>
</tr>
</tbody>
</table>
<h1 id="三-flink-state存储和管理">三、Flink State存储和管理</h1>
<h2 id="state-backend-分类">State Backend 分类</h2>
<p>State在内部如何表述，checkpoint时，State如何以及在哪里进行持久化，依赖于选择的State Backend。Flink默认捆绑了三类State Backends：MemoryStateBackend、FsStateBackend、RocksDBStateBackend。在未配置的情况下，系统将默认使用MemoryStateBackend。</p>
<p><strong>详细区别：</strong><br>
<img src="https://wangyemao-github.github.io/post-images/1609232674288.jpg" alt="" loading="lazy"></p>
<table>
<thead>
<tr>
<th>State Backend 类型</th>
<th>State存储</th>
<th>Checkpoint时存储</th>
<th>快照方式</th>
<th>是否支持增量Checkpoint</th>
<th>限制</th>
<th>使用场景</th>
<th>推荐设置</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>MemoryStateBackend</td>
<td>TaskManager内存</td>
<td>JobManager内存</td>
<td>默认异步方式</td>
<td>否</td>
<td>1. 单个State默认限制为5MB，当然这个值可以在创建state backend时调整<br>2. 但是Max State最大不能超过akka frame的size (默认10M)<br>3. 聚合的State总大小要小于JobManager内存大小</td>
<td>1. 本地开发和debugging<br>2. 需要存储很少量状态的作业<br>3. 不推荐生产场景</td>
<td>将managed memory设置为0.确保最大量地使用分配内存给用户作业</td>
<td>1. State数据作为Java堆对象在本地（TaskManager）存储<br>2. 在Checkpoint时，state backend会快照状态数据，作为checkpoint确认消息的一部分发送给JobManager。在JobManager上，状态数据也是在Java 堆上存储的</td>
</tr>
<tr>
<td>FsStateBackend</td>
<td>TaskManager内存</td>
<td>外部文件系统（本地目录或HDFS目录）</td>
<td>默认异步方式</td>
<td>否</td>
<td>1.单Taskmanager上的State总量不超过它的内存<br>2.总大小不超过配置的文件系统容量</td>
<td>1.大状态作业，长窗口，大key/value 状态<br>2.高可用设置<br>3.生产场景</td>
<td>同上</td>
<td>1.在TaskManager的内存中，存储正在使用的State数据<br>2. Checkpointing时，state backend将状态快照写文件到配置文件系统和目录中。最小化的元数据存储在JobManager的内存【状态数据存储的路径】（高可用模式，存储元数据在元数据checkpoint路径）</td>
</tr>
<tr>
<td>RocksDBStateBackend</td>
<td>TaskManager上的KV数据库(RocksDB)【实际使用内存+磁盘】</td>
<td>外部文件系统（本地目录或HDFS目录）</td>
<td>总是异步</td>
<td>是</td>
<td>1.单TaskManager上的State不超过它的内存+磁盘<br>2.单Key最大2G<br>3.总大小不超过配置的文件系统容量</td>
<td>1.大状态作业，长窗口，大key/value 状态<br>2.高可用设置<br>3.生产场景</td>
<td></td>
<td>1.RocksDBStateBackend在RocksDB数据库中保存正在使用的状态数据，默认存储在TaskManager的数据目录<br>2.在checkpointing时，整个RocksDB数据库将被快照到配置的文件系统或目录。最小化的元数据存储在JobManager的内存中（高可用模式，存储在元数据checkpoint目录）</td>
</tr>
</tbody>
</table>
<p><strong>关于RocksDBStateBackend的特别说明：</strong></p>
<ol>
<li>这种方式可以维持的状态量只受限于可用的磁盘空间。与FsStateBackend将state存储在内存相比，这种方式允许保留更大的state</li>
<li>但是，这也意味着，可以获取的最大吞吐量将比FsStateBackend方式的低</li>
<li>所有对backend的read/write都需要通过序列化/反序列化，以存储/获取状态对象。这种方式与基于堆的存储方式代价更高</li>
</ol>
<h2 id="state-backend配置">State Backend配置</h2>
<ul>
<li>配置默认State Backend</li>
</ul>
<blockquote>
<p>flink-conf.yml<br>
state.backend: 【可能的取值：jobmanager(MemoryStateBackend)、fileSystem(FsStateBackend)、rocksdb(RocksDBStateBackend)、或者实现了state backend factory StateBackendFactory的全限定类名（org.apache.flink.contrib.streaming.state.RocksDBStateBackendFactory）】<br>
state.checkpoints.dir:定义backend写快照数据和元数据文件的目录</p>
</blockquote>
<ul>
<li>per-job 设置State Backend</li>
</ul>
<blockquote>
<p>StreamExecutionEnvironment</p>
</blockquote>
<pre><code class="language-java">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setStateBackend(new FsStateBackend(&quot;hdfs://namenode:40010/flink/checkpoints&quot;));
</code></pre>
<h1 id="四-flink-state-划分和动态扩容">四、Flink State 划分和动态扩容</h1>
<h2 id="state-划分">State 划分</h2>
<ul>
<li>对于Operator State类型，每一个并行的Operator实例对应一个State</li>
<li>对于Keyed State类型，每一个Current Key对应一个State</li>
</ul>
<h2 id="state-动态扩容">State 动态扩容</h2>
<p><strong>Operator State类型</strong><br>
<img src="https://wangyemao-github.github.io/post-images/1609232700062.jpg" alt="" loading="lazy"></p>
<ul>
<li>ListState：并发度改变的时候，会将并发实例上的ListState都合并到一个新的List，然后均匀分配到变更后的并发Task上</li>
<li>UnionListState：每一个并发Task都会拿到全量的ListState</li>
<li>BroadcastState：每个并发Task上的State都是完全一致的，因此，当增大并发度时，只需要Copy一份State到新Task即可</li>
</ul>
<p><strong>Keyed State类型</strong><br>
在并行度改变时，如何重分布KeyedState数据，最直观的做法就是计算每个Key的Hash值，并基于并行度parallelism取余。下图为当并行度改变时，基于Hash取余算法的State数据重分布情况：<br>
<img src="https://wangyemao-github.github.io/post-images/1609232708709.png" alt="" loading="lazy"></p>
<p>hash取余重分布算法存在的问题：之前在各个Task上维护好的State数据会根据新的并行度重新组织，State数据在各个Task之间传输。这对于KeyedState数据较大时，数据重新组织的代价会很高。</p>
<p>为了规避上述问题，Flink基于Key-Group(KeyedState的最小组织单位)组织、分配KeyedState。具体的映射关系为</p>
<pre><code class="language-java">public static int assignToKeyGroup(Object key, int maxParallelism) {
   return computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);
}
public static int computeKeyGroupForKeyHash(int keyHash, int maxParallelism) {
   return MathUtils.murmurHash(keyHash) % maxParallelism;
}
</code></pre>
<p>也就是说，Key-Group数量取决于最大并行度（MaxParallism），只要最大并行度不变，同一个key归属的Key-Group是不会变更的。另外，此处在HashCode的基础上又调用murmurHash方法是为了保证尽量散列。</p>
<p>在对Key划分Key-Group之后，MaxParallism个Key-Group 基于KeyGroupRange分配到parallelism个并行Task中，每一个并行Task持有1个KeyGroupRange，具体计算方法：</p>
<pre><code>public static KeyGroupRange computeKeyGroupRangeForOperatorIndex(
   int maxParallelism,
   int parallelism,
   int operatorIndex) {
   int start = ((operatorIndex * maxParallelism + parallelism - 1) / parallelism);
   int end = ((operatorIndex + 1) * maxParallelism - 1) / parallelism;
   return new KeyGroupRange(start, end);
}
</code></pre>
<p><strong>以一个具体的实例说明：</strong><br>
对于一个Key 空间=[0,1,2,3,4,5,6,7,8,9] 的数据，MaxParallelism=5，并行度Parallelism由2扩大到3的过程中， 各个并行Task的Key-Group重分布情况为：<br>
<img src="https://wangyemao-github.github.io/post-images/1609232723011.png" alt="" loading="lazy"><br>
<strong>总结：</strong></p>
<ol>
<li>只要MaxParallelism不变，整个Key空间的Key-Group划分情况是不会变更的</li>
<li>当并行度变更时，基于Key-Group，将Key-Group重新分配到并行Task中，并且这种重新分配不是一个Shuffle，Key-Group归属的并行Task的变更很小。</li>
</ol>
<p>参考资料：<br>
https://ci.apache.org/projects/flink/flink-docs-release-1.12/dev/stream/state/state.html<br>
https://ci.apache.org/projects/flink/flink-docs-release-1.12/ops/state/state_backends.html<br>
http://www.54tianzhisheng.cn/2019/06/18/flink-state/</p>
]]></content>
    </entry>
</feed>