<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://wangyemao-github.github.io</id>
    <title>Gridea</title>
    <updated>2021-04-14T03:30:43.783Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://wangyemao-github.github.io"/>
    <link rel="self" href="https://wangyemao-github.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://wangyemao-github.github.io/images/avatar.png</logo>
    <icon>https://wangyemao-github.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[Table API & SQL Streaming Concepts]]></title>
        <id>https://wangyemao-github.github.io/post/table-api-and-sql-streaming-concepts/</id>
        <link href="https://wangyemao-github.github.io/post/table-api-and-sql-streaming-concepts/">
        </link>
        <updated>2021-04-13T15:25:48.000Z</updated>
        <content type="html"><![CDATA[<p>Flink 的Table API 和SQL 对batch 和stream处理支持统一的APIs。这就意味着不管它们的输入是否为有界的batch 输入还是无界的stream input，Table API 和SQL查询具有相同的语义。</p>
<h1 id="dynamic-tables">Dynamic Tables</h1>
<h2 id="relational-queries-on-data-streams">Relational Queries on Data Streams</h2>
<p>关系代数/SQL 与 Stream Processing 比较<br>
<img src="https://wangyemao-github.github.io/post-images/1618364966334.png" alt="" loading="lazy"><br>
** 物化视图**<br>
高级数据库系统中提供了物化视图的特性。物化视图被定义为SQL查询，就像常规的虚拟视图一样。与虚拟视图不同，物化视图缓存查询结果，因此在访问查询时不需要对查询进行计算。<br>
缓存的一个常见挑战是防止缓存提供过时的结果。当定义查询物化视图的基表被修改时，物化视图中的数据就会过时。<br>
Eager View Maintenance 是一种一旦基表更新，就更新物化视图的一种技术。<br>
如果我们考虑以下问题，Eager View Maintenance和 SQL queries on streams的联系就会变得明显起来：</p>
<ul>
<li>a database table 由INSERT、UPDATE和DELETE DML语句流产生，通常称为changelog stream（更改日志流）</li>
<li>A materialized view 定义为一个SQL 查询。为了更新视图，查询必须持续处理视图基表的变更日志流（changelog stream）</li>
<li>materialized view 就是streaming SQL query 的结果</li>
</ul>
<h2 id="dynamic-tables-continuous-queries">Dynamic Tables &amp; Continuous Queries</h2>
<p><strong>Dynamic tables</strong>为Flink Table API &amp; SQL 支持Streaming data的核心概念。与表示批处理数据的静态表不同，Dynamic tables会随时间变化。但就像静态批处理表一样，系统也可以对动态表执行查询。查询动态表产生持续的查询。持续查询永远不会终止并产生动态结果—另一个动态表。查询不断更新其(动态)结果表，以反映其(动态)输入表的更改。<strong>从本质上讲，动态表上的持续查询非常类似于定义物化视图的查询</strong>。<br>
值得注意的是：持续查询的输出在语义上总是等同于在输入表的快照上以批处理模式执行的相同查询的结果。<br>
Streams，dynamic tables, and continuous queries的关系：<br>
<img src="https://wangyemao-github.github.io/post-images/1618367200840.png" alt="" loading="lazy"></p>
<ol>
<li>Stream 转换为 dynamic table</li>
<li>在dynamic table上执行A continuous query 生成了一张新的dynamic table</li>
<li>生成的dynamic table又转换回stream</li>
</ol>
<p>注意：dynamic table 是一个逻辑概念。在查询执行期间，它不一定完全物化。</p>
<h2 id="defining-a-table-on-a-stream">Defining a Table on a Stream</h2>
<figure data-type="image" tabindex="1"><img src="https://wangyemao-github.github.io/post-images/1618367649017.png" alt="" loading="lazy"></figure>
<p>使用关系查询处理流需要将其转换为Table。从概念上讲，流的每条记录都被解释为对结果表的INSERT修改。<br>
<img src="https://wangyemao-github.github.io/post-images/1618367877328.png" alt="" loading="lazy"><br>
注意: 上图是一种概念上的描述，实际上在流上定义的Table在内部没有物化</p>
<h2 id="continuous-queries">Continuous Queries</h2>
<p>在dynamic table上执行A continuous query 生成了一张新的dynamic table。与批查询不同，持续查询不会终止，它会根据输入表的更新不断更新结果表。</p>
<ul>
<li>simple GROUP-BY COUNT 聚合查询<br>
<img src="https://wangyemao-github.github.io/post-images/1618368281876.png" alt="" loading="lazy"></li>
<li>基于user + hourly tumbling window Group-BY COUNT 聚合查询<br>
<img src="https://wangyemao-github.github.io/post-images/1618368506928.png" alt="" loading="lazy"></li>
</ul>
<h3 id="update-and-append-queries">Update and Append Queries</h3>
<p>两个实例查询的差异：</p>
<ul>
<li>第一个查询会更新之前计算的结果。也就是说定义结果表的changelog stream 包含了INSERT 和UPDATE 变更</li>
<li>第二个查询只会追加数据。也就是说定义结果表的changelog stream 只包含INSERT 变更<br>
查询生成append-only table 还是 updated table 具有如下一些含义：</li>
<li>具有变更的查询通常需要维护更多状态（state）</li>
<li>append-only table 到Stream的转换 与 updated table 到stream的转换不同。</li>
</ul>
<h3 id="query-restrictions">Query Restrictions</h3>
<p>很多（但是不是所有）语义有效查询可以作为流上的持续查询进行评估。由于需要维护的状态的大小或计算更新的开销太大，有些查询的计算成本太高。</p>
<ul>
<li>State Size ：连续查询在无界流上进行计算，通常需要运行数周或数月。因此，连续查询处理的数据总量可能非常大。需要更新以前发出的结果的查询需要维护所有发出的行来更新它们。例如，第一个示例查询需要存储每个用户的URL计数，以增加计数，并在输入表接收到新行时发送一个新结果。如果只跟踪注册用户，要维护的计数可能不会太高。但是，如果非注册用户分配了唯一的用户名，那么要维护的计数会随着时间的推移而增加，最终可能导致查询失败。</li>
</ul>
<pre><code class="language-text">SELECT user, COUNT(url) FROM clicks GROUP BY user;
</code></pre>
<ul>
<li>Computing Updates: 有些查询需要重新计算和更新大部分发出的结果行，即使只添加或更新了单个输入记录.这样的查询不太适合作为连续查询执行.下面的查询是一个例子，它根据最后一次单击的时间计算每个用户的排名.一旦单击表收到一个新行，就更新用户的lastAction并计算一个新的排名。但是，由于两行不能具有相同的排名，所以还需要更新所有排名较低的行。</li>
</ul>
<pre><code class="language-text">SELECT user, RANK() OVER (ORDER BY lastAction)
FROM (
  SELECT user, MAX(cTime) AS lastAction FROM clicks GROUP BY user
);
</code></pre>
<h3 id="table-to-stream-conversion">Table to Stream Conversion</h3>
<p>dynamic table可以像普通的数据库表一样，通过插入、更新和删除更改来持续修改。它可能是一个只有一行且不断更新的表，也可能是一个只包含插入的表，而不包含更新和删除修改，或者介于两者之间的任何内容。<br>
在将dynamic table转换为Stream或将其写入外部系统时，需要对这些更改进行编码.Flink的Table API和SQL支持三种方法来编码动态表的更改:</p>
<ul>
<li><strong>Append-only stream:</strong> 只有INSERT 修改的dynamic table 转换为Stream 可以通过发出插入的行</li>
<li><strong>Retract stream:</strong> a retract  stream 是包含两种类型消息的流：add message 和 retract message。dynamic table转换为retract stream，通过将INSERT 变更编码为add message；将DELETE 变更编码为retract message；UPDATE 变更编码为对已更新的（先前的）行的retract message + 对正在更新的（新的）行编码add message。<br>
<img src="https://wangyemao-github.github.io/post-images/1618370395219.png" alt="" loading="lazy"></li>
<li>**Upsert stream: **upsert stream 是包含两种类型的消息：upsert messages 和delete messages。dynamic table 转换为upsert stream需要一个唯一key（可能是组合字段）。具有唯一key的dynamic table转换为stream，通过将INSERT和UPDATE变更编码为 upsert messages；DELETE变更编码为delete message。流消费operator需要知道唯一key属性才能正确的应用消息。与retract  stream 的主要区别在于：UPDATE变更被编码为唯一的消息，因此更高效。<br>
<img src="https://wangyemao-github.github.io/post-images/1618370817342.png" alt="" loading="lazy"></li>
</ul>
<p>dynamic table into a DataStream 只支持：append and retract streams；</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Table & SQL Connectors]]></title>
        <id>https://wangyemao-github.github.io/post/table-and-sql-connectors/</id>
        <link href="https://wangyemao-github.github.io/post/table-and-sql-connectors/">
        </link>
        <updated>2021-04-13T08:13:09.000Z</updated>
        <content type="html"><![CDATA[<p>Table API &amp; SQL 可以connect to 外部系统，读取、写入 batch 或streaming table。** a table source** 提供了访问存储在外部系统的数据（比如，数据库，key-value存储，消息队列，文件系统等）；** a table sink ** emit a table 到外部存储系统。<br>
Flink 当前支持的Connectors：<br>
<img src="https://wangyemao-github.github.io/post-images/1618302077330.png" alt="" loading="lazy"></p>
<h1 id="how-to-use-connectors">how to use connectors</h1>
<p>flink支持使用SQL create table 语句注册表。可以定义表名称，表模式，以及对于连接到外部系统的表选项。</p>
<pre><code class="language-java">CREATE TABLE MyUserTable (
  -- declare the schema of the table
  `user` BIGINT,
  `message` STRING,
  `rowtime` TIMESTAMP(3) METADATA FROM 'timestamp',    -- use a metadata column to access Kafka's record timestamp
  `proctime AS PROCTIME(),    -- use a computed column to define a proctime attribute
  WATERMARK FOR `rowtime` AS `rowtime` - INTERVAL '5' SECOND    -- use a WATERMARK statement to define a rowtime attribute
) WITH (
  -- declare the external system to connect to
  'connector' = 'kafka',
  'topic' = 'topic_name',
  'scan.startup.mode' = 'earliest-offset',
  'properties.bootstrap.servers' = 'localhost:9092',
  'format' = 'json'   -- declare a format for this system
)
</code></pre>
<h2 id="sql-create-statements">SQL CREATE Statements</h2>
<p>Ceate Statements 用于在当前或者指定的Catalog中注册table/view。注册的table/view可以在SQL查询中使用。FLINk SQL支持如下Create statements：CREATE TABLE / DATABASE/VIEW/FUNCTION。</p>
<h3 id="run-a-sql-create-statements">RUN a SQL CREATE Statements</h3>
<p>使用TableEnvironment的executeSql()方法执行。返回“ok”----》成功；否则抛出一个异常</p>
<pre><code class="language-java">EnvironmentSettings settings = EnvironmentSettings.newInstance()...
TableEnvironment tableEnv = TableEnvironment.create(settings);

// SQL query with a registered table
// register a table named &quot;Orders&quot;
tableEnv.executeSql(&quot;CREATE TABLE Orders (`user` BIGINT, product STRING, amount INT) WITH (...)&quot;);
// run a SQL query on the Table and retrieve the result as a new Table
Table result = tableEnv.sqlQuery(
  &quot;SELECT product, amount FROM Orders WHERE product LIKE '%Rubber%'&quot;);
// Execute insert SQL with a registered table
// register a TableSink
tableEnv.executeSql(&quot;CREATE TABLE RubberOrders(product STRING, amount INT) WITH (...)&quot;);
// run an insert SQL on the Table and emit the result to the TableSink
tableEnv.executeSql(
  &quot;INSERT INTO RubberOrders SELECT product, amount FROM Orders WHERE product LIKE '%Rubber%'&quot;);
</code></pre>
<h3 id="create-table">CREATE TABLE</h3>
<p>建表语句语法：</p>
<pre><code class="language-text">CREATE TABLE [catalog_name.][db_name.]table_name
  (
    { &lt;physical_column_definition&gt; | &lt;metadata_column_definition&gt; | &lt;computed_column_definition&gt; }[ , ...n]
    [ &lt;watermark_definition&gt; ]
    [ &lt;table_constraint&gt; ][ , ...n]
  )
  [COMMENT table_comment]
  [PARTITIONED BY (partition_column_name1, partition_column_name2, ...)]
  WITH (key1=val1, key2=val2, ...)
  [ LIKE source_table [( &lt;like_options&gt; )] ]
   
&lt;physical_column_definition&gt;:
  column_name column_type [ &lt;column_constraint&gt; ] [COMMENT column_comment]
  
&lt;column_constraint&gt;:
  [CONSTRAINT constraint_name] PRIMARY KEY NOT ENFORCED

&lt;table_constraint&gt;:
  [CONSTRAINT constraint_name] PRIMARY KEY (column_name, ...) NOT ENFORCED

&lt;metadata_column_definition&gt;:
  column_name column_type METADATA [ FROM metadata_key ] [ VIRTUAL ]

&lt;computed_column_definition&gt;:
  column_name AS computed_column_expression [COMMENT column_comment]

&lt;watermark_definition&gt;:
  WATERMARK FOR rowtime_column_name AS watermark_strategy_expression

&lt;source_table&gt;:
  [catalog_name.][db_name.]table_name

&lt;like_options&gt;:
{
   { INCLUDING | EXCLUDING } { ALL | CONSTRAINTS | PARTITIONS }
 | { INCLUDING | EXCLUDING | OVERWRITING } { GENERATED | OPTIONS | WATERMARKS } 
}[, ...]
</code></pre>
<h3 id="columns">Columns</h3>
<h4 id="physical-regular-columns">Physical / Regular Columns</h4>
<p>物理列是众所周知的数据库中常规列。它们定义物理数据中字段的名称、类型和顺序。因此，物理列表示从外部系统读取和写入的有效负载。connectors和formats使用这些列(按定义的顺序)来配置。其他类型的列可以在物理列之间声明，但不会影响最终的物理模式。</p>
<pre><code class="language-text">CREATE TABLE MyTable (
  `user_id` BIGINT,
  `name` STRING
) WITH (
  ...
);
</code></pre>
<h4 id="metadata-columns">Metadata Columns</h4>
<p>元数据列是SQL标准的扩展，并允许访问表每一行中特定connectors和formats的字段。元数据列由METADATA关键字标识。比如，对于基于时间的操作，元数据列可以用于从kafka记录中读写时间戳。声明元数据列咋table模式中是可选的。<br>
引用kafka 的timestamp字段作为元数据列：</p>
<pre><code class="language-text">CREATE TABLE MyTable (
  `user_id` BIGINT,
  `name` STRING,
  `record_time` TIMESTAMP(3) WITH LOCAL TIME ZONE METADATA FROM 'timestamp'    -- reads and writes a Kafka record's timestamp
) WITH (
  'connector' = 'kafka'
  ...
); 
</code></pre>
<p>每一个元数据标识为：字符串key+文本数据类型；比如上面kafka暴露了一个元数据字段，其key为：timestamp；数据类型为：TIMESTAMP(3) WITH LOCAL TIME ZONE。<br>
元数据列record_time会作为table schema的一部分，可以像正常列一样做转换和存储：</p>
<pre><code class="language-text">INSERT INTO MyTable SELECT user_id, name, record_time + INTERVAL '1' SECOND FROM MyTable;
</code></pre>
<p>如果元数据列名就是元数据key，FROM子句可以省略：</p>
<pre><code class="language-text">`timestamp` TIMESTAMP(3) WITH LOCAL TIME ZONE METADATA    -- use column name as metadata key
</code></pre>
<p>为方便起见，如果列的数据类型不同于元数据字段的数据类型，则运行时将执行显式强制转换。当然，这要求这两种数据类型是兼容的。</p>
<pre><code class="language-text">`timestamp` BIGINT METADATA    -- cast the timestamp as BIGINT
</code></pre>
<p>默认，planner假定元数据列可用于读写。但是，在许多情况下，外部系统提供的只读元数据字段比可写字段更多。因此，可以使用VIRTUAL关键字排除元数据列的持久化。</p>
<pre><code class="language-text">`timestamp` BIGINT METADATA VIRTUAL ,       -- no part of the query-to-sink schema
</code></pre>
<p>=====&gt;</p>
<pre><code class="language-text">source-to-query schema:
MyTable(`timestamp` BIGINT, `user_id` BIGINT, `name` STRING)

query-to-sink schema:
MyTable(`user_id` BIGINT, `name` STRING)
</code></pre>
<p>####Computed Columns<br>
Computed columns are virtual columns that are generated using the syntax column_name AS computed_column_expression.<br>
计算字段不会在表中进行物理存储，<br>
The planner will transform computed columns into a regular projection after the source。</p>
<pre><code class="language-text"> `cost` AS price * quanitity,  -- evaluate expression and supply the result to queries
</code></pre>
<p>计算列可以包含任何列、常量、函数的组合，不能包含子查询。<br>
计算列通常在CREATE TABLE statement中用于定义时间属性：</p>
<ul>
<li>processing time ：使用系统的PROCTIME() 函数定义。proc AS PROCTIME()</li>
<li>event time ：可以在WATERMARK申明之前进行预处理。<br>
计算列与元数据列相似，也不会进行持久化。因此计算列不能是INSERT INTO语句的目标。</li>
</ul>
<h3 id="wartermark">WARTERMARK</h3>
<p>WARTERMARK语句定义了table的事件时间属性（event-time属性），采用如下的格式：WATERMARK FOR rowtime_column_name AS watermark_strategy_expression<br>
<strong>rowtime_column_name</strong> 已有字段，标记为table的事件时间属性。该列数据类型必须是TIMESTAMP(3)，并且为模式中的顶级列。可以是计算列。<br>
<strong>watermark_strategy_expression</strong>定义了watermark生成策略。它允许任意非查询表达式(包括计算的列)计算水印。表达式返回类型必须为TIMESTAMP(3)，表示从Epoch开始的时间戳。只有当返回的水印是非空且其值大于先前发出的本地水印时，才会发出返回的水印；水印生成表达式由框架基于每一条记录进行计算。框架会周期性地发出生成的最大水印，如果当前的水印仍就与之前的相同，或为null，或比之前的小，则都不会发出水印。水印发出间隔由pipeline.auto-watermark-interval 配置设置。如果watermark 间隔为0ms，那么对于每条记录，如果生成的水印不为null，并且比之前的水印值大，会发射水印。<br>
Flink提供的几种常用的水印策略：</p>
<ul>
<li>严格升序时间戳：WATERMARK FOR rowtime_column AS rowtime_column</li>
<li>升序时间戳：WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL '0.001' SECOND</li>
<li>有界的乱序时间戳：WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL 'string'</li>
</ul>
<p>###WITH Options<br>
table options ： 用于创建table source/sink。这些属性通常用于查找和创建底层connector。表达式中的key，value都应该为字符串字面量。不同的connector支持不同的table属性。</p>
<h3 id="like">LIKE</h3>
<h3 id="primary-key">Primary KEY</h3>
<h1 id="schema-mapping">Schema Mapping</h1>
<p>SQL CREATE TABLE语句的body子句定义物理列、约束和水印的名称和类型。Flink不保存数据，因此模式定义只声明如何将外部系统的物理列映射到Flink的表示。映射可能不是基于名称映射的，这依赖于formats和connector的实现。比如：mysql 数据库表通过字段名映射（不区分大小写）；CSV格式的文件系统按字段顺序进行映射。</p>
<h1 id="primary-key-2">Primary KEY</h1>
<p>主键约束表示表中的一列或一组列是唯一的，并且它们不包含空值。主键唯一标识表中的一行。<br>
source table的primary key是用于优化的元数据信息。sink table的primary key 通常被sink 实现用于upserting。</p>
<h1 id="time-attributes">Time Attributes</h1>
<p>在无界流table处理中，时间属性必不可少。因此可以将proctime and rowtime attributes定义为模式的一部分。</p>
<h2 id="introduction-to-time-attributes">Introduction to Time Attributes</h2>
<p>Time attributes 可以是每个table schema的一部分。在使用CREATE TABLE DDL 或DataSteam创建table时定义。一旦定义了time attributes，就可以引用为字段，并在基于时间的操作中使用。只要time attribute（时间属性）没有被修改，只是从查询的一个部分转发到另一个部分，它就仍然是一个有效的时间属性。时间属性的行为类似于常规的时间戳，并且可以用于计算。当在计算中使用时，time attribute被物化，充当标准的时间戳。但是，常规时间戳办呢个用于替换或转换为时间属性。</p>
<p>###Event Time<br>
Event Time允许Table program根据每个记录中的时间戳产生结果，允许在无序或延迟事件的情况下产生一致的结果。它还确保了从持久存储读取记录时table program结果的可重复性。<br>
此外，Event Time允许批处理和流环境中的Table program使用统一的语法。Streaming environment中的time attribute可以是batch environment行中的常规列。<br>
为了处理无序事件并区分流中的准时事件和延迟事件，Flink需要知道每一行的时间戳，还需要定期指示到目前为止处理的事件时间进展了多少（通过所谓watermarks）<br>
Event Time 可以在CREATE table DDL 或 DataStream 到Table转换期间。</p>
<h4 id="defining-in-ddl">Defining in DDL</h4>
<p>在CREATE table DDL 中，使用WATERMARK 语句定义event time attribute。watermark 语句在已有event time 字段上定义了watermark的生成表达式，将event time 字段标记为event time attribute。</p>
<pre><code class="language-text">user_action_time TIMESTAMP(3),
  -- declare user_action_time as event time attribute and use 5 seconds delayed watermark strategy
  WATERMARK FOR user_action_time AS user_action_time - INTERVAL '5' SECOND
</code></pre>
<h4 id="during-datastream-to-table-conversion">During DataStream-to-Table Conversion</h4>
<p>当将DataStream转换为table时，可以在模式定义期间使用.rowtime属性定义事件时间属性。TimeStamp 和 watermark 必须已经在DataStream中已经分配。<br>
在将DataStream转换为Table时，有两种方式定义time attribute。依赖于指定的.rowtime字段名是否存在于DataStream的模式中，时间戳要么（1）append as a new column；要么（2）替换已经存在的某个列。</p>
<h3 id="processing-time">Processing Time</h3>
<p>Processing Time允许Table programe根据本地机器的时间产生结果。这是最简单的时间概念，但它会产生不确定性的结果。Processing Time不需要时间戳提取或水印生成<br>
三种方式定义Processing Time attribute</p>
<h4 id="defining-in-ddl-2">Defining in DDL</h4>
<p>在CREATE table DDL语句中，使用系统的PROCTIME()函数，Processing time attribute作为计算列被定义。</p>
<pre><code class="language-java">user_action_time AS PROCTIME() -- declare an additional field as a processing time attribute
</code></pre>
<h4 id="during-datastream-to-table-conversion-2">During DataStream-to-Table Conversion</h4>
<p>在模式定义中，processing time attribute使用.proctime属性定义。time attribute 必须仅通过附加的逻辑字段扩展物理模式。因此，它只能在模式定义的末尾进行定义。</p>
<pre><code class="language-java">// declare an additional logical field as a processing time attribute
Table table = tEnv.fromDataStream(stream, $(&quot;user_name&quot;), $(&quot;data&quot;), $(&quot;user_action_time&quot;).proctime()); 
</code></pre>
<h2 id="proctime-attributes">Proctime Attributes</h2>
<p>为了在模式中声明proctime属性，可以使用Computed Column语法声明从proctime()内置函数生成的Computed Column。计算列是一个虚拟列，它不存储在物理数据中</p>
<h2 id="rowtime-attributes">Rowtime Attributes</h2>
<p>为了控制表的event-time行为，Flink提供了预定义的时间戳提取器和水印策略。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flink Table & SQL]]></title>
        <id>https://wangyemao-github.github.io/post/flink-table-and-sql/</id>
        <link href="https://wangyemao-github.github.io/post/flink-table-and-sql/">
        </link>
        <updated>2021-04-13T02:31:09.000Z</updated>
        <content type="html"><![CDATA[<h1 id="table-sql-编程依赖dependency">Table &amp; SQL 编程依赖（Dependency）</h1>
<ul>
<li>为了使用Table API 或者SQL 定义pipeline，需要添加如下依赖（java 语言）：<br>
<dependency><br>
<groupId>org.apache.flink</groupId><br>
<artifactId>flink-table-api-java-bridge_2.11</artifactId><br>
<version>1.12.0</version><br>
<scope>provided</scope><br>
</dependency></li>
<li>另外，如果你想在本地IDE内运行Table API &amp; SQL程序，你需要添加如下modules集，不同的planner，依赖不一样。对应Blink planner 依赖如下：<br>
<dependency><br>
<groupId>org.apache.flink</groupId><br>
<artifactId>flink-table-planner-blink_2.11</artifactId><br>
<version>1.12.0</version><br>
<scope>provided</scope><br>
</dependency><br>
<dependency><br>
<groupId>org.apache.flink</groupId><br>
<artifactId>flink-streaming-scala_2.11</artifactId><br>
<version>1.12.0</version><br>
<scope>provided</scope><br>
</dependency></li>
<li>如果你想要实现一个定制化的format或connector，用于序列化/反序列化rows或用户定义的函数，添加如下依赖就足够了：<br>
<dependency><br>
<groupId>org.apache.flink</groupId><br>
<artifactId>flink-table-common</artifactId><br>
<version>1.12.0</version><br>
<scope>provided</scope><br>
</dependency></li>
</ul>
<h1 id="table-api-sql-编程的通用结构">Table API &amp; SQL 编程的通用结构</h1>
<p>对于批的、流式的Table API &amp; SQL 程序都遵从相同的模式。下面的例子显示了Table API &amp; SQL程序的相同结构：</p>
<pre><code class="language-java">// create a TableEnvironment for specific planner batch or streaming
TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section
// create an input Table
tableEnv.executeSql(&quot;CREATE TEMPORARY TABLE table1 ... WITH ( 'connector' = ... )&quot;);
// register an output Table
tableEnv.executeSql(&quot;CREATE TEMPORARY TABLE outputTable ... WITH ( 'connector' = ... )&quot;);
// create a Table object from a Table API query
Table table2 = tableEnv.from(&quot;table1&quot;).select(...);
// create a Table object from a SQL query
Table table3 = tableEnv.sqlQuery(&quot;SELECT ... FROM table1 ... &quot;);
// emit a Table API result Table to a TableSink, same for SQL result
TableResult tableResult = table2.executeInsert(&quot;outputTable&quot;);
tableResult...
</code></pre>
<h2 id="创建tableenvironment">创建TableEnvironment</h2>
<p><strong>TableEnvironment</strong>是Table API &amp; SQL集成的核心概念。它负责：</p>
<ul>
<li>在internal catalog中注册Table</li>
<li>注册catalog</li>
<li>加载可插拔的模块（pluggable modules）</li>
<li>执行SQL查询</li>
<li>注册用户定义的函数（scalar、table、aggregation）</li>
<li>将DataStream或DataSet转换为Table</li>
<li>维护对ExecutionEnvironment或StreamExecutionEnvironment的引用<br>
<strong>一个Table通常都绑定到一个具体的TableEnvironment。在同一个查询中不能将不同TableEnvironment中的table做组合</strong>（比如：join或union）<br>
TableEnvironment的创建：BatchTableEnvironment.create(StreamExecutionEnvironment ) 或 StreamTableEnvironment.create(ExecutionEnvironment )。另外，还要注意指定具体使用的planner：如果在classpath 路径下两个planner jar 包都存在，你需要显示指定当前应用使用哪一个planner。</li>
</ul>
<pre><code class="language-java">// **********************
// FLINK STREAMING QUERY
// **********************
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

EnvironmentSettings fsSettings = EnvironmentSettings.newInstance().useOldPlanner().inStreamingMode().build();
StreamExecutionEnvironment fsEnv = StreamExecutionEnvironment.getExecutionEnvironment();
StreamTableEnvironment fsTableEnv = StreamTableEnvironment.create(fsEnv, fsSettings);
// or TableEnvironment fsTableEnv = TableEnvironment.create(fsSettings);

// ******************
// FLINK BATCH QUERY
// ******************
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.table.api.bridge.java.BatchTableEnvironment;

ExecutionEnvironment fbEnv = ExecutionEnvironment.getExecutionEnvironment();
BatchTableEnvironment fbTableEnv = BatchTableEnvironment.create(fbEnv);

// **********************
// BLINK STREAMING QUERY
// **********************
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;

StreamExecutionEnvironment bsEnv = StreamExecutionEnvironment.getExecutionEnvironment();
EnvironmentSettings bsSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();
StreamTableEnvironment bsTableEnv = StreamTableEnvironment.create(bsEnv, bsSettings);
// or TableEnvironment bsTableEnv = TableEnvironment.create(bsSettings);

// ******************
// BLINK BATCH QUERY
// ******************
import org.apache.flink.table.api.EnvironmentSettings;
import org.apache.flink.table.api.TableEnvironment;

EnvironmentSettings bbSettings = EnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build();
TableEnvironment bbTableEnv = TableEnvironment.create(bbSettings);
</code></pre>
<p>如果，在/lib目录下只有一个plananer jar 包，可以使用useAnyPlanner创建具体的EnvironmentSetting。</p>
<p>#在catalog中创建Table</p>
<h2 id="catalogs">Catalogs</h2>
<p>提供了元数据，比如databases，tables，partitions，views和functions，以及需要去访问存储在数据库或外部系统数据的信息<br>
数据处理的一个最重要的方面就是管理元数据。它可以是像临时表一样的临时元数据，也可以是注册在table environment上的UDFS，也可以是持久化元数据，比如Hive Metastore。Catalogs提供了统一的API用于管理元数据，使得其可以通过Table API或SQL查询访问。<br>
Catalogs允许用户引用其数据系统的已有的元数据，并自动将其映射为Flink相应的元数据。比如，FLink可以自动地将JDBC表映射为Flink table，用户不用在Flink中手动重写DDLs。</p>
<h3 id="catalogs-types">Catalogs Types</h3>
<ul>
<li>GenericInMemoryCatalog : 基于内存实现的catalog。所有的对象仅在会话声明周期可用</li>
<li>JDBCCatalog ： 使得用户能通过JDBC协议连接Flink到关系型数据库。当前只实现了PostgresCatalog 。</li>
<li>HiveCatalog：The HiveCatalog serves two purposes; as persistent storage for pure Flink metadata, and as an interface for reading and writing existing Hive metaUser-Defined Catalogdata</li>
<li>User-Defined Catalog：通过实现Catalog接口，用户可以开发定制catalog。</li>
</ul>
<h2 id="table-indentifier">table indentifier</h2>
<p>TableEnvironment维护一个表目录映射，这些表目录是用标识符构建的。每个标识符由三部分组成：catalog名称、database名称和对象名称（table名称）。如果没有指定catalog或database，将使用当前的默认值。===》也就是说Tables通常注册为包含三部分的标识符。用户可以指定一个catalog和一个database 为“current catalog”和“current database”，这样在指定标识符时，前两部分就为可选的。</p>
<pre><code class="language-java">TableEnvironment tEnv = ...;
tEnv.useCatalog(&quot;custom_catalog&quot;);
tEnv.useDatabase(&quot;custom_database&quot;);
Table table = ...;
// register the view named 'exampleView' in the catalog named 'custom_catalog'
// in the database named 'custom_database' 
tableEnv.createTemporaryView(&quot;exampleView&quot;, table);
// register the view named 'exampleView' in the catalog named 'custom_catalog'
// in the database named 'other_database' 
tableEnv.createTemporaryView(&quot;other_database.exampleView&quot;, table);
// register the view named 'example.View' in the catalog named 'custom_catalog'
// in the database named 'custom_database' 
tableEnv.createTemporaryView(&quot;`example.View`&quot;, table);
// register the view named 'exampleView' in the catalog named 'other_catalog'
// in the database named 'other_database' 
tableEnv.createTemporaryView(&quot;other_catalog.other_database.exampleView&quot;, table);
</code></pre>
<h2 id="table-types">table types</h2>
<p>Table 可以是virtual（views） 也可以是正常的表（Tables）。views可以基于现有的Table创建，通常为一个Table API 或SQL查询的结果；Tables描述外部数据，比如一个文件，数据库表，或消息队列。</p>
<ul>
<li><strong>temporary table</strong> ： 与单个Flink 会话的生命周期关联。通常存储在内存中，仅在创建该Table的Flink会话期间存在。这些表对其他的Flink session是不可见的。它不与任何catalog或database绑定，但可以在其中的命名空间中创建。当对应的database被移除时，tempory table也不会被drop。</li>
<li><strong>permanent table</strong>：跨多个Flink会话和集群可见的。需要一个catalog(类似Hive Metastore)去维护table的元数据。一旦创建了一张permanent table，它对连接到该catalog的任何一个Flink session都是可见的，直到该Table被显示地Drop掉。</li>
</ul>
<p>**shadowing **：</p>
<h2 id="创建-table">创建 Table</h2>
<ul>
<li>virtual tables<br>
在SQL术语中，一个Table API 对象对应一个VIEW（virtual table）。它封装了逻辑查询计划，可以在catalog中创建：</li>
</ul>
<pre><code class="language-java">// get a TableEnvironment
TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section
// table is the result of a simple projection query 
Table projTable = tableEnv.from(&quot;X&quot;).select(...);
// register the Table projTable as table &quot;projectedTable&quot;
tableEnv.createTemporaryView(&quot;projectedTable&quot;, projTable);
</code></pre>
<ul>
<li>connector tables<br>
还可以使用connector 申明创建Table。connector 描述了存储表数据的外部系统。这里可以申明，比如：kafka或正常的文件系统</li>
</ul>
<pre><code class="language-java">tableEnvironment
  .connect(...)
  .withFormat(...)
  .withSchema(...)
  .inAppendMode()
  .createTemporaryTable(&quot;MyTable&quot;)
</code></pre>
<ul>
<li>create a view from a DataStream or DataSet<br>
一个DataStream或DataSet可以作为view注册到TableEnvironment中。结果视图的schema(模式)依赖于注册的DataStream或DataSet的数据类型。</li>
</ul>
<pre><code class="language-java">// get StreamTableEnvironment
// registration of a DataSet in a BatchTableEnvironment is equivalent
StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section
DataStream&lt;Tuple2&lt;Long, String&gt;&gt; stream = ...
// register the DataStream as View &quot;myTable&quot; with fields &quot;f0&quot;, &quot;f1&quot;
tableEnv.createTemporaryView(&quot;myTable&quot;, stream);
// register the DataStream as View &quot;myTable2&quot; with fields &quot;myLong&quot;, &quot;myString&quot;
tableEnv.createTemporaryView(&quot;myTable2&quot;, stream, $(&quot;myLong&quot;), $(&quot;myString&quot;));
</code></pre>
<p>** Data Types到Table schema的映射**</p>
<ol>
<li><strong>position-based mapping （基于字段位置的映射）</strong><br>
对于composite data types with a defined field order（such as tuples, rows, and case classes） 、atomic types可用；<br>
POJO中的字段必须使用基于名字的映射;<br>
字段可以被映射出来但是不能使用as 进行重命名;<br>
当定义基于位置的映射时，指定的字段名不能在输入数据类型中存在，否则，API将假定为基于名字的映射。</li>
</ol>
<pre><code class="language-java">   // get a StreamTableEnvironment, works for BatchTableEnvironment equivalently
StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section;
DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...
// convert DataStream into Table with default field names &quot;f0&quot; and &quot;f1&quot;
Table table = tableEnv.fromDataStream(stream);
// convert DataStream into Table with field &quot;myLong&quot; only
Table table = tableEnv.fromDataStream(stream, $(&quot;myLong&quot;));
// convert DataStream into Table with field names &quot;myLong&quot; and &quot;myInt&quot;
Table table = tableEnv.fromDataStream(stream, $(&quot;myLong&quot;), $(&quot;myInt&quot;));
</code></pre>
<ol start="2">
<li><strong>Name-based mapping（基于字段名称的映射）</strong><br>
可以用于所有包括POJO在内的数据类型。它是定义Table schema 映射的最灵活的方式。所有在映射中的字段都是通过名字引用，也可以使用 as 进行重命名。字段可以重排序投影。<br>
在没有指定字段名称时，会使用组合类型的默认字段名和字段顺序。</li>
</ol>
<pre><code class="language-java">// get a StreamTableEnvironment, works for BatchTableEnvironment equivalently
StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section
DataStream&lt;Tuple2&lt;Long, Integer&gt;&gt; stream = ...
// convert DataStream into Table with default field names &quot;f0&quot; and &quot;f1&quot;
Table table = tableEnv.fromDataStream(stream);
// convert DataStream into Table with field &quot;f1&quot; only
Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;));
// convert DataStream into Table with swapped fields
Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;), $(&quot;f0&quot;));
// convert DataStream into Table with swapped fields and field names &quot;myInt&quot; and &quot;myLong&quot;
Table table = tableEnv.fromDataStream(stream, $(&quot;f1&quot;).as(&quot;myInt&quot;), $(&quot;f0&quot;).as(&quot;myLong&quot;));
</code></pre>
<h1 id="查询table">查询Table</h1>
<h2 id="table-api">Table API</h2>
<pre><code class="language-java">// get a TableEnvironment
TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section
// register Orders table
// scan registered Orders table
Table orders = tableEnv.from(&quot;Orders&quot;);
// compute revenue for all customers from France
Table revenue = orders
  .filter($(&quot;cCountry&quot;).isEqual(&quot;FRANCE&quot;))
  .groupBy($(&quot;cID&quot;), $(&quot;cName&quot;)
  .select($(&quot;cID&quot;), $(&quot;cName&quot;), $(&quot;revenue&quot;).sum().as(&quot;revSum&quot;));
// emit or convert Table
// execute query
</code></pre>
<h2 id="sql">SQL</h2>
<pre><code class="language-java">// get a TableEnvironment
TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section
// register Orders table
// compute revenue for all customers from France
Table revenue = tableEnv.sqlQuery(
    &quot;SELECT cID, cName, SUM(revenue) AS revSum &quot; +
    &quot;FROM Orders &quot; +
    &quot;WHERE cCountry = 'FRANCE' &quot; +
    &quot;GROUP BY cID, cName&quot;
  );

  // compute revenue for all customers from France and emit to &quot;RevenueFrance&quot;
tableEnv.executeSql(
    &quot;INSERT INTO RevenueFrance &quot; +
    &quot;SELECT cID, cName, SUM(revenue) AS revSum &quot; +
    &quot;FROM Orders &quot; +
    &quot;WHERE cCountry = 'FRANCE' &quot; +
    &quot;GROUP BY cID, cName&quot;
  );
// emit or convert Table
// execute query
</code></pre>
<p><strong>Table API 和 SQL 查询能够很容易地混合，因为两者都返回Table 对象</strong>：</p>
<ul>
<li>Table API 查询可以在SQL查询返回的Table 对象上定义</li>
<li>SQL查询也可以定义在Table API 查询结果上。通过在TableEnvironment上注册结果表，然后再SQL查询的from子句中引用</li>
</ul>
<h1 id="emit-a-table">Emit a table</h1>
<p>a Table is emitted by writing it to a TableSink。 TableSink 是一个支持各种不同文件格式（CSV，Apache Parquet，Apache Avro），存储系统（JDBC，Apache Hbase，Cassandra，Elasticsearch），消息系统（kafka，rabbitMQ）的通用接口。<br>
batch table 只能写到BatchTableSink；streaming table 可以写到AppendStreamTableSink、RetractStreamTableSink或UpsertStreamTableSink<br>
Table.executeInsert(String tableName) 方法emit the table to a registered TableSink。</p>
<pre><code class="language-java">// get a TableEnvironment
TableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section
// create an output Table
final Schema schema = new Schema()
    .field(&quot;a&quot;, DataTypes.INT())
    .field(&quot;b&quot;, DataTypes.STRING())
    .field(&quot;c&quot;, DataTypes.BIGINT());
tableEnv.connect(new FileSystem().path(&quot;/path/to/file&quot;))
    .withFormat(new Csv().fieldDelimiter('|').deriveSchema())
    .withSchema(schema)
    .createTemporaryTable(&quot;CsvSinkTable&quot;);
// compute a result Table using Table API operators and/or SQL queries
Table result = ...
// emit the result Table to the registered TableSink
result.executeInsert(&quot;CsvSinkTable&quot;);
</code></pre>
<h1 id="convert-a-table-into-a-datastream-or-dataset">convert  a Table into a DataStream or DataSet</h1>
<figure data-type="image" tabindex="1"><img src="https://wangyemao-github.github.io/post-images/1618299207727.png" alt="" loading="lazy"></figure>
<h2 id="conver-a-table-into-a-datastream">conver a Table into a DataStream</h2>
<p>对于流查询的结果表将会动态地更新。即，它会随着新记录达到查询输入流而变化。因此，将动态查询转换成的DataStream需要对表的更新进行编码。将Table转换为DataStream有两种模式：</p>
<ol>
<li>Append Mode : 这种模式仅仅用于动态Table仅被INSERT 修改的情况。也就是数据记录只有append，之前的数据不会被修改</li>
<li>Retract Mode ： 这种模式经常被使用。它使用boolean 标识覆盖了INSERT和DELETE的变化</li>
</ol>
<pre><code class="language-java">// get StreamTableEnvironment. 
StreamTableEnvironment tableEnv = ...; // see &quot;Create a TableEnvironment&quot; section
// Table with two fields (String name, Integer age)
Table table = ...
// convert the Table into an append DataStream of Row by specifying the class
DataStream&lt;Row&gt; dsRow = tableEnv.toAppendStream(table, Row.class);
// convert the Table into an append DataStream of Tuple2&lt;String, Integer&gt; 
//   via a TypeInformation
TupleTypeInfo&lt;Tuple2&lt;String, Integer&gt;&gt; tupleType = new TupleTypeInfo&lt;&gt;(
  Types.STRING(),
  Types.INT());
DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dsTuple = 
  tableEnv.toAppendStream(table, tupleType);
// convert the Table into a retract DataStream of Row.
//   A retract stream of type X is a DataStream&lt;Tuple2&lt;Boolean, X&gt;&gt;. 
//   The boolean field indicates the type of the change. 
//   True is INSERT, false is DELETE.
DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; retractStream = 
  tableEnv.toRetractStream(table, Row.class);
</code></pre>
<p>一旦将Table转换为DataStream，需要使用StreamExecutionEnvironment.execute() 方法来执行DataStream程序。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[JStack]]></title>
        <id>https://wangyemao-github.github.io/post/jstack/</id>
        <link href="https://wangyemao-github.github.io/post/jstack/">
        </link>
        <updated>2021-04-12T03:20:05.000Z</updated>
        <content type="html"><![CDATA[<h1 id="jstack是什么">JStack是什么？</h1>
<p>jstack 是java虚拟机自带的一种堆栈跟踪工具。**作用：**生成JVM当前时刻线程的快照（threaddump，当前进程中所有线程的信息）；**目的：**帮助定位程序问题出现的原因，如长时间停顿、CPU占用率过高等。<br>
<strong>功能</strong>：<br>
jstack用于生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因【通过jstack来查看各个线程的调用堆栈，就可以知道线程到底在后台做什么事情，或者等待什么资源】，如线程间死锁、死循环、请求外部资源导致的长时间等待等。<br>
---如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题<br>
---对于java 程序hung住的状态，jstack非常有用。</p>
<p>**总结：**jstack命令主要用来查看Java线程的调用堆栈的，可以用来分析线程问题（如死锁）</p>
<h1 id="jstack-dump日志文件中指得关注的线程状态">jstack dump日志文件中，指得关注的线程状态</h1>
<ol>
<li><strong>死锁，Deadlock（重点关注）</strong></li>
<li>执行中，Runnable</li>
<li><strong>等待资源，Waiting on condition（重点关注）</strong></li>
<li><strong>等待获取监视器，Waiting on monitor entry（重点关注）</strong></li>
<li>暂停，Suspended</li>
<li>对象等待中，Object.wait() 或 TIMED_WAITIN</li>
<li><strong>阻塞，Blocked（重点关注）</strong></li>
<li>停止，Parked</li>
</ol>
<h1 id="jstack-命令">jstack 命令</h1>
<pre><code class="language-text">hollis@hos:~$ jstack -help
Usage:
    jstack [-l] &lt;pid&gt;
        (to connect to running process)
    jstack -F [-m] [-l] &lt;pid&gt;
        (to connect to a hung process)
    jstack [-m] [-l] &lt;executable&gt; &lt;core&gt;
        (to connect to a core file)
    jstack [-m] [-l] [server_id@]&lt;remote server IP or hostname&gt;
        (to connect to a remote debug server)

Options:
    -F  to force a thread dump. Use when jstack &lt;pid&gt; does not respond (process is hung)
    -m  to print both java and native frames (mixed mode)
    -l  long listing. Prints additional information about locks
    -h or -help to print this help message
</code></pre>
<p>#jstack dump 日志</p>
<h2 id="如何查看jstack-dump-日志">如何查看JStack Dump 日志</h2>
<p><img src="https://wangyemao-github.github.io/post-images/1618197743245.png" alt="" loading="lazy"><br>
上图jstack dump日志解读：</p>
<ul>
<li>线程状态是 Blocked，阻塞状态。说明线程等待资源超时！</li>
<li>“ waiting to lock &lt;0x00000000acf4d0c0&gt;”指，线程在等待给这个 0x00000000acf4d0c0 地址上锁（英文可描述为：trying to obtain 0x00000000acf4d0c0 lock）。</li>
<li>在 dump 日志里查找字符串 0x00000000acf4d0c0，发现有大量线程都在等待给这个地址上锁。如果能在日志里找到谁获得了这个锁（如locked &lt; 0x00000000acf4d0c0 &gt;），就可以顺藤摸瓜了。</li>
<li>“waiting for monitor entry”说明此线程通过 synchronized(obj) {……} 申请进入了临界区，从而进入了下图1中的“Entry Set”队列，但该 obj 对应的 monitor 被其他线程拥有，所以本线程在 Entry Set 队列中等待。</li>
<li>第一行里，”RMI TCP Connection(267865)-172.16.5.25”是 Thread Name 。tid指Java Thread id。nid指native线程的id【OS层面的线程id，top命令显示的就是OS层面的线程id】。prio是线程优先级。[0x00007fd4f8684000]是线程栈起始地址。</li>
</ul>
<h2 id="线程状态说明">线程状态说明</h2>
<p>线程的状态有：NEW、RUNNABLE、BLOCKED、WAITING、TIMED_WAITING、TERMINATED。其中，NEW为未启动的，不会出现在dump 日志文件中的状态。</p>
<ul>
<li>Deadlock：死锁线程，一般指多个线程调用间，进入相互资源占用，导致一直等待无法释放的情况</li>
<li>Runnable：一般指该线程正在执行状态中，该线程占用了资源，正在处理某个请求，有可能正在传递SQL到数据库执行，有可能在对某个文件操作，有可能进行数据类型等转换</li>
<li>Waiting on condition：等待资源，或等待某个条件的发生。具体原因需结合 stacktrace来分析
<ol>
<li>如果堆栈信息明确是应用代码，则证明该线程正在等待资源。一般是大量读取某资源，且该资源采用了资源锁的情况下，线程进入等待状态，等待资源的读取；又或者，正在等待其他线程的执行等。</li>
<li>如果发现有大量的线程都在处在 Wait on condition，从线程 stack看，正等待网络读写，这可能是一个网络瓶颈的征兆</li>
<li>另外一种出现 Wait on condition的常见情况是该线程在 sleep，等待 sleep的时间到了时候，将被唤醒</li>
</ol>
</li>
<li>Blocked：线程阻塞，是指当前线程执行过程中，所需要的资源长时间等待却一直未能获取到，被容器的线程管理器标识为阻塞状态，可以理解为等待资源超时的线程</li>
<li>Waiting for monitor entry 和 in Object.wait()：Monitor是 Java中用以实现线程之间的互斥与协作的主要手段，它可以看成是对象或者 Class的锁。每一个对象都有，也仅有一个 monitor。</li>
</ul>
<h2 id="关于objectclass-monitor">关于Object/Class Monitor</h2>
<p>在多线程的JAVA程序中，<strong>Monitor是java中用以实现线程之间的互斥与协作的主要手段</strong>。它可以看成是对象或Class的锁，每一个对象都有且仅有一个monitor。<br>
<img src="https://wangyemao-github.github.io/post-images/1618198555803.png" alt="" loading="lazy"><br>
每个 Monitor在某个时刻，只能被一个线程拥有，该线程就是 “Active Thread”，而其它线程都是 “Waiting Thread”，分别在两个队列 “ Entry Set”和 “Wait Set”里面等候。在 “Entry Set”中等待的线程状态是 “Waiting for monitor entry”，而在 “Wait Set”中等待的线程状态是 “in Object.wait()”。</p>
<pre><code class="language-java">synchronized(obj){
    ......
}
</code></pre>
<h3 id="调用修饰">调用修饰</h3>
<p>表示线程在方法调用时,额外的重要的操作。</p>
<ul>
<li>locked &lt;地址&gt; 目标: 使用synchronized申请对象锁成功,监视器的拥有者<br>
<img src="https://wangyemao-github.github.io/post-images/1618209989027.png" alt="" loading="lazy"><br>
通过synchronized关键字,成功获取到了对象的锁,成为监视器的拥有者,在临界区内操作。对象锁是可以线程重入的</li>
<li>waiting to lock &lt;地址&gt; 目标：使用synchronized申请对象锁未成功,在进入区等待<br>
<img src="https://wangyemao-github.github.io/post-images/1618212008930.png" alt="" loading="lazy"><br>
通过synchronized关键字,没有获取到了对象的锁,线程在监视器的进入区等待。在调用栈顶出现,线程状态为Blocked</li>
<li>waiting on &lt;地址&gt; 目标：使用synchronized申请对象锁成功后,释放锁并在等待区等待<br>
<img src="https://wangyemao-github.github.io/post-images/1618210054705.png" alt="" loading="lazy"><br>
通过synchronized关键字,成功获取到了对象的锁后,调用了wait方法,进入对象的等待区等待。在调用栈顶出现,线程状态为WAITING或TIMED_WATING</li>
<li>parking to wait for &lt;地址&gt; 目标<br>
park是基本的线程阻塞原语,不通过监视器在对象上阻塞。随concurrent包会出现的新的机制,与synchronized体系不同</li>
</ul>
<h2 id="实例">实例</h2>
<h3 id="waiting-on-condition-and-timed_waiting">waiting on condition and TIMED_WAITING</h3>
<figure data-type="image" tabindex="1"><img src="https://wangyemao-github.github.io/post-images/1618206244210.png" alt="" loading="lazy"></figure>
<ul>
<li>&quot;TIMED_WAITING(parking)&quot;中的timed_waiting 表示指定了时间的等待状态，当到达指定时间后自动退出等待状态；parking指线程处于挂起中。</li>
<li>“waiting on condition”需要与堆栈中的“parking to wait for &lt;0x00000000acd84de8&gt; (a java.util.concurrent.SynchronousQueue$TransferStack)”结合来看。首先，本线程肯定是在等待某个条件的发生，来把自己唤醒。其次，SynchronousQueue 并不是一个队列，只是线程之间移交信息的机制，当我们把一个元素放入到 SynchronousQueue 中时必须有另一个线程正在等待接受移交的任务，因此这就是本线程在等待的条件</li>
</ul>
<h3 id="in-objectwait-and-timed_waiting">in Object.wait() and TIMED_WAITING</h3>
<figure data-type="image" tabindex="2"><img src="https://wangyemao-github.github.io/post-images/1618206737075.png" alt="" loading="lazy"></figure>
<ul>
<li>“TIMED_WAITING (on object monitor)”，对于本例而言，是因为本线程调用了 java.lang.Object.wait(long timeout) 而进入等待状态</li>
<li>“Wait Set”中等待的线程状态就是“ in Object.wait() ”。当线程获得了 Monitor，进入了临界区之后，如果发现线程继续运行的条件没有满足，它则调用对象（一般就是被 synchronized 的对象）的 wait() 方法，放弃了 Monitor，进入 “Wait Set”队列。只有当别的线程在该对象上调用了 notify() 或者 notifyAll() ，“ Wait Set”队列中线程才得到机会去竞争，但是只有一个线程获得对象的 Monitor，恢复到运行态</li>
</ul>
<h2 id="线程dump的分析">线程Dump的分析</h2>
<p>一些入手点说明：</p>
<h3 id="进入区等待">进入区等待</h3>
<p><img src="https://wangyemao-github.github.io/post-images/1618210608128.png" alt="" loading="lazy"><br>
线程状态BLOCKED,线程动作wait on monitor entry,调用修饰waiting to lock总是一起出现。表示在代码级别已经存在冲突的调用。必然有问题的代码,需要尽可能减少其发生。</p>
<h3 id="同步块阻塞">同步块阻塞</h3>
<p><img src="https://wangyemao-github.github.io/post-images/1618210749419.png" alt="" loading="lazy"><br>
一个线程锁住某对象,大量其他线程在该对象上等待。</p>
<h3 id="持续运行的io">持续运行的IO</h3>
<p><img src="https://wangyemao-github.github.io/post-images/1618210836280.png" alt="" loading="lazy"><br>
IO操作是可以以RUNNABLE状态达成阻塞。例如:数据库死锁、网络读写。 格外注意对IO线程的真实状态的分析。 一般来说,被捕捉到RUNNABLE的IO调用,都是有问题的。 以下堆栈显示： 线程状态为RUNNABLE。 调用栈在SocketInputStream或SocketImpl上,socketRead0等方法。 调用栈包含了jdbc相关的包。很可能发生了数据库死锁</p>
<h3 id="总结">总结：</h3>
<ul>
<li>wait on monitor entry ： 被阻塞的肯定有问题</li>
<li>runnable：注意IO线程</li>
<li>in object.wait() : 注意非线程池等待</li>
</ul>
<h1 id="jstack-dump-日志中一些常见线程整理">jstack dump 日志中一些常见线程整理</h1>
<p>下表中整理的线程都是归属JVM的线程，通常在对每个应用进行进行jstack dump时，日志文件中都会出现如下一些线程：<br>
|线程|所属|说明|<br>
|&quot;Attach Listener&quot;|JVM|Attach Listener线程是负责接收到外部的命令，而对该命令进行执行的并且吧结果返回给发送者。通常我们会用一些命令去要求jvm给我们一些反馈信息，如：java -version、jmap、jstack等等。如果该线程在jvm启动的时候没有初始化，那么，则会在用户第一次执行jvm命令时，得到启动|<br>
|&quot;Service Thread&quot; |JVM|前面我们提到第一个Attach Listener线程的职责是接收外部jvm命令，当命令接收成功后，会交给signal dispather线程去进行分发到各个不同的模块处理命令，并且返回处理结果。signal dispather线程也是在第一次接收外部jvm命令时，进行初始化工作|<br>
|&quot;CompilerThreadX&quot;|JVM|用来调用JITing，实时编译装卸class。通常，jvm会启动多个线程来处理这部分工作，线程名称后面的数字也会累加，例如：CompilerThread1|<br>
|&quot;Reference Handler&quot;|JVM|JVM在创建main线程后就创建Reference Handler线程，其优先级最高，为10，它主要用于处理引用对象本身（软引用、弱引用、虚引用）的垃圾回收问题|<br>
|&quot;Finalizer&quot;|JVM|这个线程也是在main线程之后创建的，其优先级为10，主要用于在垃圾收集前，调用对象的finalize()方法；关于Finalizer线程的几点：1)只有当开始一轮垃圾收集时，才会开始调用finalize()方法；因此并不是所有对象的finalize()方法都会被执行；2)该线程也是daemon线程，因此如果虚拟机中没有其他非daemon线程，不管该线程有没有执行完finalize()方法，JVM也会退出；3) JVM在垃圾收集时会将失去引用的对象包装成Finalizer对象（Reference的实现），并放入ReferenceQueue，由Finalizer线程来处理；最后将该Finalizer对象的引用置为null，由垃圾收集器来回收；4) JVM为什么要单独用一个线程来执行finalize()方法呢？如果JVM的垃圾收集线程自己来做，很有可能由于在finalize()方法中误操作导致GC线程停止或不可控，这对GC线程来说是一种灾难；|<br>
|&quot;DestroyJavaVM&quot; |JVM|执行main()的线程在main执行完后调用JNI中的jni_DestroyJavaVM()方法唤起DestroyJavaVM线程.线程退出时，都会判断自己当前是否是整个JVM中最后一个非deamon线程，如果是，则通知DestroyJavaVM线程卸载JVM。ps：扩展一下：1.如果线程退出时判断自己不为最后一个非deamon线程，那么调用thread-&gt;exit(false)，并在其中抛出thread_end事件，jvm不退出。2.如果线程退出时判断自己为最后一个非deamon线程，那么调用before_exit()方法，抛出两个事件： 事件1：thread_end线程结束事件、事件2：VM的death事件。然后调用thread-&gt;exit(true)方法，接下来把线程从active list卸下，删除线程等等一系列工作执行完成后，则通知正在等待的DestroyJavaVM线程执行卸载JVM操作|<br>
实时上，我们更多地是需要关注应用相关的线程。</p>
<h2 id="jstack排查java的cpu性能问题">jstack排查java的CPU性能问题</h2>
<figure data-type="image" tabindex="3"><img src="https://wangyemao-github.github.io/post-images/1618212296706.png" alt="" loading="lazy"></figure>
<p>针对这个排查流程，github上开源的快速定位脚本：https://github.com/oldratlee/useful-scripts/blob/dev-2.x/docs/java.md#-show-busy-java-threads</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flink Docker 安装部署]]></title>
        <id>https://wangyemao-github.github.io/post/flink-docker-an-zhuang-bu-shu/</id>
        <link href="https://wangyemao-github.github.io/post/flink-docker-an-zhuang-bu-shu/">
        </link>
        <updated>2021-04-01T11:38:32.000Z</updated>
        <content type="html"><![CDATA[<p>本文为基于docker 安装部署flink集群中得一些笔记。</p>
<h1 id="关于flink-docker-概述">关于Flink Docker 概述</h1>
<p>基于Docker 的Flink 集群部署，支持Session 和Application cluster 部署模式。具体可参见官网：https://ci.apache.org/projects/flink/flink-docs-release-1.12/deployment/resource-providers/standalone/docker.html。并且提供了官方的Flink Docker 镜像 https://hub.docker.com/_/flink/。<br>
直接使用提供的镜像进行Flink 集群部署存在如下一些问题：</p>
<ul>
<li>flink conf目录下的配置文件都在容器内，不便于动态修改、调整</li>
<li>flink运行过程中执行的快照结果，可能因为容器的崩溃而丢失</li>
</ul>
<h1 id="具体实践参考">具体实践参考</h1>
<p>解决上述问题的一个可行方案就是使用docker volume。 volume的官方定义为：A data volume is a specially-designated directory within one or more containers that bypasses the Union File System。它的主要用途就在于：在运行容器间实现数据持久化和共享。</p>
<h1 id="使用volume可能回引发的问题">使用volume可能回引发的问题</h1>
<p>volume使得容器与容器之间，或者容器与宿主机之间能够共享目录。但是容器运行的用户和宿主机上运行的用户通常不会是相同的用户，这就可能导致：</p>
<ul>
<li>容器运行时写入目录的文件，宿主机上的用户不能访问；</li>
<li>容器运行时用户没有权限写入文件</li>
</ul>
<p>总结来说，因为宿主机和容器运行时用户不是相同的用户，导致了使用volume时带来的用户权限问题。换句话来说，如果宿主机上的当前用户与容器运行时的用户是同一个用户，那容器运行时写入的文件，宿主机上用户也有权限访问；宿主机上创建的文件，容器内也有权限进行操作（读写）。</p>
<h1 id="基于docker-的flink-session安装">基于Docker 的Flink Session安装</h1>
<p>参考：https://denibertovic.com/posts/handling-permissions-with-docker-volumes/ 中提到的解决方法，对官方提供的Dockerfile 和 docker-entrypoint.sh文件进行调整。<br>
调整后的Dockerfile为：</p>
<pre><code class="language-text">FROM openjdk:8-jre

# Install dependencies
RUN set -ex; \
  apt-get update; \
  apt-get -y install libsnappy1v5 gettext-base libjemalloc-dev; \
  rm -rf /var/lib/apt/lists/*

# Grab gosu for easy step-down from root
ENV GOSU_VERSION 1.11
RUN set -ex; \
  wget -nv -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture)&quot;; \
  wget -nv -O /usr/local/bin/gosu.asc &quot;https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg --print-architecture).asc&quot;; \
  export GNUPGHOME=&quot;$(mktemp -d)&quot;; \
  for server in ha.pool.sks-keyservers.net $(shuf -e \
                          hkp://p80.pool.sks-keyservers.net:80 \
                          keyserver.ubuntu.com \
                          hkp://keyserver.ubuntu.com:80 \
                          pgp.mit.edu) ; do \
      gpg --batch --keyserver &quot;$server&quot; --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4 &amp;&amp; break || : ; \
  done &amp;&amp; \
  gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu; \
  gpgconf --kill all; \
  rm -rf &quot;$GNUPGHOME&quot; /usr/local/bin/gosu.asc; \
  chmod +x /usr/local/bin/gosu; \
  gosu nobody true

# Configure Flink version
ENV FLINK_TGZ_URL=https://www.apache.org/dyn/closer.cgi?action=download&amp;filename=flink/flink-1.12.2/flink-1.12.2-bin-scala_2.11.tgz \
    FLINK_ASC_URL=https://www.apache.org/dist/flink/flink-1.12.2/flink-1.12.2-bin-scala_2.11.tgz.asc \
    GPG_KEY=0D545F264D2DFDEBFD4E038F97B4625E2FCF517C \
    CHECK_GPG=true

# Prepare environment
ENV FLINK_HOME=/opt/flink
ENV PATH=$FLINK_HOME/bin:$PATH
#RUN groupadd --system --gid=9999 flink &amp;&amp; \
#    useradd --system --home-dir $FLINK_HOME --uid=9999 --gid=flink flink
WORKDIR $FLINK_HOME

# Install Flink
RUN set -ex; \
  wget -nv -O flink.tgz &quot;$FLINK_TGZ_URL&quot;; \
  \
  if [ &quot;$CHECK_GPG&quot; = &quot;true&quot; ]; then \
    wget -nv -O flink.tgz.asc &quot;$FLINK_ASC_URL&quot;; \
    export GNUPGHOME=&quot;$(mktemp -d)&quot;; \
    for server in ha.pool.sks-keyservers.net $(shuf -e \
                            hkp://p80.pool.sks-keyservers.net:80 \
                            keyserver.ubuntu.com \
                            hkp://keyserver.ubuntu.com:80 \
                            pgp.mit.edu) ; do \
        gpg --batch --keyserver &quot;$server&quot; --recv-keys &quot;$GPG_KEY&quot; &amp;&amp; break || : ; \
    done &amp;&amp; \
    gpg --batch --verify flink.tgz.asc flink.tgz; \
    gpgconf --kill all; \
    rm -rf &quot;$GNUPGHOME&quot; flink.tgz.asc; \
  fi; \
  \
  tar -xf flink.tgz --strip-components=1; \
  rm flink.tgz; 
#  \
#  \
#  chown -R flink:flink .;

# Configure container
COPY docker-entrypoint.sh /
RUN chmod +x /docker-entrypoint.sh
ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]
EXPOSE 6123 8081
CMD [&quot;help&quot;]
</code></pre>
<p>调整后的docker-entryoint.sh 为：</p>
<pre><code class="language-text">#!/usr/bin/env bash
COMMAND_STANDALONE=&quot;standalone-job&quot;
# Deprecated, should be remove in Flink release 1.13
COMMAND_NATIVE_KUBERNETES=&quot;native-k8s&quot;
COMMAND_HISTORY_SERVER=&quot;history-server&quot;

# If unspecified, the hostname of the container is taken as the JobManager address
JOB_MANAGER_RPC_ADDRESS=${JOB_MANAGER_RPC_ADDRESS:-$(hostname -f)}
CONF_FILE=&quot;${FLINK_HOME}/conf/flink-conf.yaml&quot;
USER_ID=${LOCAL_USER_ID:-9001}

drop_privs_cmd() {
	echo &quot;Starting with UID : $USER_ID&quot;
	useradd --shell /bin/bash -u $USER_ID -o -c &quot;&quot; -m flink
	chown -R flink:flink /opt/flink
    if [ $(id -u) != 0 ]; then
        # Don't need to drop privs if EUID != 0
        return
    elif [ -x /sbin/su-exec ]; then
        # Alpine
        echo su-exec flink
    else
        # Others
        echo gosu flink
    fi
}

copy_plugins_if_required() {
  if [ -z &quot;$ENABLE_BUILT_IN_PLUGINS&quot; ]; then
    return 0
  fi

  echo &quot;Enabling required built-in plugins&quot;
  for target_plugin in $(echo &quot;$ENABLE_BUILT_IN_PLUGINS&quot; | tr ';' ' '); do
    echo &quot;Linking ${target_plugin} to plugin directory&quot;
    plugin_name=${target_plugin%.jar}

    mkdir -p &quot;${FLINK_HOME}/plugins/${plugin_name}&quot;
    if [ ! -e &quot;${FLINK_HOME}/opt/${target_plugin}&quot; ]; then
      echo &quot;Plugin ${target_plugin} does not exist. Exiting.&quot;
      exit 1
    else
      ln -fs &quot;${FLINK_HOME}/opt/${target_plugin}&quot; &quot;${FLINK_HOME}/plugins/${plugin_name}&quot;
      echo &quot;Successfully enabled ${target_plugin}&quot;
    fi
  done
}

set_config_option() {
  local option=$1
  local value=$2

  # escape periods for usage in regular expressions
  local escaped_option=$(echo ${option} | sed -e &quot;s/\./\\\./g&quot;)

  # either override an existing entry, or append a new one
  if grep -E &quot;^${escaped_option}:.*&quot; &quot;${CONF_FILE}&quot; &gt; /dev/null; then
        sed -i -e &quot;s/${escaped_option}:.*/$option: $value/g&quot; &quot;${CONF_FILE}&quot;
  else
        echo &quot;${option}: ${value}&quot; &gt;&gt; &quot;${CONF_FILE}&quot;
  fi
}

prepare_configuration() {
    set_config_option jobmanager.rpc.address ${JOB_MANAGER_RPC_ADDRESS}
    set_config_option blob.server.port 6124
    set_config_option query.server.port 6125

    TASK_MANAGER_NUMBER_OF_TASK_SLOTS=${TASK_MANAGER_NUMBER_OF_TASK_SLOTS:-1}
    set_config_option taskmanager.numberOfTaskSlots ${TASK_MANAGER_NUMBER_OF_TASK_SLOTS}

    if [ -n &quot;${FLINK_PROPERTIES}&quot; ]; then
        echo &quot;${FLINK_PROPERTIES}&quot; &gt;&gt; &quot;${CONF_FILE}&quot;
    fi
    envsubst &lt; &quot;${CONF_FILE}&quot; &gt; &quot;${CONF_FILE}.tmp&quot; &amp;&amp; mv &quot;${CONF_FILE}.tmp&quot; &quot;${CONF_FILE}&quot;
}

maybe_enable_jemalloc() {
    if [ &quot;${DISABLE_JEMALLOC:-false}&quot; == &quot;false&quot; ]; then
        export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libjemalloc.so
    fi
}

maybe_enable_jemalloc

copy_plugins_if_required

prepare_configuration

args=(&quot;$@&quot;)
if [ &quot;$1&quot; = &quot;help&quot; ]; then
    printf &quot;Usage: $(basename &quot;$0&quot;) (jobmanager|${COMMAND_STANDALONE}|taskmanager|${COMMAND_HISTORY_SERVER})\n&quot;
    printf &quot;    Or $(basename &quot;$0&quot;) help\n\n&quot;
    printf &quot;By default, Flink image adopts jemalloc as default memory allocator. This behavior can be disabled by setting the 'DISABLE_JEMALLOC' environment variable to 'true'.\n&quot;
    exit 0
elif [ &quot;$1&quot; = &quot;jobmanager&quot; ]; then
    args=(&quot;${args[@]:1}&quot;)

    echo &quot;Starting Job Manager&quot;

    exec $(drop_privs_cmd) &quot;$FLINK_HOME/bin/jobmanager.sh&quot; start-foreground &quot;${args[@]}&quot;
elif [ &quot;$1&quot; = ${COMMAND_STANDALONE} ]; then
    args=(&quot;${args[@]:1}&quot;)

    echo &quot;Starting Job Manager&quot;

    exec $(drop_privs_cmd) &quot;$FLINK_HOME/bin/standalone-job.sh&quot; start-foreground &quot;${args[@]}&quot;
elif [ &quot;$1&quot; = ${COMMAND_HISTORY_SERVER} ]; then
    args=(&quot;${args[@]:1}&quot;)

    echo &quot;Starting History Server&quot;

    exec $(drop_privs_cmd) &quot;$FLINK_HOME/bin/historyserver.sh&quot; start-foreground &quot;${args[@]}&quot;
elif [ &quot;$1&quot; = &quot;taskmanager&quot; ]; then
    args=(&quot;${args[@]:1}&quot;)

    echo &quot;Starting Task Manager&quot;

    exec $(drop_privs_cmd) &quot;$FLINK_HOME/bin/taskmanager.sh&quot; start-foreground &quot;${args[@]}&quot;
elif [ &quot;$1&quot; = &quot;$COMMAND_NATIVE_KUBERNETES&quot; ]; then
    args=(&quot;${args[@]:1}&quot;)

    export _FLINK_HOME_DETERMINED=true
    . $FLINK_HOME/bin/config.sh
    export FLINK_CLASSPATH=&quot;`constructFlinkClassPath`:$INTERNAL_HADOOP_CLASSPATHS&quot;
    # Start commands for jobmanager and taskmanager are generated by Flink internally.
    echo &quot;Start command: ${args[@]}&quot;
    exec $(drop_privs_cmd) bash -c &quot;${args[@]}&quot;
fi

args=(&quot;${args[@]}&quot;)

# Set the Flink related environments
export _FLINK_HOME_DETERMINED=true
. $FLINK_HOME/bin/config.sh
export FLINK_CLASSPATH=&quot;`constructFlinkClassPath`:$INTERNAL_HADOOP_CLASSPATHS&quot;

# Running command in pass-through mode
exec $(drop_privs_cmd) &quot;${args[@]}&quot;
</code></pre>
<h2 id="安装部署步骤">安装部署步骤</h2>
<ol>
<li>构建镜像<br>
将调整后的Dockerfile文件和docker-entrypoint.sh 文件copy到同一个目录下。执行如下命令构建镜像<br>
docker build -t wmz/flink:1.12.2_2.11 .</li>
<li>创建网络<br>
docker network create flink-network ：容器间使用同一个的网络，使得相互间可以通过容器名进行访问。</li>
<li>启动jobmanager</li>
</ol>
<pre><code class="language-text">docker run -itd --name=myjobmanager -h myjobmanager --network flink-network --publish 8081:8081 --env LOCAL_USER_ID=`id -u $USER` -v /root/wym/conf:/opt/flink/conf -v /root/wym/data/save_point:/opt/flink/save_point -v /root/wym/data/check_point:/opt/flink/check_point -v /root/wym/data/blob_store:/opt/flink/blob_store -v /root/wym/data/upload_dir:/opt/flink/upload_dir -v /root/wym/data/log:/opt/flink/log flink1:latest jobmanager 
</code></pre>
<p>说明：<br>
(1)--env LOCAL_USER_ID=<code>id -u $USER</code>  将当前宿主机上的用户的uid作为环境变量传到容器内；容器内会基于改用户uid创建容器用户，并运行容器。这就保证了宿主机上的用户与容器内运行时的用户时相同的用户<br>
4. 启动taskmanager</p>
<pre><code class="language-text">docker run -itd --name=mytaskmanager1 -h mytaskmanager1 --network flink-network  \
 -v /root/wym/conf:/opt/flink/conf -v /root/wym/data/save_point:/opt/flink/save_point -v /root/wym/data/check_point:/opt/flink/check_point -v /root/wym/data/blob_store:/opt/flink/blob_store -v /root/wym/data/upload_dir:/opt/flink/upload_dir -v /root/wym/data/log:/opt/flink/log --env FLINK_PROPERTIES=&quot;jobmanager.rpc.address: myjobmanager&quot; --env LOCAL_USER_ID=`id -u $USER` flink1:latest taskmanager
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Handleing  Permissions with Docker Volume]]></title>
        <id>https://wangyemao-github.github.io/post/handleing-permissions-with-docker-volume/</id>
        <link href="https://wangyemao-github.github.io/post/handleing-permissions-with-docker-volume/">
        </link>
        <updated>2021-04-01T09:37:17.000Z</updated>
        <content type="html"><![CDATA[<h1 id="docker-volume">Docker Volume</h1>
<p>A data volume is a specially-designated directory within one or more containers that bypasses the Union File System.</p>
<h2 id="the-main-use-case-for-volumes">The main use-case for Volumes</h2>
<p>is for persisting data between container runs。</p>
<ul>
<li>在容器中运行数据库（比如Postgresql）时，对于数据目录很有用；</li>
<li>在开发环境运行时，有助于共享主机系统的代码目录给容器</li>
</ul>
<h2 id="使用volume面临的两个问题">使用volume面临的两个问题</h2>
<ul>
<li>对于指定的Volume，你不能访问容器写入的文件。因为容器中的进程通常以root用户运行</li>
<li>即便容器中以一个普通用户运行，它仍旧不能与宿主机上的用户相匹配。</li>
</ul>
<p>因为通常</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[DockerFile]]></title>
        <id>https://wangyemao-github.github.io/post/dockerfile/</id>
        <link href="https://wangyemao-github.github.io/post/dockerfile/">
        </link>
        <updated>2021-03-29T14:21:07.000Z</updated>
        <content type="html"><![CDATA[<p>DockerFile是一个<strong>文本格式的配置文件</strong>，用户可以<strong>使用Dockerfile来快速创建自定义的镜像</strong>。</p>
<h1 id="基本结构">基本结构</h1>
<p>一般而言Dockerfile主体内容分为四部分：基础镜像信息、维护者信息、镜像操作指令和容器启动时执行指令。</p>
<h2 id="基础镜像信息">基础镜像信息</h2>
<p><strong>FROM 指令</strong>：指明基础镜像。<strong>Dockerfile中的第一条指令必须为FROM指令</strong>。在同一个Dockerfile中创建多个镜像时，可以使用多个FROM指令（每个镜像一个）<br>
为了保证镜像精简，可以选用体积较小的镜像作为基础镜像。<br>
格式：FROM <image> [AS name]  或 FROM <image>:<tag> [AS name] 或 FROM <image>@<digest> [AS name]</p>
<h2 id="镜像操作">镜像操作</h2>
<h3 id="镜像配置指令">镜像配置指令</h3>
<ul>
<li>
<p>EXPOSE指令：<strong>声明镜像内服务监听的端口</strong>。该指令只起到声明作用，并不会自动完成端口映射。如果需要映射端口出来，在启动容器时可以使用-P参数（Docker主机会自动分配一个宿主机的临时端口）或-p HOST_PORT：CONTAINER_PORT参数（具体制定所映射的本地端口）<br>
格式：EXPOSE <port> [<port>/<protocl>....]</p>
</li>
<li>
<p>ENTRYPOINT指令：指定镜像的默认入口命令，<strong>该入口命令会在启动容器时作为根命令执行，所有传入值作为该命令的参数</strong>。在指定了 ENTRYPOINT指令的情况下，CMD指令指定值将作为根命令的参数。每个Dockerfile中只能有一个ENTRYPOINT，当指定多个时，只有最后一个起效。在运行时，可以被--entrypoint参数覆盖掉<br>
格式：ENTRYPOINT [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]  exec调用执行<br>
ENTRYPOINT command param1 param2 ：shell中执行</p>
</li>
<li>
<p>ARG指令：<strong>定义创建镜像过程中使用的变量</strong>。在执行docker build时，可以通过-build-arg[=]来为变量赋值。当镜像编译成功后，ARG指定的变量将不再存在【<strong>ENV指定的变量将在镜像中保留</strong>】<br>
格式：ARG <name> [=<default value>]</p>
</li>
<li>
<p>ENV 指令：<strong>指定环境变量</strong>，在镜像生成过程中会被后续RUN指令使用，在镜像启动的容器中也会存在。指定的环境变量在运行时可以被覆盖掉，如：docker run --env <key>=<value></p>
</li>
<li>
<p>VOLUME指令：创建一个数据卷挂载点。运行容器时可以从本地主机或其他容器挂载数据卷，一般用来存放数据库和需要保持的数据等。<br>
格式：VOLUME [&quot;/data&quot;]</p>
</li>
<li>
<p>WORKDIR指令：为后续的RUN、CMD、ENTRYPOINT指令配置工作目录。可以使用多个WORKDIR指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。<br>
格式：WORKDIR /path/to/wordir<br>
WORKDIR /a<br>
WORKDIR b<br>
WORKDIR c<br>
RUN pwd   最终路径为/a/b/c</p>
</li>
</ul>
<h3 id="镜像操作指令">镜像操作指令</h3>
<ul>
<li>
<p>RUN指令：运行指定命令。每条RUN指令将在当前镜像基础上执行指定命令，并提交为新的镜像层。</p>
<ul>
<li>RUN <command>    : 默认将在shell终端中运行命令，即/bin/bash -c ;</li>
<li>RUN [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]  :使用exec执行，不会启动shell环境。指令会被解析为JSON，因此必须使用双引号。<br>
指定使用其他终端类型可以通过第二种方式实现：RUN [&quot;/bin/bash&quot;,&quot;-c&quot;,&quot;echo hello&quot;]</li>
</ul>
</li>
<li>
<p>CMD 指令：用来指定启动容器时默认执行的命令。每个Dockerfile只能有一条CMD命令。如果指定了多条，只有最后一条会被执行。如果用户启动容器时手动指定了运行的命令（作为run命令的参数），则会覆盖掉CMD指定的命令。</p>
<ul>
<li>CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] : 相当于 executable param1 param2 ,推荐方式</li>
<li>CMD command param1 param2 ：默认在shell中执行，提供给需要交互的应用</li>
<li>CMD [&quot;param1&quot;,&quot;param2&quot;] ： 提供给ENTRYPOINT的默认参数</li>
</ul>
</li>
<li>
<p>ADD指令：添加内容到镜像。将复制指定的<src>路径下的内容到容器的<dest>路径下。其中<src>可以是Dockerfile所在目录的一个相对路径（文件或目录）；也可以是一个URL；还可以是一个tar文件（自动解压为目录）；<dest>可以是镜像内的绝对路径，或相对于工作目录（WORKDIR）的相对路径。路径支持正则格式。</p>
<ul>
<li>ADD <src> <dest></li>
</ul>
</li>
<li>
<p>COPY指令：复制内容到镜像。复制从主机的src（为Dockerfile所在目录的相对路径）下内容到镜像中的dest。目标路径不存在时，会自动创建。</p>
<ul>
<li>COPY <src> <dest></li>
</ul>
</li>
</ul>
<h1 id="docker-flink-安装-部署">docker flink 安装、部署</h1>
<ul>
<li></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flink 内存设置——个人理解]]></title>
        <id>https://wangyemao-github.github.io/post/flink-nei-cun-she-zhi-ge-ren-li-jie/</id>
        <link href="https://wangyemao-github.github.io/post/flink-nei-cun-she-zhi-ge-ren-li-jie/">
        </link>
        <updated>2021-02-02T11:27:47.000Z</updated>
        <content type="html"><![CDATA[<p>本文主要讲解关于TaskManager 1.10版本和JobManager 1.11版本之后，Flink内存相关设置。</p>
<h1 id="flink-memory-组成">Flink memory 组成</h1>
<p>Flink JVM进程的总内存包括<em>Flink应用程序内存（total Flink memory）<em>和</em>JVM运行该进程所消耗的内存</em>组成。total Flink memory 又包括<em>JVM Heap memory</em> 和<em>Off-Heap(direct and native)memory</em>。<br>
<img src="https://wangyemao-github.github.io/post-images/1612265850568.png" alt="" loading="lazy"></p>
<p>JobManager的内存模型同上，TaskManager 的内存模型为：<br>
<img src="https://wangyemao-github.github.io/post-images/1612266194318.png" alt="" loading="lazy"><br>
可见，TaskManager 进程中，total Flink memory 包括JVM Heap memory，Managed memory(由Flink管理的) 和其他Direct(或者native) memory组成。</p>
<h1 id="configure-total-memory">Configure Total Memory</h1>
<ol>
<li>
<p>在Flink中设置内存最简单的方式就是配置如下两个组合选项之一：<br>
<img src="https://wangyemao-github.github.io/post-images/1612266389363.png" alt="" loading="lazy"><br>
<strong>注意</strong>：不推荐同时配置<em>total process memory * 和</em>total Flink memory*，可能因为潜在的配置冲突，造成部署失败。</p>
</li>
<li>
<p>设置内存的另一种方式就是配置<em>total flink memory 的必须组成部分的内存</em><br>
<img src="https://wangyemao-github.github.io/post-images/1612267283859.png" alt="" loading="lazy"></p>
</li>
</ol>
<ul>
<li>对于TaskManager，指定<em>task heap</em>和<em>managed memory</em>，它可以更好地控制Flink任务的可用JVM堆内存以及Managed memory。其余的内存组成部分将根据默认值或其他配置项进行自动调整。<br>
** Task Heap Memory** ： 如果你想要确保特定大小的JVM Heap 内存用于用户代码任务执行，你可以显示指定 <em>task heap memory</em></li>
</ul>
<pre><code class="language-text">taskmanager.memory.task.heap.size 
</code></pre>
<p><strong>Managed Memory</strong> : 由Flink管理的，被分类为off-heap memory。Managed memory主要用于：(1) 流作业用于RocksDB state backend；（2）批作业用于排序，hash table，缓存中间结果等；（3）在流/批作业中用于user defined function in python processes。设置方式为：</p>
<pre><code class="language-text">taskmanager.memory.managed.size    
or
taskmanager.memory.managed.fraction
</code></pre>
<p><strong>注意</strong>：  taskmanager.memory.managed.fraction 设置的占比是基于<em>total Flink Memory</em> 的占比。Size和fraction都配置的情况下，Size将覆盖fraction；如果都没设置，将使用默认的fraction计算。</p>
<ul>
<li>对于JobManager，指定*JVM Heap *大小，它提供了对可用JVM堆的更多控制。<br>
JVM Heap内存用于：（1）Flink framework；（2）job提交期间执行的用户代码(比如，特定的批数据源)或者checkpoint完成时的回调。所需的JVM堆大小主要由正在运行的作业的数量、它们的结构和对上述用户代码的需求驱动。</li>
</ul>
<ol start="3">
<li>推荐：已经在按照<strong>2</strong>进行配置的情况下，建议不要做<strong>1</strong>相关的配置。<br>
具体来说，对TaskManager，如果已经配置了task heap memory 和managed memory的情况下，推荐不要在设置total process memory 和 total Flink memory。否则，很可能导致内存配置冲突问题。对JobManager，如果配置了JVM heap，也推荐不要在设置total process memory 和total Flink memory，否则也会导致同样的问题。</li>
<li>Configure off-heap memory  ： off-heap memory 的相关配置，具体可参见官网。</li>
</ol>
<h1 id="local-execution">Local Execution</h1>
<p>如果您在本地（例如从IDE中）运行Flink而没有创建集群，针对不同的进程有效的内存设置分别为：</p>
<ul>
<li>TaskManager ：除了下表中的设置项，其他的内存设置都会忽略<br>
<img src="https://wangyemao-github.github.io/post-images/1612315668235.png" alt="" loading="lazy"><br>
对于本地执行，当前Task Heap size 与 实际Heap size 没有任何关系。在下一个版本，它是未来优化的重要内容。实际的JVM Heap Size不是由Flink控制，而是依赖于如何启动Flink 进程。如果你想要控制JVM Heap 大小，你需要显示传递相应的JVM参数：比如，-Xmx，-Xms 。</li>
<li>JobManager ： JobManager的内存配置项都会被忽略</li>
</ul>
<h1 id="memory-tuning">Memory tuning</h1>
<p>本节主要说明：针对不同的使用场景如何设置内存，以及在每种场景下的重要设置项。</p>
<h2 id="configure-memory-for-standalone-deployment">Configure memory for standalone deployment</h2>
<p>对于standalone 部署模式，推荐配置*total Flink memory （taskmanager.memory.flink.size 或jobmanager.memory.flink.size）*或者其组成部分。另外，如果JVM MetaSpace存在问题，则可以对其进行调整。<br>
*the total Process memory *是无关紧要的，因为JVM 开销并不由Flink或者部署环境控制，在这种情况下，只有执行机器的物理资源是重要的。</p>
<h2 id="configure-memory-for-state-backends">Configure memory for state backends</h2>
<p>该配置只与TaskManager有关。部署Flink 流应用程序时，使用的状态后端（state backend）类型将决定集群的最佳内存配置。</p>
<h3 id="heap-state-backend">Heap state backend</h3>
<p><em>当运行无状态作业或者使用heap state backend(MemoryStateBackend 或者 FsStateBackend)时，将Managed Memory设置为0</em>. 这可以确保为JVM上的用户代码分配最大数量的堆内存</p>
<h3 id="rocksdb-state-backend">RocksDB state backend</h3>
<p>RocksDBStateBackend 使用native memory。默认地，RocksDB设置将native memory限制为 managed memory的大小。因此，预留足够的managed memory给你的状态（state）是很重要的。如果禁用了RocksDB的内存控制，对于容器部署模式，当RocksDB分配的内存超过了请求的容器内存大小（the total process memory）时，TaskManager会被kill掉。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Set up Flink Memory——翻译]]></title>
        <id>https://wangyemao-github.github.io/post/set-up-flink-de-process-memory/</id>
        <link href="https://wangyemao-github.github.io/post/set-up-flink-de-process-memory/">
        </link>
        <updated>2021-02-01T02:40:19.000Z</updated>
        <content type="html"><![CDATA[<h1 id="set-up-flinks-process-memory">Set up Flink's Process Memory</h1>
<p>从TaskManager 1.10版本和JobManager 1.11版本开始，进一步的描述内存配置成为可能。如果从早期版本升级Flink，请查看迁移指南，因此1.10版本和1.11版本引入了很多变更。</p>
<h2 id="configure-total-memory配置总内存">Configure Total Memory（配置总内存）</h2>
<p>Flink JVM进程的总进程内存由<strong>Flink应用程序（总Flink内存）<strong>和</strong>JVM运行该进程所消耗的内存</strong>组成。 Flink的总内存消耗包括<strong>JVM堆</strong>和**非堆内存（Direct and Native）**的使用</p>
<figure data-type="image" tabindex="1"><img src="https://wangyemao-github.github.io/post-images/1612157871063.png" alt="" loading="lazy"></figure>
<ul>
<li>
<p>在Flink中设置内存的最简单方法是配置以下两个选项之一：<br>
<img src="https://wangyemao-github.github.io/post-images/1612158099424.png" alt="" loading="lazy"><br>
注意：对于本地执行，请参阅TaskManager和JobManager进程的详细信息<br>
其余的内存组件将根据默认值或其他配置的选项自动调整。 另请参阅如何为TaskManager和JobManager内存设置其他组件。<br>
配置总Flink内存更适合于独立部署，在该部署中您需要声明为Flink本身分配了多少内存。 Flink的总内存分为JVM Heap和Off-heap内存。 另请参阅如何为独立部署配置内存。<br>
如果配置了总进程内存，则声明应分配给Flink JVM进程总共多少内存。 对于容器化的部署，它与请求的容器的大小相对应，另请参阅如何为容器（Kubernetes，Yarn或Mesos）配置内存。</p>
</li>
<li>
<p>设置内存的另一种方法是配置<strong>total flink memory</strong>(总Flink内存)必须的内部组成，这些内部组成与特定的Flink进程有关。 检查如何为TaskManager和JobManager配置它们。<br>
<strong>注意</strong>：必须使用上述三种方式之一来配置Flink的内存（本地执行除外），否则Flink启动将会失败。 这意味着必须显式配置以下没有默认值的选项子集之一：<br>
<img src="https://wangyemao-github.github.io/post-images/1612158530384.png" alt="" loading="lazy"></p>
</li>
</ul>
<p><strong>注意</strong>：不建议同时配置总进程内存和总Flink内存。 由于潜在的内存配置冲突，可能会导致部署失败。 配置其他内存组件也需要谨慎，因为它可能会导致进一步的配置冲突。</p>
<h2 id="jvm-parameters">JVM Parameters</h2>
<p>Flink根据配置的或派生的内存组件大小，在启动其进程时显式添加以下与内存相关的JVM参数：<br>
<img src="https://wangyemao-github.github.io/post-images/1612158811921.png" alt="" loading="lazy"></p>
<p>（<em>）请记住，根据所使用的GC算法，您可能无法使用全部堆内存。一些GC算法会为自身分配一定数量的堆内存。这将导致堆度量标准返回不同的最大值。<br>
（<strong>）注意，用户代码中的 native non-direct内存使用也可以解释为堆外内存的一部分。<br>
（</strong></em>）仅当设置了相应的选项：jobmanager.memory.enable-jvm-direct-memory-limit ，才会为JobManager 进程添加JVM Direct内存限制。</p>
<h2 id="capped-fractionated-components">Capped Fractionated Components</h2>
<ul>
<li><strong>JVM Overhead</strong>(JVM开销)仅占total process memory （总进程内存）的一小部分</li>
<li><strong>Network memory</strong>（网络内存）可以是Flink memory的一小部分（仅适用于TaskManager）</li>
</ul>
<ol>
<li>这些组件的大小必须始终在其最大值和最小值之间，否则Flink启动将失败。 最大值和最小值具有默认值，或者可以通过相应的配置选项明确设置。 例如，如果您仅设置以下内存选项：<br>
<img src="https://wangyemao-github.github.io/post-images/1612159667353.png" alt="" loading="lazy"></li>
</ol>
<p>那么，JVM开销将为：1000Mb * 0.1 = 100Mb，在64Mb-128Mb范围。<br>
<strong>注意</strong>：如果您配置相同的最大值和最小值，则可以有效地将大小固定为该值。</p>
<ol start="2">
<li>
<p>如果你没有显示设置组成部分的内存，那么Flink将基于total memory使用fraction计算内存大小。计算的结果由其相应的最大/最小值限制。比如，仅设置了如下内存选项：<br>
<img src="https://wangyemao-github.github.io/post-images/1612160004173.png" alt="" loading="lazy"><br>
那么，JVM的开销将为：128Mb，因为根据fraction计算的内存大小为100Mb，小于了设置的最小值128Mb。</p>
</li>
<li>
<p>如果定义了总内存及其其他组件的大小，也可能会忽略该fraction. 在这种情况下，JVM开销是总内存的剩余部分。 派生值仍必须在其最小/最大范围内，否则配置将失败。 例如，假设仅设置了以下内存选项：<br>
<img src="https://wangyemao-github.github.io/post-images/1612160242506.png" alt="" loading="lazy"><br>
total process memory(进程总内存)中的所有其他组件都有默认值，包括默认的Managed Memory 部分（或JobManager中的堆外内存）。 这种情况下，JVM开销不是基于fraction（1000Mb x 0.1 = 100Mb），而是总的进程内存的其余部分，该部分必须在64-256Mb范围内，否则将失败</p>
</li>
</ol>
<h1 id="set-up-taskmanager-memory">Set up TaskManager Memory</h1>
<p>TaskManager在Flink中运行用户代码。 根据需要配置内存使用量可以大大减少Flink的资源占用并提高作业稳定性。进一步的内存配置描述从发行版1.10开始可用。<br>
<strong>注意</strong>：本内存设置指南仅与TaskManager有关！ 与JobManager进程的内存模型相比，TaskManager内存组件具有相似但更复杂的结构。</p>
<h2 id="configure-total-memory">Configure Total Memory</h2>
<p>Flink JVM 进程的<em>total process memory</em>包含<strong>Flink 应用（total Flink memory）<strong>消耗内存和</strong>JVM运行Flink应用所消耗内存</strong>。<em>total Flink memory</em>包括JVM Heap，managed memory（由Flink管理的）和其他direct（或者native）memory。<br>
<img src="https://wangyemao-github.github.io/post-images/1612161359813.png" alt="" loading="lazy"><br>
如果本地运行Flink（比如IDE）而没有创建集群，那么仅有一部分内存设置选项是相关的，更多详细信息，请参见本地执行。<em>否则，为TaskManager设置内存的最简单方法是配置总内存</em>，其余的内存组件将根据默认值或其他配置的选项自动调整。<br>
<img src="https://wangyemao-github.github.io/post-images/1612161593452.png" alt="" loading="lazy"></p>
<h2 id="configure-heap-and-managed-memory">Configure Heap and Managed Memory</h2>
<p><em>在Flink中设置内存的另一种方法是显式地指定task heap和managed memory</em>，<strong>它可以更好地控制Flink任务的可用JVM堆及托管内存</strong>。其余的内存组成部分将根据默认值或其他配置的选项自动调整。<br>
<strong>注意</strong>：如果已经配置了task heap 和 managed memory，推荐不要在设置total process momory 和 total Flink memory。否则，可能更容易导致内存配置冲突问题。</p>
<ul>
<li>Task （Operator）Heap Memory<br>
如果你想保证特定量的JVM Heap可用于你的用户代码，则可以显式设置<em>task heap memory</em>（taskmanager.memory.task.heap.size）。它将被添加到JVM堆大小中，并专门用于运行用户代码的Flink operator</li>
<li>Managed Memory<br>
Managed Memory由Flink管理，被分配为native memory（off-heap）。如下工作负载使用托管的内存：<br>
（1）流式作业用于RocksDB state backend<br>
（2）批作业用于排序，hash table，缓存中间结果<br>
（3）流作业和批作业用它执行 user defined function in python processes<br>
managed memory大小可以：</li>
<li>或者显示通过taskmanager.memory.managed.size 配置</li>
<li>或者通过taskmanager.memory.managed.fraction 基于<em>total Flink Memory</em> 的占比计算。<br>
在两者都设置的情况下，配置的具体Size会覆盖占比。如果Size和fraction都没有指定，将使用默认的fraction。</li>
</ul>
<h2 id="configure-off-heap-memorydirect-or-native">Configure off-heap Memory（direct or native）</h2>
<p>用户代码分配的off-heap内存应在<em>任务off-heap内存</em>(taskmanager.memory.task.off-heap.size)中考虑.<br>
<strong>注意</strong>：您还可以调整框架的堆外内存。 仅在确定Flink框架需要更多内存的情况下，才应更改此值<br>
Flink将*framework off-heap memory(框架的堆外内存)<em>和</em>task off-heap memory(任务的堆外内存)*纳入JVM的直接内存限制，另请参见JVM参数<br>
<strong>注意</strong>：尽管native non-direct memory（本机非直接内存）使用量可以解释为框架堆外内存或任务堆外内存的一部分，但在这种情况下，它将导致更高的JVM直接内存限制<br>
<strong>注意</strong> ： network memory 也是JVM direct memory 的一部分，但是它是有Flink管理的，并保证永远不会超过其配置的大小。因此，在这种情况下，调整网络内存大小将没有任何帮助。</p>
<h2 id="detail-memory-model">Detail Memory Model</h2>
<p><img src="https://wangyemao-github.github.io/post-images/1612164790742.png" alt="" loading="lazy"><br>
下表列出了上面描述的所有内存组成部分，并引用了影响各个组件大小的Flink配置选项：<br>
<img src="https://wangyemao-github.github.io/post-images/1612165042476.png" alt="" loading="lazy"></p>
<h2 id="framework-memory">Framework Memory</h2>
<p>如果没有充分的理由，不应更改<em>framework heap memory（框架堆内存）</em> 和 <em>framework off-heap memory（框架堆外内存）</em>。 仅在确定Flink需要更多内存用于某些内部数据结构或操作时才进行调整。 它可能与特定的部署环境或工作结构有关，例如高并行度。 此外，在某些设置中，诸如Hadoop之类的Flink依赖项可能会消耗更多的直接或本地内存（direct or native memory ）。<br>
<strong>注意</strong>：目前，Flink既不隔离框架和任务内存的堆版本，也不隔离堆版本。 框架和任务内存的分离可以在将来的版本中使用，以进行进一步的优化。</p>
<h2 id="local-execution">Local Execution</h2>
<p>如果您在计算机上作为单个Java程序在本地启动Flink而不创建集群（例如从IDE中创建），则除以下内容外，所有其他内存组成部分都将被忽略：<br>
<img src="https://wangyemao-github.github.io/post-images/1612165649391.png" alt="" loading="lazy"><br>
<strong>注意</strong>：在这种情况下，任务堆大小与实际堆大小没有任何关系。 它可能与后续版本的未来优化相关。 启动的本地进程的实际JVM Heap大小不受Flink的控制，并且取决于您如何启动该进程。 如果要控制JVM堆大小，则必须显式传递相应的JVM参数，例如 -Xmx，-Xms</p>
<h1 id="set-up-jobmanager-memory">Set up JobManager Memory</h1>
<p>JobManager是Flink cluster的控制元素。 它由三个不同的组件组成：Resource Manager(资源管理器)，Dispatcher(分派器)和每个运行的Flink作业一个JobMaster。 本指南将引导您完成JobManager的高级和细粒度内存配置。以下内存配置描述在1.11版本之后可用。<br>
<strong>注意</strong>：本内存设置指南仅与JobManager有关！与TaskManager的内存配置相比，JobManager的内存组件具有相似但更简单的结构</p>
<h2 id="configure-total-memory-2">Configure Total Memory</h2>
<p>设置内存配置的最简单方法是配置该进程的<em>total memory</em>(总内存)。 如果使用本地执行模式运行JobManager进程，则无需配置内存选项，它们将无效。<br>
<img src="https://wangyemao-github.github.io/post-images/1612166962045.png" alt="" loading="lazy"></p>
<h2 id="detailed-configuration">Detailed configuration</h2>
<p><img src="https://wangyemao-github.github.io/post-images/1612166775467.png" alt="" loading="lazy"><br>
下表列出了上面描述的所有内存组件，并引用了影响各个组件大小的Flink配置选项：<br>
<img src="https://wangyemao-github.github.io/post-images/1612166871341.png" alt="" loading="lazy"></p>
<h3 id="configure-jvm-heap">Configure JVM Heap</h3>
<p><em>为JobManager设置内存的另一种方法是显式指定JVM堆大小（jobmanager.memory.heap.size）</em>。 它提供了对可用的JVM堆的更多控制，JVM堆内存用于：</p>
<ul>
<li>Flink framework</li>
<li>job提交期间（比如，对于特定的批处理源）或者checkpoint完成回调中指定的用户代码。</li>
</ul>
<p>所需的JVM Heap大小主要取决于正在运行的作业的数量，其结构以及所提到的用户代码的要求。<br>
<strong>注意</strong>：如果配置了JVM heap，推荐不要在设置total process memory 和total Flink memory，否则，很可能导致内存配置冲突。<br>
Flink 脚本和CLI在启动JobManager进程时，通过<strong>JVM参数</strong>-Xms 和 -Xmx设置JVM heap 大小。</p>
<h3 id="configure-off-heap-memory">Configure Off-heap Memory</h3>
<p>堆外内存组件考虑了任何类型的JVM直接内存和本机内存使用情况。 因此，您还可以通过设置jobmanager.memory.enable-jvm-direct-memory-limit选项来启用JVM直接内存限制。 如果配置了此选项，则Flink将通过相应的JVM参数-XX：MaxDirectMemorySize将限制设置为堆外内存大小。<br>
可以通过jobmanager.memory.off-heap.size选项配置此组件的大小。 可以调整此选项，例如 如果JobManager进程抛出“ OutOfMemoryError：直接缓冲内存”<br>
堆外内存消耗可能有以下几种来源：</p>
<ul>
<li>Flink framework dependencies (e.g. Akka network communication)</li>
<li>User code executed during job submission (e.g. for certain batch sources) or in checkpoint completion callbacks</li>
</ul>
<p><strong>注意</strong>：如果您已显式配置了总Flink内存和JVM堆，但尚未配置Off-heap内存，则将使用Total Flink Memory减去JVM Heap得出Off-heap内存的大小。 堆外内存选项的默认值将被忽略。</p>
<h3 id="local-execution-2">Local Execution</h3>
<p>如果您在本地（例如从IDE中）运行Flink而没有创建集群，则JobManager内存配置选项将被忽略。</p>
<h1 id="内存调优指南">内存调优指南</h1>
<p>本节说明根据不同的使用场景如何设置内存，以及在每种情况下的重要设置项：</p>
<h2 id="configure-memory-for-standalone-deployment">Configure memory for standalone deployment</h2>
<p>对于standalone 部署模式，在要声明给Flink分配多少内存的地方，推荐配置<em>total Flink memory</em> （taskmanager.memory.flink.size 或jobmanager.memory.flink.size）或者<em>其组成部分</em>。此外，如果JVM metaSpace引起问题，则可以对其进行调整。</p>
<p><em>the total Process memory</em> 是无关紧要的，因为JVM 开销并不由Flink或者部署环境控制，在这种情况下，只有执行机器的物理资源是重要的。</p>
<h2 id="configure-memory-for-containers">Configure memory for containers</h2>
<p>对于容器部署模式（kubernetes，yarn或Mesos），推荐配置<em>total procss momery</em>（taskmanager.memory.process.size 或  jobmanager.memory.process.size）。它声明应总共为Flink JVM进程分配多少内存，并与请求的容器的大小相对应。</p>
<p><strong>注意</strong>：<br>
** 警告**：如果Flink或用户代码分配了超出容器大小的<em>unmanaged off-heap (native) memory</em>（非托管堆外（本机）内存），则作业可能会失败，因为部署环境会杀死有问题的容器</p>
<h2 id="configure-memory-for-state-backends">Configure memory for state backends</h2>
<p>这只与TaskManager有关。部署Flink 流应用程序时，使用的状态后端（state backend）类型将决定集群的最佳内存配置。</p>
<h3 id="heap-state-backend">Heap state backend</h3>
<p>当运行无状态作业或者使用heap state backend(MemoryStateBackend 或者 FsStateBackend)时，将Managed Memory设置为0. 这可以确保为JVM上的用户代码分配最大数量的堆内存。</p>
<h3 id="rocksdb-state-backend">RocksDB state backend</h3>
<p>RocksDBStateBackend 使用native memory。默认地，RocksDB设置将native memory限制为 managed memory的大小。因此，预留足够的managed memory给你的状态（state）是很重要的。如果禁用了RocksDB的内存控制，对于容器部署模式，当RocksDB分配的内存超过了请求的容器内存大小（the total process memory）时，TaskManager会被kill掉。</p>
<h1 id="troubleshooting">Troubleshooting</h1>
<h2 id="illegalconfigurationexception">IllegalConfigurationException</h2>
<p>如果你看到<em>从TaskExecutorProcessUtils 或者 JobManagerProcessUtils抛出IIIegalConfigurationException异常</em>，通常表明：存在无效的配置值（比如，内存大小设置为负值，占比(fraction)设置大于1等）或者配置冲突。</p>
<h2 id="outofmemoryerrorjava-heap-space">OutofMemoryError：Java heap space</h2>
<p>该异常通常标识*JVM Heap *设置的太小。你可以尝试增大total memory的大小来增加JVM heap，你也可以直接增大TaskManager的task heap memory 或JobManager的JVM Heap memory。</p>
<p><strong>注意</strong>：你也可以增大TaskManager的framework heap memory，但是当前只当你确定Flink framework 自身需要更多的内存时，你才考虑变更该选项值</p>
<h2 id="outofmemoryerror-direct-buffer-memory">OutOfMemoryError: Direct buffer memory</h2>
<p>该异常通常表明：JVM direct memory限制太小或者存在direct memory leak。检查用户代码或者外部依赖是否使用了JVM direct memory，以及是否很好地考虑了这部分内存的使用。你可以尝试通过调整direct off-heap memory来增大限制。-----可以参考TaskManager，JobManager以及JVM参数如何配置off-heap memory。</p>
<h2 id="outofmemoryerror-metaspace">OutOfMemoryError: Metaspace</h2>
<p>该异常通常表明JVM metaspace limit的配置太小。你可以尝试加大TaskManager或者JobManager的JVM metaspace配置项。</p>
<h2 id="ioexception-insufficient-number-of-network-buffers">IOException: Insufficient number of network buffers</h2>
<p>该异常只与TaskManager相关。该异常通常表明: 配置的network memory 不够大。你可以通过调整如下选项尝试加大network memory：</p>
<ul>
<li>taskmanager.memory.network.min</li>
<li>taskmanager.memory.network.max</li>
<li>taskmanager.memory.network.fraction</li>
</ul>
<h2 id="container-memory-exceeded">Container Memory Exceeded</h2>
<h1 id="迁移指南migration-guide内存配置相关">迁移指南（Migration Guide）——内存配置相关</h1>
<p>对于TaskManagers的1.10版本和JobManagers的1.11版本，内存设置发生了很大变化。 许多配置选项已删除或它们的语义已更改。 本指南将帮助您将TaskManager内存配置从Flink &lt;= 1.9迁移到&gt; = 1.10，并将JobManager内存配置从Flink &lt;= 1.10迁移到&gt; = 1.11。</p>
<p><strong>警告</strong>：阅读本指南很重要，因为旧的和新的内存配置可能会导致内存组件的大小不同。 如果您尝试从适用于TaskManager的1.10之前或适用于JobManager的1.11之前的旧版本重用Flink配置，则可能导致应用程序的行为，性能甚至配置失败。</p>
<p><strong>注意</strong>：对于1.10版之前的TaskManager，和1.11版本之前JobManager，Flink根本不需要设置与内存相关的选项，因为它们都有默认值。 新的内存配置要求显式配置以下选项的至少一个子集，否则配置将失败。<br>
<img src="https://wangyemao-github.github.io/post-images/1612149087148.png" alt="" loading="lazy"></p>
<p>默认flink-conf.yaml中附带了Flink设置taskmanager.memory.process.size（从1.10开始）和jobmanager.memory.process.size（从1.11开始）以使默认内存配置一致。</p>
<p>该表格还可以帮助评估新旧版本内存计算的结果。</p>
<h1 id="migrate-task-manager-memory-configuration">Migrate Task Manager Memory Configuration</h1>
<h2 id="配置项变更">配置项变更</h2>
<p>本章简要列出了1.10版本引入的Flink内存配置选项的所有更改。也参考了其他章节，以获得迁移到新配置选项的更多详细信息。<br>
以下选项已被完全删除。 如果仍然使用将被忽略。</p>
<figure data-type="image" tabindex="2"><img src="https://wangyemao-github.github.io/post-images/1612149586700.png" alt="" loading="lazy"></figure>
<p>不建议使用以下选项，但如果仍使用它们，则将它们解释为向后兼容的新选项</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Configuratio—翻译]]></title>
        <id>https://wangyemao-github.github.io/post/configuratio-fan-yi/</id>
        <link href="https://wangyemao-github.github.io/post/configuratio-fan-yi/">
        </link>
        <updated>2021-01-31T11:57:10.000Z</updated>
        <content type="html"><![CDATA[<p>所有的配置都在conf/flink-conf.yaml中。配置子Flink程序启动时解析和执行，因此，配置文件的变更需要重启相关进程（jobmanager/taskmanager）<br>
开箱即用的配置将使用默认的Java安装。你可以手动设置环境变量：JAVA_HOME或者如果你想要手动覆盖使用的java运行时，可以在conf/flink-conf.yaml文件中配置：env.java.home键值。</p>
<h1 id="basic-setup">Basic Setup</h1>
<p>默认的配置支持单节点的Flink session cluster，不需要任何变更。本节的设置项对于基本的Flink分布式设置是最常需要的。</p>
<h2 id="hostnamesports">Hostnames/Ports</h2>
<p>这些选项只对standalone application/ session （简单的standalone 或kubernetes）部署模式是需要的<br>
如果你使用Flink集成Yarn，Mesos或者active kubernetes ，hostnames和ports是自动发现的。</p>
<ul>
<li>rest.address，rest.port ：用于client连接到Flink。<br>
将其设置为运行的JobManager的主机名，或者设置为Jobmanager REST 接口前面的（kubernetes）服务的主机名。</li>
<li>jobmanager.rpc.address（默认 “localhost”）和jobmanager.rpc.port (默认6123）.用于Taskmanager连接到jobmanager/ResourceManager.<br>
将其设置为Jobmanager运行的主机名，或者Jobmanager的（kubernetes 内部）服务的主机名。该选项在设置为高可用时是被忽略的，这种情况下，jobmanager/resourceManager使用leader选举机制自动发现</li>
</ul>
<h2 id="memory-size">Memory Size</h2>
<p>默认的内存大小设置支持简单的流式/批应用，但是对于更多复杂的应用，这个设置会太小而不能产生很好的性能</p>
<ul>
<li>jobmanager.memory.process.size : JobManager进程的总内存大小（JobMaster/ResourceManager/Dispatcher）</li>
<li>taskmanager.memory.process.size: TaskManager 进程的总内存大小</li>
</ul>
<p>总大小包括了所有。Flink将为JVM自身的内存需求减去一些内存（metaspace以及其他的），并在组成部分（JVM 堆，堆外，对于TaskManager还有网络，托管内存等）中自动划分和分配剩下的内存。<br>
这些选项配置为内存大小，例如：1536m 或者2g。</p>
<h2 id="parallelism">Parallelism</h2>
<ul>
<li>taskmanager.numberOfTaskSlots： 一个TaskManager提供的slots数目(默认为1)。每一个slot可以执行一个task或者一个pipeline。在一个TaskManager中设置多个slots，有利于在并行的任务或pipelines之间分摊某些恒定的开销（JVM、应用程序库，网络连接）。**see Task Slots and Resources  **<br>
运行更小的TaskManagers，比如每个TaskManager设置1个Slot是一个很好地起始点，可以实现最好的任务隔离。将相同的资源分配给数量更少、slots数更多的大型TaskManagers有助于提高资源利用率，但代价是削弱了任务之间的隔离性（更多的任务共享相同的JVM）。</li>
<li>parallelism.default ： 当没有指定具体制定并行度时，默认的使用的并行度。（默认为1）</li>
</ul>
<h2 id="checkpointing">Checkpointing</h2>
<p>你可以在Flink作业或应用代码中直接配置checkpointing。在应用没有做任何配置的情况下，配置文件中配置这些值会作为默认设置。</p>
<ul>
<li>state.backend : 要使用的state backend。该选项定义了快照的数据结构机制，常用的取值为filesystem和rockdb。</li>
<li>state.checkpoints.dir : 要写入快照的目录。需要设置一个路径URI，比如：s3://mybucket/flink-app/checkpoints 或 hdfs://namenode:port/flink/checkpoints</li>
<li>state.savepoints.dir: savepoint的默认目录。需要像state.checkpoints.dir一样，设置一个路径URI。</li>
</ul>
<h2 id="web-ui">Web UI</h2>
<ul>
<li>web.submit.enable : 开启通过Flink UI上传和启动jobs(默认为true)。请注意，即便该选项设置为false，session cluster仍然可以通过REST请求(HTTP调用)接受jobs。此标识只保护在UI上上传job的功能。</li>
<li>web.upload.dir : 存储上传jobs的目录。只有在web.submit.enable开启的情况下使用</li>
</ul>
<h2 id="other">Other</h2>
<p>io.tmp.dirs : Flink放置本地数据的目录，默认为系统临时目录（java.io.tmpdir 属性）。如果配置了目录列表，Flink will rotate files across the directories。<br>
默认情况下，放在这些目录中的数据包括RocksDB创建的文件，溢出的中间结果（批处理算法）和缓存的jar文件。<br>
<strong>持久化/恢复不依赖此数据，但是如果删除此数据，通常会导致很重的恢复操作。 因此，建议将其设置为不会自动定期清除的目录</strong>。<br>
默认情况下，Yarn，Mesos和Kubernetes设置会自动将此值配置为本地工作目录。</p>
<h1 id="通用配置项">通用配置项</h1>
<h2 id="hosts-and-ports">Hosts and Ports</h2>
<p>用于为Flink的不同组件配置主机名和端口的选项。<br>
JobManager主机名和端口仅与非高可用的独立模式设置有关。 在该设置中，TaskManager使用该配置值来查找（并连接到）JobManager。 在所有高可用的设置中，TaskManager通过高可用服务（例如ZooKeeper）自动发现JobManager。<br>
使用资源编排框架(K8，Yarn，Mesos)的设置，通常使用框架的服务发现机制。<br>
除非安装程序要求使用特定的端口范围或特定的网络接口进行绑定，否则你无需配置任何TaskManager的主机和端口。</p>
<table>
<thead>
<tr>
<th>key</th>
<th>default</th>
<th>type</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>jobmanager.rpc.address</td>
<td>(none)</td>
<td>String</td>
<td>该配置参数定义了与JobManager进行通信，需要连接的网络地址。仅在存在静态名称或地址的单个JobManager的设置中解释此值（简单的standalone 设置，或具有动态服务名称解析的容器设置）。在很多高可用设置中不会使用该选项，在这种情况下，比如zookeeper，会使用leader选举服务从潜在的多个后背JobManager中选举和发现JobManager leader。</td>
</tr>
<tr>
<td>jobmanager.rpc.port</td>
<td>6123</td>
<td>Integer</td>
<td>该配置参数定义了与JobManager进行通信，需要连接的网络端口。其他同上</td>
</tr>
<tr>
<td>metrics.internal.query-service.port</td>
<td>“0”</td>
<td>String</td>
<td>用于Flink内部指标查询服务的端口范围。推荐设置一个端口范围，避免多个Flink组件运行在同一台机器上时产生端口冲突。可以设置端口集合（“50100”，“50101”），范围（“50100-50200”）或两者的组合。</td>
</tr>
<tr>
<td>rest.address</td>
<td>（none）</td>
<td>String</td>
<td>用于client连接到server的地址。注意：仅在高可用配置为NONE时，才使用此选项</td>
</tr>
<tr>
<td>rest.bind-address</td>
<td>（none）</td>
<td>String</td>
<td>server绑定的地址</td>
</tr>
<tr>
<td>rest.bind-port</td>
<td>“8081”</td>
<td>String</td>
<td>server绑定的端口。推荐设置为一个端口范围，以避免多个Rest servers运行在同一台机器上导致的端口冲突。可以设置端口集合（“50100”，“50101”），范围（“50100-50200”）或两者的组合</td>
</tr>
<tr>
<td>rest.port</td>
<td>8081</td>
<td>Integer</td>
<td>client连接的端口。如果未指定rest.bind-port，则REST server将绑定到该端口</td>
</tr>
<tr>
<td>taskmanager.data.port</td>
<td>0</td>
<td>Integer</td>
<td>task manager的外部端口，用于数据交换</td>
</tr>
<tr>
<td>taskmanager.host</td>
<td>（none）</td>
<td>String</td>
<td>TaskManager所在的网络接口的外部地址。 因为不同的TaskManager对此选项需要不同的值，所以通常在其他非共享TaskManager特定的配置文件中指定它</td>
</tr>
<tr>
<td>taskmanager.rpc.port</td>
<td>“0”</td>
<td>String</td>
<td>TaskManager所在的外部RPC端口。 接受端口列表（“ 50100,50101”），范围（“ 50100-50200”）或两者的组合。 建议设置一定范围的端口，以避免在同一台计算机上运行多个TaskManager时发生冲突</td>
</tr>
</tbody>
</table>
<h2 id="fault-tolerance">Fault Tolerance</h2>
<p>**Restart strategies **decide whether and when the failed/affected tasks can be restarted。<br>
这些配置选项可控制Flink在执行过程中发生故障时的重启行为。 通过在flink-conf.yaml中配置这些选项，可以定义集群的默认重启策略。<br>
仅当没有通过ExecutionConfig配置任何特定于作业的重启策略时，默认重启策略才会生效。</p>
<table>
<thead>
<tr>
<th>key</th>
<th>default</th>
<th>type</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>restart-strategy</td>
<td>（none）</td>
<td>String</td>
<td>定义重启策略，以便在job失败时使用。可设置值为：(1)none,off,disable： 无重启策略；（2）fixeddelay，fixed-delay ： 固定延迟的重启策略；（3）failurerate，failure-rate ： 故障率重启策略；</td>
</tr>
<tr>
<td>如果禁用checkpointing ，默认取值为none。如果开启checkpointing，默认值为fixed-delay，并且重启尝试次数为Integer.MAX_VALUE, 1秒延迟。</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="fixed-delay-restart-strategy">Fixed Delay Restart Strategy</h3>
<table>
<thead>
<tr>
<th>key</th>
<th>default</th>
<th>type</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>restart-strategy.fixed-delay.attempts</td>
<td>1</td>
<td>Integer</td>
<td>如果重新启动策略已设置为固定延迟，则Flink在宣布作业失败之前重试执行的次数</td>
</tr>
<tr>
<td>restart-strategy.fixed-delay.delay</td>
<td>1s</td>
<td>Duration</td>
<td>如果重新启动策略已设置为固定延迟，则两次连续的重新启动尝试之间的延迟。当程序与外部系统进行交互（例如，连接或挂起的事务在尝试重新执行之前应达到超时）时，延迟重试可能会有所帮助。可以使用表示法指定：“ 1分钟”，“ 20秒”</td>
</tr>
</tbody>
</table>
<h3 id="failure-rate-restart-strategy">Failure Rate Restart Strategy</h3>
<table>
<thead>
<tr>
<th>key</th>
<th>default</th>
<th>type</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>restart-strategy.failure-rate.delay</td>
<td>1s</td>
<td>Duration</td>
<td>如果将重新启动策略设置为故障率，则两次连续的重新启动尝试之间的延迟。 可以使用表示法指定：“ 1分钟”，“ 20秒”</td>
</tr>
<tr>
<td>restart-strategy.failure-rate.failure-rate-interval</td>
<td>1min</td>
<td>Duration</td>
<td>如果将重新启动策略设置为失败率，则测量失败率的时间间隔。 可以使用表示法指定：“ 1分钟”，“ 20秒”</td>
</tr>
<tr>
<td>restart-strategy.failure-rate.max-failures-per-interval</td>
<td>1</td>
<td>Integer</td>
<td>如果重新启动策略已设置为失败率，则在使作业失败之前的给定时间间隔内的最大重新启动次数</td>
</tr>
</tbody>
</table>
<h3 id="指定特定job的restart-strategy">指定特定job的Restart Strategy</h3>
<p>处理基于fink-conf.yaml 定义默认重启策略，还可以定义每一个flink job的具体重启策略。</p>
<pre><code class="language-java">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setRestartStrategy(RestartStrategies.fixedDelayRestart(
  3, // number of restart attempts
  Time.of(10, TimeUnit.SECONDS) // delay
)); 
</code></pre>
<h3 id="failover-strategies">Failover Strategies</h3>
<p><strong>Failover strategies</strong> decide which tasks should be restarted to recover the job。Flink支持不同失败策略，可以通过在Flink的配置文件flink-conf.yaml中配置参数：jobmanager.execution.failover-strategy 进行配置：<br>
<img src="https://wangyemao-github.github.io/post-images/1612334885673.png" alt="" loading="lazy"></p>
<ul>
<li>Restart All ： 当某个task失败时，重启job的所有Tasks</li>
<li>Restart Pipelined Region ： 此策略将任务分为不相交的区域。 当检测到任务故障时，此策略将计算必须重新启动以从故障中恢复的最小区域集。 与“重新启动所有故障转移策略”相比，对于某些作业，这可能导致要重新启动的任务更少。<br>
一个区域就是通过pipeline data exchange的任务集。换句话说，batch data exchange标识了region的边界。
<ul>
<li>DataStrem job or Streaming Table/SQL job 都是 pipelined data exchange</li>
<li>默认，Batch Table/SQL job 都是batched data exchange</li>
<li>DataSet job中的data exchange 类型是由ExecutionMode决定的，ExecutionMode可以由ExecutionConfig设置<br>
重启的region根据如下规则确定：</li>
<li>包含了失败任务的region会重启</li>
<li>如果要重新启动的区域需要结果分区时该分区不可用，则生成结果分区的区域也将重新启动</li>
<li>如果要重新启动区域，则其所有使用方区域也将重新启动。 这是为了保证数据的一致性，因为不确定的处理或分区会导致不同的分区。</li>
</ul>
</li>
</ul>
<h2 id="checkpoints-and-state-backends">Checkpoints and State Backends</h2>
<p>这些选项控制状态后端和检查点行为的基本设置<br>
这些选项仅与以连续流方式执行的作业/应用程序相关。 以批处理方式执行的作业/应用程序不使用状态后端和检查点，而是使用针对批处理而优化的不同内部数据结构。</p>
<table>
<thead>
<tr>
<th>key</th>
<th>default</th>
<th>type</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>state.backend</td>
<td>（none）</td>
<td>String</td>
<td>用于存储和快照状态的状态后端</td>
</tr>
<tr>
<td>state.checkpoints.dir</td>
<td>（none）</td>
<td>String</td>
<td>用于在Flink支持的文件系统中存储检查点的数据文件和元数据的默认目录。存储路径必须是所有参与进程/节点可访问的（即，所有的Taskmanegers和JobManagers）</td>
</tr>
<tr>
<td>state.savepoints.dir</td>
<td>（none）</td>
<td>String</td>
<td>savepoint的默认目录。用于状态后端（MemoryStateBackend，FsStateBackend，RocksDBStateBackend）将savepoints写入到文件系统</td>
</tr>
<tr>
<td>state.backend.incremental</td>
<td>false</td>
<td>Boolean</td>
<td>选择状态后端是否应创建增量检查点（如果可能）。 对于增量检查点，仅存储与前一个检查点的差异，而不是完整的检查点状态。 启用后，显示在Web UI中或从rest API获取的状态大小仅代表增量检查点大小，而不是完整的检查点大小。 某些状态后端可能不支持增量检查点，因此会忽略此选项。</td>
</tr>
<tr>
<td>state.backend.local-recovery</td>
<td>false</td>
<td>Boolean</td>
<td>此选项为此状态后端配置本地恢复。 默认情况下，本地恢复处于禁用状态。 本地恢复当前仅涵盖键控状态后端。 当前，MemoryStateBackend不支持本地恢复，会忽略此选项。</td>
</tr>
<tr>
<td>state.checkpoints.num-retained</td>
<td>1</td>
<td>Integer</td>
<td>保留的最大已完成检查点数</td>
</tr>
<tr>
<td>taskmanager.state.local.root-dirs</td>
<td>（none）</td>
<td>String</td>
<td>config参数定义用于存储基于文件的状态以进行本地恢复的根目录。 本地恢复当前仅涵盖键控状态后端。 当前，MemoryStateBackend不支持本地恢复，请忽略此选项</td>
</tr>
</tbody>
</table>
<h2 id="memory-configuration">Memory Configuration</h2>
<p>,</p>
]]></content>
    </entry>
</feed>